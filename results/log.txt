[[34m2025-08-26 21:45:25[0m] Wandb disabled by configuration
[[34m2025-08-26 21:45:25[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 21:45:25[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 21:45:25[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 21:48:26[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-26 21:48:30[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 21:48:33[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 21:53:47[0m] Wandb disabled by configuration
[[34m2025-08-26 21:53:47[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 21:53:47[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 21:53:47[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 21:53:47[0m] enabling finetuning
[[34m2025-08-26 21:53:47[0m] set up language model
[[34m2025-08-26 21:56:36[0m] using visual_und
[[34m2025-08-26 21:56:41[0m] using generation
[[34m2025-08-26 21:56:43[0m] model loaded
[[34m2025-08-26 21:56:43[0m] tokenizers loaded
[[34m2025-08-26 21:56:48[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-26 21:56:51[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 21:56:55[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 21:57:55[0m] Wandb disabled by configuration
[[34m2025-08-26 21:57:55[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 21:57:55[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 21:57:55[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 21:57:55[0m] enabling finetuning
[[34m2025-08-26 21:57:55[0m] set up language model
[[34m2025-08-26 22:00:47[0m] using visual_und
[[34m2025-08-26 22:00:53[0m] using generation
[[34m2025-08-26 22:00:54[0m] model loaded
[[34m2025-08-26 22:00:54[0m] tokenizers loaded
[[34m2025-08-26 22:00:59[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-26 22:01:04[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 22:01:07[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 22:01:44[0m] Wandb disabled by configuration
[[34m2025-08-26 22:01:44[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=0.0001, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-26 22:01:44[0m] Model arguments ModelArguments(model_path='hf/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=32, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:01:44[0m] Data arguments DataArguments(dataset_config_file='data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:04:21[0m] Wandb disabled by configuration
[[34m2025-08-26 22:04:21[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:04:21[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:04:21[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:11:07[0m] Wandb disabled by configuration
[[34m2025-08-26 22:11:07[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:11:07[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:11:07[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:16:13[0m] Wandb disabled by configuration
[[34m2025-08-26 22:16:13[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=False, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:16:13[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:16:13[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:16:13[0m] enabling finetuning
[[34m2025-08-26 22:16:13[0m] set up language model
[[34m2025-08-26 22:18:58[0m] using visual_und
[[34m2025-08-26 22:19:04[0m] using generation
[[34m2025-08-26 22:19:07[0m] model loaded
[[34m2025-08-26 22:19:07[0m] tokenizers loaded
[[34m2025-08-26 22:19:12[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-26 22:40:37[0m] Wandb disabled by configuration
[[34m2025-08-26 22:40:37[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:40:37[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:40:37[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:41:54[0m] Wandb disabled by configuration
[[34m2025-08-26 22:41:54[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:41:54[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:41:54[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:42:32[0m] Wandb disabled by configuration
[[34m2025-08-26 22:42:32[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:42:32[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:42:32[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:49:15[0m] Wandb disabled by configuration
[[34m2025-08-26 22:49:15[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:49:15[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:49:15[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:50:34[0m] Wandb disabled by configuration
[[34m2025-08-26 22:50:34[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 22:50:34[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 22:50:34[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 22:50:35[0m] No finetune
[[34m2025-08-26 22:50:35[0m] set up language model
[[34m2025-08-26 22:50:42[0m] using visual_und
[[34m2025-08-26 23:20:07[0m] Wandb disabled by configuration
[[34m2025-08-26 23:20:07[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:20:07[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:20:07[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:20:07[0m] No finetune
[[34m2025-08-26 23:20:07[0m] set up language model
[[34m2025-08-26 23:20:12[0m] using visual_und
[[34m2025-08-26 23:20:18[0m] using generation
[[34m2025-08-26 23:22:58[0m] Wandb disabled by configuration
[[34m2025-08-26 23:22:58[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:22:58[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:22:58[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:22:58[0m] No finetune
[[34m2025-08-26 23:22:58[0m] set up language model
[[34m2025-08-26 23:23:03[0m] using visual_und
[[34m2025-08-26 23:23:04[0m] using generation
[[34m2025-08-26 23:23:04[0m] model loaded
[[34m2025-08-26 23:23:07[0m] tokenizers loaded
[[34m2025-08-26 23:23:07[0m] frozen done
[[34m2025-08-26 23:23:07[0m] fsdp done
[[34m2025-08-26 23:23:08[0m] Training from scratch.
[[34m2025-08-26 23:23:10[0m] checkpoint loading done
[[34m2025-08-26 23:28:18[0m] Wandb disabled by configuration
[[34m2025-08-26 23:28:18[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:28:18[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:28:18[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:28:18[0m] No finetune
[[34m2025-08-26 23:28:18[0m] set up language model
[[34m2025-08-26 23:28:23[0m] using visual_und
[[34m2025-08-26 23:28:23[0m] using generation
[[34m2025-08-26 23:28:24[0m] model loaded
[[34m2025-08-26 23:28:24[0m] tokenizers loaded
[[34m2025-08-26 23:28:24[0m] frozen done
[[34m2025-08-26 23:28:24[0m] fsdp done
[[34m2025-08-26 23:28:25[0m] Training from scratch.
[[34m2025-08-26 23:28:28[0m] checkpoint loading done
[[34m2025-08-26 23:35:28[0m] Wandb disabled by configuration
[[34m2025-08-26 23:35:28[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:35:28[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:35:28[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:35:28[0m] No finetune
[[34m2025-08-26 23:35:28[0m] set up language model
[[34m2025-08-26 23:35:33[0m] using visual_und
[[34m2025-08-26 23:35:33[0m] using generation
[[34m2025-08-26 23:35:34[0m] model loaded
[[34m2025-08-26 23:35:34[0m] tokenizers loaded
[[34m2025-08-26 23:35:34[0m] frozen done
[[34m2025-08-26 23:35:34[0m] fsdp done
[[34m2025-08-26 23:35:35[0m] Training from scratch.
[[34m2025-08-26 23:35:38[0m] checkpoint loading done
[[34m2025-08-26 23:37:08[0m] Wandb disabled by configuration
[[34m2025-08-26 23:37:08[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:37:08[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:37:08[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:37:08[0m] No finetune
[[34m2025-08-26 23:37:08[0m] set up language model
[[34m2025-08-26 23:37:13[0m] using visual_und
[[34m2025-08-26 23:37:13[0m] using generation
[[34m2025-08-26 23:37:14[0m] model loaded
[[34m2025-08-26 23:37:14[0m] tokenizers loaded
[[34m2025-08-26 23:37:14[0m] frozen done
[[34m2025-08-26 23:37:14[0m] fsdp done
[[34m2025-08-26 23:37:15[0m] Training from scratch.
[[34m2025-08-26 23:37:18[0m] checkpoint loading done
[[34m2025-08-26 23:37:18[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-26 23:37:18[0m] === STARTING SINGLE FORWARD PASS DEBUG ===
[[34m2025-08-26 23:37:22[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-26 23:37:22[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-26 23:37:22[0m] Input sequence_length: <class 'int'> = 33041
[[34m2025-08-26 23:37:22[0m] Input sample_lens: <class 'list'> = [2323, 2710, 3828, 2323, 1423, 937, 2071, 2323, 3337, 2323, 1403, 3394, 2323, 2323, 3823]
[[34m2025-08-26 23:37:22[0m] Input packed_text_ids: shape=torch.Size([2184]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_text_indexes: shape=torch.Size([2184]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_position_ids: shape=torch.Size([33041]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input batch_data_indexes: <class 'list'> = [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}]
[[34m2025-08-26 23:37:22[0m] Input split_lens: <class 'list'> = [17, 2306, 2522, 64, 124, 3782, 35, 11, 17, 2306, 974, 11, 438, 893, 41, 3, 1109, 182, 780, 17, 2306, 3292, 27, 18, 17, 2306, 1325, 15, 31, 11, 21, 3152, 78, 164, 17, 2306, 17, 2306, 3823]
[[34m2025-08-26 23:37:22[0m] Input attn_modes: <class 'list'> = ['causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'causal', 'noise', 'causal']
[[34m2025-08-26 23:37:22[0m] Input padded_images: shape=torch.Size([6, 3, 1024, 576]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Input patchified_vae_latent_shapes: <class 'list'> = [(64, 36), (64, 36), (64, 36), (64, 36), (64, 36), (64, 36)]
[[34m2025-08-26 23:37:22[0m] Input packed_latent_position_ids: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_vae_token_indexes: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_vit_tokens: shape=torch.Size([17033, 588]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Input packed_vit_position_ids: shape=torch.Size([17033]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_vit_token_indexes: shape=torch.Size([17033]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input vit_token_seqlens: shape=torch.Size([8]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_timesteps: shape=torch.Size([13824]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Input mse_loss_indexes: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input packed_label_ids: shape=torch.Size([1581]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input ce_loss_indexes: shape=torch.Size([1581]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Input ce_loss_weights: shape=torch.Size([1581]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}]
[[34m2025-08-26 23:37:22[0m] CE loss weights: tensor([0.0902, 0.0902, 0.0902,  ..., 0.0783, 0.0783, 0.0783], device='cuda:0')
[[34m2025-08-26 23:37:22[0m] === DATA AFTER METADATA REMOVAL ===
[[34m2025-08-26 23:37:22[0m] Model input packed_text_ids: shape=torch.Size([2184]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_text_indexes: shape=torch.Size([2184]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_position_ids: shape=torch.Size([33041]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input padded_images: shape=torch.Size([6, 3, 1024, 576]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Model input packed_latent_position_ids: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_vae_token_indexes: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_vit_tokens: shape=torch.Size([17033, 588]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Model input packed_vit_position_ids: shape=torch.Size([17033]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_vit_token_indexes: shape=torch.Size([17033]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input vit_token_seqlens: shape=torch.Size([8]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_timesteps: shape=torch.Size([13824]), dtype=torch.float32
[[34m2025-08-26 23:37:22[0m] Model input mse_loss_indexes: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input packed_label_ids: shape=torch.Size([1581]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] Model input ce_loss_indexes: shape=torch.Size([1581]), dtype=torch.int64
[[34m2025-08-26 23:37:22[0m] === VAE ENCODING ===
[[34m2025-08-26 23:37:22[0m] Original images shape: torch.Size([6, 3, 1024, 576])
[[34m2025-08-26 23:37:23[0m] Encoded latents shape: torch.Size([6, 16, 128, 72])
[[34m2025-08-26 23:37:23[0m] === FINAL MODEL INPUTS ===
[[34m2025-08-26 23:37:23[0m] Final model input packed_text_ids: shape=torch.Size([2184]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_text_indexes: shape=torch.Size([2184]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_position_ids: shape=torch.Size([33041]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_latent_position_ids: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_vae_token_indexes: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_vit_tokens: shape=torch.Size([17033, 588]), dtype=torch.float32
[[34m2025-08-26 23:37:23[0m] Final model input packed_vit_position_ids: shape=torch.Size([17033]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_vit_token_indexes: shape=torch.Size([17033]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input vit_token_seqlens: shape=torch.Size([8]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_timesteps: shape=torch.Size([13824]), dtype=torch.float32
[[34m2025-08-26 23:37:23[0m] Final model input mse_loss_indexes: shape=torch.Size([13824]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input packed_label_ids: shape=torch.Size([1581]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input ce_loss_indexes: shape=torch.Size([1581]), dtype=torch.int64
[[34m2025-08-26 23:37:23[0m] Final model input padded_latent: shape=torch.Size([6, 16, 128, 72]), dtype=torch.float32
[[34m2025-08-26 23:37:23[0m] === FORWARD PASS ===
[[34m2025-08-26 23:37:32[0m] === MODEL OUTPUTS ===
[[34m2025-08-26 23:37:32[0m] Output mse: shape=torch.Size([13824, 64]), dtype=torch.float32
[[34m2025-08-26 23:37:32[0m]   Mean: 1.798987, Std: 2.523016
[[34m2025-08-26 23:37:32[0m]   Min: 0.000000, Max: 38.285156
[[34m2025-08-26 23:37:32[0m] Output ce: shape=torch.Size([1581]), dtype=torch.float32
[[34m2025-08-26 23:37:32[0m]   Mean: 22.063467, Std: 6.316255
[[34m2025-08-26 23:37:32[0m]   Min: 6.625000, Max: 50.500000
[[34m2025-08-26 23:37:32[0m] === LOSS CALCULATION ===
[[34m2025-08-26 23:37:32[0m] Processing CE loss:
[[34m2025-08-26 23:37:32[0m]   Raw CE loss shape: torch.Size([1581])
[[34m2025-08-26 23:37:32[0m]   CE loss indexes length: 1581
[[34m2025-08-26 23:37:32[0m]   Using standard CE loss averaging
[[34m2025-08-26 23:37:32[0m]   Averaged CE loss: 22.063467
[[34m2025-08-26 23:37:32[0m]   CE contribution to total loss: 22.063467
[[34m2025-08-26 23:37:32[0m] Processing MSE loss:
[[34m2025-08-26 23:37:32[0m]   Raw MSE loss shape: torch.Size([13824, 64])
[[34m2025-08-26 23:37:32[0m]   MSE loss indexes length: 13824
[[34m2025-08-26 23:37:32[0m]   Averaged MSE loss: 1.798987
[[34m2025-08-26 23:37:32[0m]   MSE contribution to total loss: 1.798987
[[34m2025-08-26 23:37:32[0m] === FINAL LOSS SUMMARY ===
[[34m2025-08-26 23:37:32[0m] Total loss: 23.862453
[[34m2025-08-26 23:37:32[0m] CE loss: 22.063467
[[34m2025-08-26 23:37:32[0m] MSE loss: 1.798987
[[34m2025-08-26 23:37:32[0m] CE weight: 1.0
[[34m2025-08-26 23:37:32[0m] MSE weight: 1.0
[[34m2025-08-26 23:37:32[0m] Total CE tokens: 1581
[[34m2025-08-26 23:37:32[0m] Total MSE tokens: 13824
[[34m2025-08-26 23:37:32[0m] === MEMORY USAGE ===
[[34m2025-08-26 23:37:32[0m] GPU memory allocated: 15271.78 MB
[[34m2025-08-26 23:37:32[0m] GPU memory reserved: 22942.00 MB
[[34m2025-08-26 23:37:32[0m] === SINGLE FORWARD PASS COMPLETED ===
[[34m2025-08-26 23:37:32[0m] Exiting after single forward pass debug
[[34m2025-08-26 23:46:10[0m] Wandb disabled by configuration
[[34m2025-08-26 23:46:10[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:46:10[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:46:10[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:46:10[0m] enabling finetuning
[[34m2025-08-26 23:46:10[0m] set up language model
[[34m2025-08-26 23:48:54[0m] using visual_und
[[34m2025-08-26 23:49:00[0m] using generation
[[34m2025-08-26 23:49:02[0m] model loaded
[[34m2025-08-26 23:49:02[0m] tokenizers loaded
[[34m2025-08-26 23:49:02[0m] frozen done
[[34m2025-08-26 23:49:02[0m] fsdp done
[[34m2025-08-26 23:49:07[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-26 23:49:12[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 23:49:15[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-26 23:52:42[0m] Wandb disabled by configuration
[[34m2025-08-26 23:52:42[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-26 23:52:42[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-26 23:52:42[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-26 23:52:42[0m] No finetune
[[34m2025-08-26 23:52:42[0m] set up language model
[[34m2025-08-26 23:52:47[0m] using visual_und
[[34m2025-08-26 23:52:47[0m] using generation
[[34m2025-08-26 23:52:48[0m] model loaded
[[34m2025-08-26 23:52:48[0m] tokenizers loaded
[[34m2025-08-26 23:52:48[0m] frozen done
[[34m2025-08-26 23:52:48[0m] fsdp done
[[34m2025-08-26 23:52:49[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 00:00:23[0m] Wandb disabled by configuration
[[34m2025-08-27 00:00:23[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 00:00:23[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 00:00:23[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 00:00:23[0m] No finetune
[[34m2025-08-27 00:00:23[0m] set up language model
[[34m2025-08-27 00:00:27[0m] using visual_und
[[34m2025-08-27 00:00:28[0m] using generation
[[34m2025-08-27 00:00:28[0m] model loaded
[[34m2025-08-27 00:00:29[0m] tokenizers loaded
[[34m2025-08-27 00:00:29[0m] frozen done
[[34m2025-08-27 00:00:29[0m] fsdp done
[[34m2025-08-27 00:00:29[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 00:03:32[0m] Wandb disabled by configuration
[[34m2025-08-27 00:03:32[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 00:03:32[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 00:03:32[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 00:03:32[0m] enabling finetuning
[[34m2025-08-27 00:03:32[0m] set up language model
[[34m2025-08-27 00:06:36[0m] using visual_und
[[34m2025-08-27 00:06:42[0m] using generation
[[34m2025-08-27 00:06:43[0m] model loaded
[[34m2025-08-27 00:06:43[0m] tokenizers loaded
[[34m2025-08-27 00:06:43[0m] frozen done
[[34m2025-08-27 00:06:43[0m] fsdp done
[[34m2025-08-27 00:06:46[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 00:06:51[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:06:53[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:10:57[0m] Wandb disabled by configuration
[[34m2025-08-27 00:10:57[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 00:10:57[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 00:10:57[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 00:10:57[0m] enabling finetuning
[[34m2025-08-27 00:10:57[0m] set up language model
[[34m2025-08-27 00:13:42[0m] using visual_und
[[34m2025-08-27 00:13:48[0m] using generation
[[34m2025-08-27 00:13:49[0m] model loaded
[[34m2025-08-27 00:13:50[0m] tokenizers loaded
[[34m2025-08-27 00:13:50[0m] frozen done
[[34m2025-08-27 00:13:50[0m] fsdp done
[[34m2025-08-27 00:13:54[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 00:13:58[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:14:01[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:18:53[0m] Wandb disabled by configuration
[[34m2025-08-27 00:18:53[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 00:18:53[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 00:18:53[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 00:18:53[0m] enabling finetuning
[[34m2025-08-27 00:18:53[0m] set up language model
[[34m2025-08-27 00:21:36[0m] using visual_und
[[34m2025-08-27 00:21:42[0m] using generation
[[34m2025-08-27 00:21:43[0m] model loaded
[[34m2025-08-27 00:21:43[0m] tokenizers loaded
[[34m2025-08-27 00:21:43[0m] frozen done
[[34m2025-08-27 00:21:43[0m] fsdp done
[[34m2025-08-27 00:21:47[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 00:21:50[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:21:53[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:24:16[0m] Wandb disabled by configuration
[[34m2025-08-27 00:24:16[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 00:24:16[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 00:24:16[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=4, max_num_tokens_per_sample=16384, max_num_tokens=36864, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 00:24:16[0m] enabling finetuning
[[34m2025-08-27 00:24:16[0m] set up language model
[[34m2025-08-27 00:27:04[0m] using visual_und
[[34m2025-08-27 00:27:09[0m] using generation
[[34m2025-08-27 00:27:11[0m] model loaded
[[34m2025-08-27 00:27:11[0m] tokenizers loaded
[[34m2025-08-27 00:27:11[0m] frozen done
[[34m2025-08-27 00:27:11[0m] fsdp done
[[34m2025-08-27 00:27:15[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 00:27:19[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:27:22[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 00:58:58[0m] Wandb disabled by configuration
[[34m2025-08-27 00:58:58[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=2, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 00:58:58[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 00:58:58[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 00:58:58[0m] enabling finetuning
[[34m2025-08-27 00:58:58[0m] set up language model
[[34m2025-08-27 01:01:46[0m] using visual_und
[[34m2025-08-27 01:01:53[0m] using generation
[[34m2025-08-27 01:01:54[0m] model loaded
[[34m2025-08-27 01:01:54[0m] tokenizers loaded
[[34m2025-08-27 01:01:54[0m] frozen done
[[34m2025-08-27 01:01:54[0m] fsdp done
[[34m2025-08-27 01:01:59[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 01:02:03[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:02:06[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:04:26[0m] Wandb disabled by configuration
[[34m2025-08-27 01:04:26[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 01:04:26[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:04:26[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:04:26[0m] enabling finetuning
[[34m2025-08-27 01:04:26[0m] set up language model
[[34m2025-08-27 01:07:13[0m] using visual_und
[[34m2025-08-27 01:07:20[0m] using generation
[[34m2025-08-27 01:07:21[0m] model loaded
[[34m2025-08-27 01:07:21[0m] tokenizers loaded
[[34m2025-08-27 01:07:21[0m] frozen done
[[34m2025-08-27 01:07:21[0m] fsdp done
[[34m2025-08-27 01:07:26[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 01:07:30[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:07:33[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:12:40[0m] Wandb disabled by configuration
[[34m2025-08-27 01:12:40[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=False, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 01:12:40[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:12:40[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:12:40[0m] enabling finetuning
[[34m2025-08-27 01:12:40[0m] set up language model
[[34m2025-08-27 01:15:38[0m] using visual_und
[[34m2025-08-27 01:15:43[0m] using generation
[[34m2025-08-27 01:15:45[0m] model loaded
[[34m2025-08-27 01:15:45[0m] tokenizers loaded
[[34m2025-08-27 01:15:45[0m] frozen done
[[34m2025-08-27 01:15:45[0m] fsdp done
[[34m2025-08-27 01:15:49[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 01:16:26[0m] Wandb disabled by configuration
[[34m2025-08-27 01:16:26[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 01:16:26[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:16:26[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:16:26[0m] enabling finetuning
[[34m2025-08-27 01:16:26[0m] set up language model
[[34m2025-08-27 01:19:09[0m] using visual_und
[[34m2025-08-27 01:19:14[0m] using generation
[[34m2025-08-27 01:19:16[0m] model loaded
[[34m2025-08-27 01:19:16[0m] tokenizers loaded
[[34m2025-08-27 01:19:16[0m] frozen done
[[34m2025-08-27 01:19:16[0m] fsdp done
[[34m2025-08-27 01:19:20[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-08-27 01:19:23[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:19:26[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:22:22[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=10240, num_replicate=1, num_shard=8, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 01:22:22[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:22:22[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=10240, max_num_tokens=11520, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:25:36[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 01:25:44[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:25:50[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:40:42[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=10240, num_replicate=1, num_shard=2, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 01:40:42[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:40:42[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=10240, max_num_tokens=11520, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:44:14[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 01:44:39[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:44:47[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:49:30[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=10240, num_replicate=1, num_shard=2, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 01:49:30[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:49:30[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=10240, max_num_tokens=11520, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:53:08[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 01:53:18[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:53:25[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 01:53:45[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-27 01:55:38[0m] Wandb disabled by configuration
[[34m2025-08-27 01:55:38[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 01:55:38[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:55:38[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:55:39[0m] No finetune
[[34m2025-08-27 01:55:39[0m] set up language model
[[34m2025-08-27 01:55:43[0m] using visual_und
[[34m2025-08-27 01:55:44[0m] using generation
[[34m2025-08-27 01:59:58[0m] Wandb disabled by configuration
[[34m2025-08-27 01:59:58[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='none', resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-27 01:59:58[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 01:59:58[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 01:59:58[0m] No finetune
[[34m2025-08-27 01:59:58[0m] set up language model
[[34m2025-08-27 02:00:03[0m] using visual_und
[[34m2025-08-27 02:00:04[0m] using generation
[[34m2025-08-27 02:00:04[0m] model loaded
[[34m2025-08-27 02:00:05[0m] tokenizers loaded
[[34m2025-08-27 02:00:05[0m] frozen done
[[34m2025-08-27 02:00:05[0m] fsdp done
[[34m2025-08-27 02:00:06[0m] Training from scratch.
[[34m2025-08-27 02:00:09[0m] checkpoint loading done
[[34m2025-08-27 02:00:09[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-27 02:00:09[0m] === STARTING SINGLE FORWARD PASS DEBUG ===
[[34m2025-08-27 02:00:13[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-27 02:00:13[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-27 02:00:13[0m] Input sequence_length: <class 'int'> = 15862
[[34m2025-08-27 02:00:13[0m] Input sample_lens: <class 'list'> = [2323, 827, 1507, 2323, 1239, 3310, 1015, 2323, 995, 522]
[[34m2025-08-27 02:00:13[0m] Input packed_text_ids: shape=torch.Size([699]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_text_indexes: shape=torch.Size([699]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_position_ids: shape=torch.Size([15862]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input batch_data_indexes: <class 'list'> = [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-27 02:00:13[0m] Input split_lens: <class 'list'> = [17, 2306, 785, 38, 4, 1472, 32, 3, 17, 2306, 1192, 44, 3, 3082, 86, 142, 974, 33, 8, 17, 2306, 758, 113, 124, 522]
[[34m2025-08-27 02:00:13[0m] Input attn_modes: <class 'list'> = ['causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'causal']
[[34m2025-08-27 02:00:13[0m] Input padded_images: shape=torch.Size([3, 3, 1024, 576]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Input patchified_vae_latent_shapes: <class 'list'> = [(64, 36), (64, 36), (64, 36)]
[[34m2025-08-27 02:00:13[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_vit_tokens: shape=torch.Size([8251, 588]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Input packed_vit_position_ids: shape=torch.Size([8251]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_vit_token_indexes: shape=torch.Size([8251]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input vit_token_seqlens: shape=torch.Size([6]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input packed_label_ids: shape=torch.Size([278]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input ce_loss_indexes: shape=torch.Size([278]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Input ce_loss_weights: shape=torch.Size([278]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-27 02:00:13[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774, 0.7071, 0.7071, 0.7071, 0.7071, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842, 0.0842,
        0.0842, 0.0842, 0.0842, 0.0842, 0.3780, 0.3780, 0.3780, 0.3780, 0.3780,
        0.3780, 0.3780, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902,
        0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902, 0.0902],
       device='cuda:0')
[[34m2025-08-27 02:00:13[0m] === DATA AFTER METADATA REMOVAL ===
[[34m2025-08-27 02:00:13[0m] Model input packed_text_ids: shape=torch.Size([699]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_text_indexes: shape=torch.Size([699]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_position_ids: shape=torch.Size([15862]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input padded_images: shape=torch.Size([3, 3, 1024, 576]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Model input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_vit_tokens: shape=torch.Size([8251, 588]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Model input packed_vit_position_ids: shape=torch.Size([8251]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_vit_token_indexes: shape=torch.Size([8251]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input vit_token_seqlens: shape=torch.Size([6]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32
[[34m2025-08-27 02:00:13[0m] Model input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input packed_label_ids: shape=torch.Size([278]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] Model input ce_loss_indexes: shape=torch.Size([278]), dtype=torch.int64
[[34m2025-08-27 02:00:13[0m] === VAE ENCODING ===
[[34m2025-08-27 02:00:13[0m] Original images shape: torch.Size([3, 3, 1024, 576])
[[34m2025-08-27 02:00:14[0m] Encoded latents shape: torch.Size([3, 16, 128, 72])
[[34m2025-08-27 02:00:14[0m] === FINAL MODEL INPUTS ===
[[34m2025-08-27 02:00:14[0m] Final model input packed_text_ids: shape=torch.Size([699]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_text_indexes: shape=torch.Size([699]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_position_ids: shape=torch.Size([15862]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_vit_tokens: shape=torch.Size([8251, 588]), dtype=torch.float32
[[34m2025-08-27 02:00:14[0m] Final model input packed_vit_position_ids: shape=torch.Size([8251]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_vit_token_indexes: shape=torch.Size([8251]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input vit_token_seqlens: shape=torch.Size([6]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32
[[34m2025-08-27 02:00:14[0m] Final model input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input packed_label_ids: shape=torch.Size([278]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input ce_loss_indexes: shape=torch.Size([278]), dtype=torch.int64
[[34m2025-08-27 02:00:14[0m] Final model input padded_latent: shape=torch.Size([3, 16, 128, 72]), dtype=torch.float32
[[34m2025-08-27 02:00:14[0m] === FORWARD PASS ===
[[34m2025-08-27 02:00:22[0m] === MODEL OUTPUTS ===
[[34m2025-08-27 02:00:22[0m] Output mse: shape=torch.Size([6912, 64]), dtype=torch.float32
[[34m2025-08-27 02:00:22[0m]   Mean: 1.796768, Std: 2.523048
[[34m2025-08-27 02:00:22[0m]   Min: 0.000000, Max: 38.285156
[[34m2025-08-27 02:00:22[0m] Output ce: shape=torch.Size([278]), dtype=torch.float32
[[34m2025-08-27 02:00:22[0m]   Mean: 21.273607, Std: 5.260282
[[34m2025-08-27 02:00:22[0m]   Min: 9.437500, Max: 41.000000
[[34m2025-08-27 02:00:22[0m] === LOSS CALCULATION ===
[[34m2025-08-27 02:00:22[0m] Processing CE loss:
[[34m2025-08-27 02:00:22[0m]   Raw CE loss shape: torch.Size([278])
[[34m2025-08-27 02:00:22[0m]   CE loss indexes length: 278
[[34m2025-08-27 02:00:22[0m]   Using standard CE loss averaging
[[34m2025-08-27 02:00:22[0m]   Averaged CE loss: 21.273607
[[34m2025-08-27 02:00:22[0m]   CE contribution to total loss: 21.273607
[[34m2025-08-27 02:00:22[0m] Processing MSE loss:
[[34m2025-08-27 02:00:22[0m]   Raw MSE loss shape: torch.Size([6912, 64])
[[34m2025-08-27 02:00:22[0m]   MSE loss indexes length: 6912
[[34m2025-08-27 02:00:22[0m]   Averaged MSE loss: 1.796768
[[34m2025-08-27 02:00:22[0m]   MSE contribution to total loss: 1.796768
[[34m2025-08-27 02:00:22[0m] === FINAL LOSS SUMMARY ===
[[34m2025-08-27 02:00:22[0m] Total loss: 23.070375
[[34m2025-08-27 02:00:22[0m] CE loss: 21.273607
[[34m2025-08-27 02:00:22[0m] MSE loss: 1.796768
[[34m2025-08-27 02:00:22[0m] CE weight: 1.0
[[34m2025-08-27 02:00:22[0m] MSE weight: 1.0
[[34m2025-08-27 02:00:22[0m] Total CE tokens: 278
[[34m2025-08-27 02:00:22[0m] Total MSE tokens: 6912
[[34m2025-08-27 02:00:22[0m] === MEMORY USAGE ===
[[34m2025-08-27 02:00:22[0m] GPU memory allocated: 12639.11 MB
[[34m2025-08-27 02:00:22[0m] GPU memory reserved: 16750.00 MB
[[34m2025-08-27 02:00:22[0m] === SINGLE FORWARD PASS COMPLETED ===
[[34m2025-08-27 02:00:22[0m] Exiting after single forward pass debug
[[34m2025-08-27 02:05:29[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:05:29[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=10240, num_replicate=1, num_shard=2, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 02:05:29[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 02:05:29[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=10240, max_num_tokens=11520, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 02:08:25[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:08:32[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:08:33[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:08:34[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:08:56[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 02:09:03[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 02:09:10[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 02:09:31[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-27 02:15:19[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:15:21[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=10240, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 02:15:21[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 02:15:21[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=10240, max_num_tokens=11520, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 02:18:13[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:18:19[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:18:20[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:18:21[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:18:43[0m] 
=== MEMORY AFTER EMA MODEL COPY ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:18:43[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 02:18:52[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 02:19:01[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 02:19:02[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.03 GB
Max Allocated: 0.00 GB
Max Reserved:  0.03 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:11[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 13.60 GB
Reserved:  21.91 GB
Max Allocated: 21.79 GB
Max Reserved:  21.91 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:25[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 27.21 GB
Reserved:  35.84 GB
Max Allocated: 35.33 GB
Max Reserved:  35.84 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:25[0m] 
=== MEMORY AFTER ACTIVATION CHECKPOINTING ===
Allocated: 27.21 GB
Reserved:  35.84 GB
Max Allocated: 35.33 GB
Max Reserved:  35.84 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:25[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 27.21 GB
Reserved:  35.84 GB
Max Allocated: 35.33 GB
Max Reserved:  35.84 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:25[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 27.52 GB
Reserved:  35.84 GB
Max Allocated: 35.33 GB
Max Reserved:  35.84 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:25[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-27 02:19:26[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 27.52 GB
Reserved:  35.84 GB
Max Allocated: 35.33 GB
Max Reserved:  35.84 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:26[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 27.63 GB
Reserved:  35.84 GB
Max Allocated: 35.33 GB
Max Reserved:  35.84 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 02:19:29[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 32.14 GB
Reserved:  64.22 GB
Max Allocated: 35.33 GB
Max Reserved:  64.22 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:32:04[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:32:07[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=8192, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 20:32:07[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 20:32:07[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=8192, max_num_tokens=10240, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 20:35:15[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:35:21[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:35:22[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:35:23[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:35:47[0m] 
=== MEMORY AFTER EMA MODEL COPY ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:35:47[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 20:35:57[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 20:40:21[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:40:22[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=8192, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 20:40:22[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 20:40:22[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=8192, max_num_tokens=10240, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 20:43:09[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:43:15[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:43:16[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:43:17[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:43:37[0m] 
=== MEMORY AFTER EMA MODEL COPY ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:43:37[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 20:43:44[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 20:43:50[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 20:43:51[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 20:43:59[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 27.23 GB
Reserved:  37.99 GB
Max Allocated: 35.43 GB
Max Reserved:  37.99 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:11:01[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:11:01[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=8192, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 23:11:01[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 23:11:01[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=1, max_num_tokens_per_sample=8192, max_num_tokens=10240, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 23:13:50[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:13:55[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:13:56[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:13:56[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:14:00[0m] 
=== MEMORY AFTER EMA MODEL COPY ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:14:00[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 23:14:03[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 23:14:05[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 23:14:05[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:14:15[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 54.42 GB
Reserved:  59.05 GB
Max Allocated: 58.55 GB
Max Reserved:  59.05 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:21:56[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:21:57[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=8192, num_replicate=1, num_shard=2, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 23:21:57[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 23:21:57[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=10240, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 23:24:45[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:24:52[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:24:52[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:24:53[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:14[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 23:25:21[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 23:25:28[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 23:25:28[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:36[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 27.23 GB
Reserved:  37.99 GB
Max Allocated: 35.43 GB
Max Reserved:  37.99 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:48[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 54.46 GB
Reserved:  64.37 GB
Max Allocated: 62.59 GB
Max Reserved:  64.37 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:48[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 54.46 GB
Reserved:  64.37 GB
Max Allocated: 62.59 GB
Max Reserved:  64.37 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:48[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 54.78 GB
Reserved:  64.38 GB
Max Allocated: 62.59 GB
Max Reserved:  64.38 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:48[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-27 23:25:49[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 54.78 GB
Reserved:  64.38 GB
Max Allocated: 62.59 GB
Max Reserved:  64.38 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:49[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 54.85 GB
Reserved:  64.38 GB
Max Allocated: 62.59 GB
Max Reserved:  64.38 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:25:54[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 59.38 GB
Reserved:  71.05 GB
Max Allocated: 62.59 GB
Max Reserved:  71.05 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:35:10[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:35:11[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=4096, num_replicate=1, num_shard=1, sharding_strategy='HYBRID_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 23:35:11[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 23:35:11[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=4096, max_num_tokens=6144, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 23:38:10[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:38:16[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:38:17[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:38:18[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:38:40[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 23:41:09[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:41:10[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.9999, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=4096, num_replicate=1, num_shard=2, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 23:41:10[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 23:41:10[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=4096, max_num_tokens=6144, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-27 23:44:01[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:44:07[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:44:07[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:44:08[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:44:29[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-27 23:44:38[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 23:44:45[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-27 23:44:45[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:44:53[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 27.23 GB
Reserved:  37.99 GB
Max Allocated: 35.43 GB
Max Reserved:  37.99 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:45:16[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 54.46 GB
Reserved:  64.37 GB
Max Allocated: 62.59 GB
Max Reserved:  64.37 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:45:16[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 54.46 GB
Reserved:  64.37 GB
Max Allocated: 62.59 GB
Max Reserved:  64.37 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:45:16[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 54.78 GB
Reserved:  64.38 GB
Max Allocated: 62.59 GB
Max Reserved:  64.38 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:45:16[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-27 23:45:17[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 54.78 GB
Reserved:  64.38 GB
Max Allocated: 62.59 GB
Max Reserved:  64.38 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:45:17[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 54.81 GB
Reserved:  64.38 GB
Max Allocated: 62.59 GB
Max Reserved:  64.38 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:45:18[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 58.14 GB
Reserved:  70.15 GB
Max Allocated: 62.59 GB
Max Reserved:  70.15 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:57:07[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-27 23:57:09[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=1024, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-27 23:57:09[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-27 23:57:09[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=1024, max_num_tokens=2048, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 00:00:02[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:00:08[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:00:08[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:00:09[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:00:30[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 00:00:39[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:00:46[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:00:46[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:00:55[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 13.61 GB
Reserved:  23.82 GB
Max Allocated: 21.80 GB
Max Reserved:  23.82 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:01:10[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:01:10[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:01:10[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:01:10[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-28 00:01:11[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:01:11[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 27.53 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:10:27[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:10:29[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=1200, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 00:10:29[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 00:10:29[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=1200, max_num_tokens=2048, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 00:13:23[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:13:29[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:13:30[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:13:30[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:13:52[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 00:14:00[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:14:07[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:14:07[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:14:16[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 13.61 GB
Reserved:  23.82 GB
Max Allocated: 21.80 GB
Max Reserved:  23.82 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:14:37[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:14:37[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:14:37[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:14:37[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-28 00:14:38[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:14:38[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 27.53 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:16:32[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:16:34[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=1028, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 00:16:34[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 00:16:34[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=1500, max_num_tokens=2048, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 00:19:27[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:19:33[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:19:34[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:19:34[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:19:56[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 00:20:04[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:20:11[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:20:12[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:20:19[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 13.61 GB
Reserved:  23.82 GB
Max Allocated: 21.80 GB
Max Reserved:  23.82 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:20:31[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:20:31[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:20:32[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:20:32[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-28 00:20:32[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:20:32[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 27.53 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:52:14[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:52:16[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=1028, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 00:52:16[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 00:52:16[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=1500, max_num_tokens=2048, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 00:55:08[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:55:14[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:55:15[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:55:16[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:55:37[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 00:55:47[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:55:55[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 00:55:55[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:04[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 13.61 GB
Reserved:  23.82 GB
Max Allocated: 21.80 GB
Max Reserved:  23.82 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:16[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:16[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 27.21 GB
Reserved:  38.04 GB
Max Allocated: 35.34 GB
Max Reserved:  38.04 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:16[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:16[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-28 00:56:18[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 27.52 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:18[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 27.53 GB
Reserved:  38.06 GB
Max Allocated: 35.34 GB
Max Reserved:  38.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 00:56:23[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 29.97 GB
Reserved:  41.55 GB
Max Allocated: 35.34 GB
Max Reserved:  41.55 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 01:16:39[0m] === BAGEL VILEX FORWARD PASS TEST ===
[[34m2025-08-28 01:16:39[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:16:39[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:16:39[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:16:39[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 01:16:39[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 01:16:39[0m] Setting up language model...
[[34m2025-08-28 01:18:31[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 01:18:31[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:18:31[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:18:31[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:18:31[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 01:18:31[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:18:31[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 01:18:31[0m] Loaded pretrained ViT model
[[34m2025-08-28 01:18:31[0m] ViT config - patch size: 14
[[34m2025-08-28 01:18:31[0m] ViT config - hidden size: 1152
[[34m2025-08-28 01:18:31[0m] ViT config - num layers: 26
[[34m2025-08-28 01:18:31[0m] ViT config - image size: 980
[[34m2025-08-28 01:18:31[0m] ViT max patches per side: 50
[[34m2025-08-28 01:18:32[0m] ViT model moved to GPU and set to eval mode
[[34m2025-08-28 01:20:18[0m] === BAGEL VILEX FORWARD PASS TEST ===
[[34m2025-08-28 01:20:18[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:20:18[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:20:18[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:20:18[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 01:20:18[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 01:20:18[0m] Setting up language model...
[[34m2025-08-28 01:20:23[0m] Language model hidden size: 896
[[34m2025-08-28 01:20:23[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:20:23[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 01:20:23[0m] ViT max patches per side: 50
[[34m2025-08-28 01:20:23[0m] Setting up VAE model...
[[34m2025-08-28 01:20:23[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 01:20:23[0m] Max latent size: 48
[[34m2025-08-28 01:20:23[0m] Creating Bagel model...
[[34m2025-08-28 01:20:23[0m] Bagel model created successfully
[[34m2025-08-28 01:20:23[0m] Setting up tokenizer...
[[34m2025-08-28 01:20:24[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 01:20:24[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 01:20:24[0m] VAE model frozen
[[34m2025-08-28 01:20:24[0m] Setting up FSDP...
[[34m2025-08-28 01:20:26[0m] Model setup complete
[[34m2025-08-28 01:20:26[0m] === SETTING UP DATASET ===
[[34m2025-08-28 01:20:27[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 01:20:31[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 01:20:31[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 01:20:31[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 01:20:31[0m] Input sequence_length: <class 'int'> = 16289
[[34m2025-08-28 01:20:31[0m] Input sample_lens: list of length 24
[[34m2025-08-28 01:20:31[0m] Input packed_text_ids: shape=torch.Size([1953]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_text_indexes: shape=torch.Size([1953]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_position_ids: shape=torch.Size([16289]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input batch_data_indexes: list of length 23
[[34m2025-08-28 01:20:31[0m] Input split_lens: list of length 59
[[34m2025-08-28 01:20:31[0m] Input attn_modes: list of length 59
[[34m2025-08-28 01:20:31[0m] Input padded_images: shape=torch.Size([10, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input patchified_vae_latent_shapes: list of length 10
[[34m2025-08-28 01:20:31[0m] Input packed_latent_position_ids: shape=torch.Size([5760]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_vae_token_indexes: shape=torch.Size([5760]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_vit_tokens: shape=torch.Size([8576, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_vit_position_ids: shape=torch.Size([8576]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_vit_token_indexes: shape=torch.Size([8576]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input vit_token_seqlens: shape=torch.Size([13]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m]   Values: tensor([ 728,  407,  333,  851,  690,   81,  121,  925,  592,  999,  888,  888,
        1073], device='cuda:0')
[[34m2025-08-28 01:20:31[0m] Input packed_timesteps: shape=torch.Size([5760]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input mse_loss_indexes: shape=torch.Size([5760]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input packed_label_ids: shape=torch.Size([1141]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input ce_loss_indexes: shape=torch.Size([1141]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Input ce_loss_weights: shape=torch.Size([1141]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 01:20:31[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}]
[[34m2025-08-28 01:20:31[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.0874, 0.0874, 0.0874], device='cuda:0')
[[34m2025-08-28 01:20:31[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 01:20:31[0m] Sequence length: 16289
[[34m2025-08-28 01:20:31[0m] Packed text IDs shape: torch.Size([1953])
[[34m2025-08-28 01:20:31[0m] Packed text indexes shape: torch.Size([1953])
[[34m2025-08-28 01:20:31[0m] Text embeddings shape: torch.Size([1953, 896])
[[34m2025-08-28 01:20:31[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 01:20:31[0m] Initial packed sequence shape: torch.Size([16289, 896])
[[34m2025-08-28 01:20:31[0m] Non-zero elements in sequence: 1749888
[[34m2025-08-28 01:20:31[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 01:20:31[0m] ViT tokens shape: torch.Size([8576, 588])
[[34m2025-08-28 01:20:31[0m] ViT token indexes shape: torch.Size([8576])
[[34m2025-08-28 01:20:31[0m] ViT position IDs shape: torch.Size([8576])
[[34m2025-08-28 01:20:31[0m] ViT token seq lengths: tensor([ 728,  407,  333,  851,  690,   81,  121,  925,  592,  999,  888,  888,
        1073], device='cuda:0')
[[34m2025-08-28 01:20:31[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503, 8576], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 01:20:31[0m] Max seqlen: 1073
[[34m2025-08-28 01:20:32[0m] ViT output shape: torch.Size([8576, 1152])
[[34m2025-08-28 01:20:32[0m] ViT output dtype: torch.float32
[[34m2025-08-28 01:20:32[0m] After connector shape: torch.Size([8576, 896])
[[34m2025-08-28 01:20:32[0m] ViT position embeddings shape: torch.Size([8576, 896])
[[34m2025-08-28 01:20:32[0m] ViT final embeddings shape: torch.Size([8576, 896])
[[34m2025-08-28 01:21:51[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 01:21:51[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:21:51[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:21:51[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:21:51[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 01:21:51[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:21:51[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 01:21:51[0m] Loaded pretrained ViT model
[[34m2025-08-28 01:21:51[0m] ViT config - patch size: 14
[[34m2025-08-28 01:21:51[0m] ViT config - hidden size: 1152
[[34m2025-08-28 01:21:51[0m] ViT config - num layers: 26
[[34m2025-08-28 01:21:51[0m] ViT config - image size: 980
[[34m2025-08-28 01:21:51[0m] ViT max patches per side: 50
[[34m2025-08-28 01:21:52[0m] ViT model moved to GPU and set to eval mode
[[34m2025-08-28 01:21:52[0m] Converted ViT conv2d to linear
[[34m2025-08-28 01:21:52[0m] Setting up tokenizer...
[[34m2025-08-28 01:21:52[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 01:21:52[0m] === SETTING UP DATASET ===
[[34m2025-08-28 01:21:52[0m] === STARTING ViT ENCODING TEST ===
[[34m2025-08-28 01:21:57[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 01:21:57[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 01:21:57[0m] === ViT INPUT DATA ANALYSIS ===
[[34m2025-08-28 01:21:57[0m] ViT tokens shape: torch.Size([5871, 588])
[[34m2025-08-28 01:21:57[0m] ViT tokens dtype: torch.float32
[[34m2025-08-28 01:21:57[0m] ViT tokens device: cuda:0
[[34m2025-08-28 01:21:57[0m] ViT tokens min/max: -1.0000 / 1.0000
[[34m2025-08-28 01:21:57[0m] ViT token indexes shape: torch.Size([5871])
[[34m2025-08-28 01:21:57[0m] ViT position IDs shape: torch.Size([5871])
[[34m2025-08-28 01:21:57[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 144],
       device='cuda:0')
[[34m2025-08-28 01:21:57[0m] Total ViT patches: 5871
[[34m2025-08-28 01:21:57[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 5871],
       device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 01:21:57[0m] Max seqlen: 999
[[34m2025-08-28 01:21:57[0m] === ViT FORWARD PASS ===
[[34m2025-08-28 01:21:57[0m] Running ViT forward pass...
[[34m2025-08-28 01:21:57[0m] === ViT OUTPUT ANALYSIS ===
[[34m2025-08-28 01:21:57[0m] ViT output shape: torch.Size([5871, 1152])
[[34m2025-08-28 01:21:57[0m] ViT output dtype: torch.float32
[[34m2025-08-28 01:21:57[0m] ViT output device: cuda:0
[[34m2025-08-28 01:21:57[0m] ViT output min/max: -60.2930 / 107.1815
[[34m2025-08-28 01:21:57[0m] ViT output mean/std: 0.0422 / 2.2303
[[34m2025-08-28 01:21:57[0m] First patch features (first 10): tensor([ 2.1117,  0.6659, -0.9839, -1.1677, -0.5980, -0.0110,  0.0316, -0.4238,
        -1.9744,  0.9462], device='cuda:0')
[[34m2025-08-28 01:21:57[0m] Second patch features (first 10): tensor([-1.3575, -0.1096,  2.8656, -2.5558,  1.6504, -1.7299, -0.3853, -1.1989,
        -1.5767,  0.7621], device='cuda:0')
[[34m2025-08-28 01:21:57[0m] Contains NaN: False
[[34m2025-08-28 01:21:57[0m] Contains Inf: False
[[34m2025-08-28 01:21:57[0m] === PER-IMAGE STATISTICS ===
[[34m2025-08-28 01:21:57[0m] Image 0: 728 patches, mean=0.0358, std=2.0881
[[34m2025-08-28 01:21:57[0m] Image 1: 407 patches, mean=0.0556, std=2.3486
[[34m2025-08-28 01:21:57[0m] Image 2: 333 patches, mean=0.0483, std=2.2049
[[34m2025-08-28 01:21:57[0m] Image 3: 851 patches, mean=0.0410, std=2.2951
[[34m2025-08-28 01:21:57[0m] Image 4: 690 patches, mean=0.0402, std=2.2360
[[34m2025-08-28 01:21:57[0m] Image 5: 81 patches, mean=0.0428, std=2.1704
[[34m2025-08-28 01:21:57[0m] Image 6: 121 patches, mean=0.0441, std=2.1374
[[34m2025-08-28 01:21:57[0m] Image 7: 925 patches, mean=0.0423, std=2.2652
[[34m2025-08-28 01:21:57[0m] Image 8: 592 patches, mean=0.0467, std=2.2899
[[34m2025-08-28 01:21:57[0m] Image 9: 999 patches, mean=0.0381, std=2.1826
[[34m2025-08-28 01:21:57[0m] Image 10: 144 patches, mean=0.0455, std=2.1856
[[34m2025-08-28 01:21:57[0m] === ViT ENCODING SUMMARY ===
[[34m2025-08-28 01:21:57[0m] Input: 5871 ViT patches of shape torch.Size([588])
[[34m2025-08-28 01:21:57[0m] Output: 5871 feature vectors of dim 1152
[[34m2025-08-28 01:21:57[0m] Processed 11 images with lengths: [728, 407, 333, 851, 690, 81, 121, 925, 592, 999, 144]
[[34m2025-08-28 01:21:57[0m] ViT encoding successful: output range [-60.2930, 107.1815]
[[34m2025-08-28 01:21:57[0m] === MEMORY USAGE ===
[[34m2025-08-28 01:21:57[0m] GPU memory allocated: 1.57 GB
[[34m2025-08-28 01:21:57[0m] GPU memory reserved: 2.52 GB
[[34m2025-08-28 01:21:57[0m] === ViT ENCODING TEST COMPLETED SUCCESSFULLY ===
[[34m2025-08-28 01:53:22[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 01:53:22[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, tail_drop_prob=4, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:53:22[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:53:22[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:53:22[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 01:53:22[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:53:22[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 01:53:22[0m] Loaded pretrained ViT model
[[34m2025-08-28 01:53:22[0m] ViT config - patch size: 14
[[34m2025-08-28 01:53:22[0m] ViT config - hidden size: 1152
[[34m2025-08-28 01:53:22[0m] ViT config - num layers: 26
[[34m2025-08-28 01:53:22[0m] ViT config - image size: 980
[[34m2025-08-28 01:53:22[0m] ViT max patches per side: 50
[[34m2025-08-28 01:55:00[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 01:55:00[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, tail_drop_prob=4, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:55:00[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:55:00[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:55:00[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 01:55:00[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:55:00[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 01:55:01[0m] Loaded pretrained ViT model
[[34m2025-08-28 01:55:01[0m] ViT config - patch size: 14
[[34m2025-08-28 01:55:01[0m] ViT config - hidden size: 1152
[[34m2025-08-28 01:55:01[0m] ViT config - num layers: 26
[[34m2025-08-28 01:55:01[0m] ViT config - image size: 980
[[34m2025-08-28 01:55:01[0m] ViT max patches per side: 50
[[34m2025-08-28 01:55:01[0m] ViT model moved to GPU and set to eval mode
[[34m2025-08-28 01:55:01[0m] Converted ViT conv2d to linear
[[34m2025-08-28 01:55:01[0m] Setting up tokenizer...
[[34m2025-08-28 01:55:02[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 01:55:02[0m] === SETTING UP DATASET ===
[[34m2025-08-28 01:55:02[0m] === STARTING ViT ENCODING TEST ===
[[34m2025-08-28 01:55:06[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 01:55:06[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 01:55:06[0m] === ViT INPUT DATA ANALYSIS ===
[[34m2025-08-28 01:55:06[0m] ViT tokens shape: torch.Size([5727, 588])
[[34m2025-08-28 01:55:06[0m] ViT tokens dtype: torch.float32
[[34m2025-08-28 01:55:06[0m] ViT tokens device: cuda:0
[[34m2025-08-28 01:55:06[0m] ViT tokens min/max: -1.0000 / 1.0000
[[34m2025-08-28 01:55:06[0m] ViT token indexes shape: torch.Size([5727])
[[34m2025-08-28 01:55:06[0m] ViT position IDs shape: torch.Size([5727])
[[34m2025-08-28 01:55:06[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999], device='cuda:0')
[[34m2025-08-28 01:55:06[0m] Total ViT patches: 5727
[[34m2025-08-28 01:55:06[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727],
       device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 01:55:06[0m] Max seqlen: 999
[[34m2025-08-28 01:55:06[0m] === ViT FORWARD PASS ===
[[34m2025-08-28 01:55:06[0m] Running ViT forward pass...
[[34m2025-08-28 01:58:25[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 01:58:25[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, tail_drop_prob=4, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:58:25[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:58:25[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:58:25[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 01:58:25[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:58:25[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 01:58:25[0m] Loaded pretrained ViT model
[[34m2025-08-28 01:58:25[0m] ViT config - patch size: 14
[[34m2025-08-28 01:58:25[0m] ViT config - hidden size: 1152
[[34m2025-08-28 01:58:25[0m] ViT config - num layers: 26
[[34m2025-08-28 01:58:25[0m] ViT config - image size: 980
[[34m2025-08-28 01:58:25[0m] ViT max patches per side: 50
[[34m2025-08-28 01:58:26[0m] ViT model moved to GPU and set to eval mode
[[34m2025-08-28 01:58:26[0m] Converted ViT conv2d to linear
[[34m2025-08-28 01:58:26[0m] Setting up tokenizer...
[[34m2025-08-28 01:58:26[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 01:58:26[0m] === SETTING UP DATASET ===
[[34m2025-08-28 01:58:26[0m] === STARTING ViT ENCODING TEST ===
[[34m2025-08-28 01:58:31[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 01:58:31[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 01:58:31[0m] === ViT INPUT DATA ANALYSIS ===
[[34m2025-08-28 01:58:31[0m] ViT tokens shape: torch.Size([5727, 588])
[[34m2025-08-28 01:58:31[0m] ViT tokens dtype: torch.float32
[[34m2025-08-28 01:58:31[0m] ViT tokens device: cuda:0
[[34m2025-08-28 01:58:31[0m] ViT tokens min/max: -1.0000 / 1.0000
[[34m2025-08-28 01:58:31[0m] ViT token indexes shape: torch.Size([5727])
[[34m2025-08-28 01:58:31[0m] ViT position IDs shape: torch.Size([5727])
[[34m2025-08-28 01:58:31[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999], device='cuda:0')
[[34m2025-08-28 01:58:31[0m] Total ViT patches: 5727
[[34m2025-08-28 01:58:31[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727],
       device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 01:58:31[0m] Max seqlen: 999
[[34m2025-08-28 01:58:31[0m] === ViT FORWARD PASS ===
[[34m2025-08-28 01:58:31[0m] Running ViT forward pass...
[[34m2025-08-28 01:59:52[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 01:59:52[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, tail_drop_prob=4, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 01:59:52[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 01:59:52[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 01:59:52[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 01:59:52[0m] Setting up vision model (ViT)...
[[34m2025-08-28 01:59:52[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 01:59:52[0m] Loaded pretrained ViT model
[[34m2025-08-28 01:59:52[0m] ViT config - patch size: 14
[[34m2025-08-28 01:59:52[0m] ViT config - hidden size: 1152
[[34m2025-08-28 01:59:52[0m] ViT config - num layers: 26
[[34m2025-08-28 01:59:52[0m] ViT config - image size: 980
[[34m2025-08-28 01:59:52[0m] ViT max patches per side: 50
[[34m2025-08-28 01:59:53[0m] ViT model moved to GPU and set to eval mode
[[34m2025-08-28 01:59:53[0m] Converted ViT conv2d to linear
[[34m2025-08-28 01:59:53[0m] Setting up tokenizer...
[[34m2025-08-28 01:59:53[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 01:59:53[0m] === SETTING UP DATASET ===
[[34m2025-08-28 01:59:53[0m] === STARTING ViT ENCODING TEST ===
[[34m2025-08-28 01:59:57[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 01:59:57[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 01:59:57[0m] === ViT INPUT DATA ANALYSIS ===
[[34m2025-08-28 01:59:57[0m] ViT tokens shape: torch.Size([5727, 588])
[[34m2025-08-28 01:59:57[0m] ViT tokens dtype: torch.float32
[[34m2025-08-28 01:59:57[0m] ViT tokens device: cuda:0
[[34m2025-08-28 01:59:57[0m] ViT tokens min/max: -1.0000 / 1.0000
[[34m2025-08-28 01:59:57[0m] ViT token indexes shape: torch.Size([5727])
[[34m2025-08-28 01:59:57[0m] ViT position IDs shape: torch.Size([5727])
[[34m2025-08-28 01:59:57[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999], device='cuda:0')
[[34m2025-08-28 01:59:57[0m] Total ViT patches: 5727
[[34m2025-08-28 01:59:57[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727],
       device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 01:59:57[0m] Max seqlen: 999
[[34m2025-08-28 01:59:57[0m] === ViT FORWARD PASS ===
[[34m2025-08-28 01:59:57[0m] Running ViT forward pass...
[[34m2025-08-28 01:59:58[0m] === ViT OUTPUT ANALYSIS ===
[[34m2025-08-28 01:59:58[0m] ViT output shape: torch.Size([5727, 1152])
[[34m2025-08-28 01:59:58[0m] ViT output dtype: torch.float32
[[34m2025-08-28 01:59:58[0m] ViT output device: cuda:0
[[34m2025-08-28 01:59:58[0m] ViT output min/max: -60.2930 / 107.1815
[[34m2025-08-28 01:59:58[0m] ViT output mean/std: 0.0421 / 2.2314
[[34m2025-08-28 01:59:58[0m] First patch features (first 10): tensor([ 2.1117,  0.6659, -0.9839, -1.1677, -0.5980, -0.0110,  0.0316, -0.4238,
        -1.9744,  0.9462], device='cuda:0')
[[34m2025-08-28 01:59:58[0m] Second patch features (first 10): tensor([-1.3575, -0.1096,  2.8656, -2.5558,  1.6504, -1.7299, -0.3853, -1.1989,
        -1.5767,  0.7621], device='cuda:0')
[[34m2025-08-28 01:59:58[0m] Contains NaN: False
[[34m2025-08-28 01:59:58[0m] Contains Inf: False
[[34m2025-08-28 01:59:58[0m] === PER-IMAGE STATISTICS ===
[[34m2025-08-28 01:59:58[0m] Image 0: 728 patches, mean=0.0358, std=2.0881
[[34m2025-08-28 01:59:58[0m] Image 1: 407 patches, mean=0.0556, std=2.3486
[[34m2025-08-28 01:59:58[0m] Image 2: 333 patches, mean=0.0483, std=2.2049
[[34m2025-08-28 01:59:58[0m] Image 3: 851 patches, mean=0.0410, std=2.2951
[[34m2025-08-28 01:59:58[0m] Image 4: 690 patches, mean=0.0402, std=2.2360
[[34m2025-08-28 01:59:58[0m] Image 5: 81 patches, mean=0.0428, std=2.1704
[[34m2025-08-28 01:59:58[0m] Image 6: 121 patches, mean=0.0441, std=2.1374
[[34m2025-08-28 01:59:58[0m] Image 7: 925 patches, mean=0.0423, std=2.2652
[[34m2025-08-28 01:59:58[0m] Image 8: 592 patches, mean=0.0467, std=2.2899
[[34m2025-08-28 01:59:58[0m] Image 9: 999 patches, mean=0.0381, std=2.1826
[[34m2025-08-28 01:59:58[0m] === ViT ENCODING SUMMARY ===
[[34m2025-08-28 01:59:58[0m] Input: 5727 ViT patches of shape torch.Size([588])
[[34m2025-08-28 01:59:58[0m] Output: 5727 feature vectors of dim 1152
[[34m2025-08-28 01:59:58[0m] Processed 10 images with lengths: [728, 407, 333, 851, 690, 81, 121, 925, 592, 999]
[[34m2025-08-28 01:59:58[0m] ViT encoding successful: output range [-60.2930, 107.1815]
[[34m2025-08-28 01:59:58[0m] === MEMORY USAGE ===
[[34m2025-08-28 01:59:58[0m] GPU memory allocated: 1.59 GB
[[34m2025-08-28 01:59:58[0m] GPU memory reserved: 2.56 GB
[[34m2025-08-28 01:59:58[0m] === ViT ENCODING TEST COMPLETED SUCCESSFULLY ===
[[34m2025-08-28 02:01:17[0m] === ViT ENCODING TEST ===
[[34m2025-08-28 02:01:17[0m] Model arguments: ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, tail_drop_prob=4, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 02:01:17[0m] Data arguments: DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 02:01:17[0m] Training arguments: TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 02:01:17[0m] === SETTING UP ViT MODEL ===
[[34m2025-08-28 02:01:17[0m] Setting up vision model (ViT)...
[[34m2025-08-28 02:01:17[0m] Loaded ViT config from pretrained path: HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit
[[34m2025-08-28 02:01:17[0m] Loaded pretrained ViT model
[[34m2025-08-28 02:01:17[0m] ViT config - patch size: 14
[[34m2025-08-28 02:01:17[0m] ViT config - hidden size: 1152
[[34m2025-08-28 02:01:17[0m] ViT config - num layers: 26
[[34m2025-08-28 02:01:17[0m] ViT config - image size: 980
[[34m2025-08-28 02:01:17[0m] ViT max patches per side: 50
[[34m2025-08-28 02:01:18[0m] ViT model moved to GPU and set to eval mode
[[34m2025-08-28 02:01:18[0m] Converted ViT conv2d to linear
[[34m2025-08-28 02:01:18[0m] Setting up tokenizer...
[[34m2025-08-28 02:01:18[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 02:01:18[0m] === SETTING UP DATASET ===
[[34m2025-08-28 02:01:18[0m] === STARTING ViT ENCODING TEST ===
[[34m2025-08-28 02:01:22[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 02:01:22[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 02:01:22[0m] === ViT INPUT DATA ANALYSIS ===
[[34m2025-08-28 02:01:22[0m] ViT tokens shape: torch.Size([8576, 588])
[[34m2025-08-28 02:01:22[0m] ViT tokens dtype: torch.float32
[[34m2025-08-28 02:01:22[0m] ViT tokens device: cuda:0
[[34m2025-08-28 02:01:22[0m] ViT tokens min/max: -1.0000 / 1.0000
[[34m2025-08-28 02:01:22[0m] ViT token indexes shape: torch.Size([8576])
[[34m2025-08-28 02:01:22[0m] ViT position IDs shape: torch.Size([8576])
[[34m2025-08-28 02:01:22[0m] ViT token seq lengths: tensor([ 728,  407,  333,  851,  690,   81,  121,  925,  592,  999,  888,  888,
        1073], device='cuda:0')
[[34m2025-08-28 02:01:22[0m] Total ViT patches: 8576
[[34m2025-08-28 02:01:22[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503, 8576], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 02:01:22[0m] Max seqlen: 1073
[[34m2025-08-28 02:01:22[0m] === ViT FORWARD PASS ===
[[34m2025-08-28 02:01:22[0m] Running ViT forward pass...
[[34m2025-08-28 02:01:23[0m] === ViT OUTPUT ANALYSIS ===
[[34m2025-08-28 02:01:23[0m] ViT output shape: torch.Size([8576, 1152])
[[34m2025-08-28 02:01:23[0m] ViT output dtype: torch.float32
[[34m2025-08-28 02:01:23[0m] ViT output device: cuda:0
[[34m2025-08-28 02:01:23[0m] ViT output min/max: -60.2930 / 107.1815
[[34m2025-08-28 02:01:23[0m] ViT output mean/std: 0.0396 / 2.1820
[[34m2025-08-28 02:01:23[0m] First patch features (first 10): tensor([ 2.1117,  0.6659, -0.9839, -1.1677, -0.5980, -0.0110,  0.0316, -0.4238,
        -1.9744,  0.9462], device='cuda:0')
[[34m2025-08-28 02:01:23[0m] Second patch features (first 10): tensor([-1.3575, -0.1096,  2.8656, -2.5558,  1.6504, -1.7299, -0.3853, -1.1989,
        -1.5767,  0.7621], device='cuda:0')
[[34m2025-08-28 02:01:23[0m] Contains NaN: False
[[34m2025-08-28 02:01:23[0m] Contains Inf: False
[[34m2025-08-28 02:01:23[0m] === PER-IMAGE STATISTICS ===
[[34m2025-08-28 02:01:23[0m] Image 0: 728 patches, mean=0.0358, std=2.0881
[[34m2025-08-28 02:01:23[0m] Image 1: 407 patches, mean=0.0556, std=2.3486
[[34m2025-08-28 02:01:23[0m] Image 2: 333 patches, mean=0.0483, std=2.2049
[[34m2025-08-28 02:01:23[0m] Image 3: 851 patches, mean=0.0410, std=2.2951
[[34m2025-08-28 02:01:23[0m] Image 4: 690 patches, mean=0.0402, std=2.2360
[[34m2025-08-28 02:01:23[0m] Image 5: 81 patches, mean=0.0428, std=2.1704
[[34m2025-08-28 02:01:23[0m] Image 6: 121 patches, mean=0.0441, std=2.1374
[[34m2025-08-28 02:01:23[0m] Image 7: 925 patches, mean=0.0423, std=2.2652
[[34m2025-08-28 02:01:23[0m] Image 8: 592 patches, mean=0.0467, std=2.2899
[[34m2025-08-28 02:01:23[0m] Image 9: 999 patches, mean=0.0381, std=2.1826
[[34m2025-08-28 02:01:23[0m] Image 10: 888 patches, mean=0.0312, std=1.9899
[[34m2025-08-28 02:01:23[0m] Image 11: 888 patches, mean=0.0316, std=2.0203
[[34m2025-08-28 02:01:23[0m] Image 12: 1073 patches, mean=0.0401, std=2.1961
[[34m2025-08-28 02:01:23[0m] === ViT ENCODING SUMMARY ===
[[34m2025-08-28 02:01:23[0m] Input: 8576 ViT patches of shape torch.Size([588])
[[34m2025-08-28 02:01:23[0m] Output: 8576 feature vectors of dim 1152
[[34m2025-08-28 02:01:23[0m] Processed 13 images with lengths: [728, 407, 333, 851, 690, 81, 121, 925, 592, 999, 888, 888, 1073]
[[34m2025-08-28 02:01:23[0m] ViT encoding successful: output range [-60.2930, 107.1815]
[[34m2025-08-28 02:01:23[0m] === MEMORY USAGE ===
[[34m2025-08-28 02:01:23[0m] GPU memory allocated: 1.63 GB
[[34m2025-08-28 02:01:23[0m] GPU memory reserved: 2.66 GB
[[34m2025-08-28 02:01:23[0m] === ViT ENCODING TEST COMPLETED SUCCESSFULLY ===
[[34m2025-08-28 02:02:07[0m] === BAGEL VILEX FORWARD PASS TEST ===
[[34m2025-08-28 02:02:07[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 02:02:07[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, tail_drop_prob=4, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 02:02:07[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 02:02:07[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 02:02:08[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 02:02:08[0m] Setting up language model...
[[34m2025-08-28 02:02:12[0m] Language model hidden size: 896
[[34m2025-08-28 02:02:12[0m] Setting up vision model (ViT)...
[[34m2025-08-28 02:02:12[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 02:02:12[0m] ViT max patches per side: 50
[[34m2025-08-28 02:02:12[0m] Setting up VAE model...
[[34m2025-08-28 02:02:13[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 02:02:13[0m] Max latent size: 48
[[34m2025-08-28 02:02:13[0m] Creating Bagel model...
[[34m2025-08-28 02:02:13[0m] Bagel model created successfully
[[34m2025-08-28 02:02:13[0m] Setting up tokenizer...
[[34m2025-08-28 02:02:14[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 02:02:14[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 02:02:14[0m] VAE model frozen
[[34m2025-08-28 02:02:14[0m] Setting up FSDP...
[[34m2025-08-28 02:02:16[0m] Model setup complete
[[34m2025-08-28 02:02:16[0m] === SETTING UP DATASET ===
[[34m2025-08-28 02:02:16[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 02:02:21[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 02:02:21[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 02:02:21[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 02:02:21[0m] Input sequence_length: <class 'int'> = 16289
[[34m2025-08-28 02:02:21[0m] Input sample_lens: list of length 24
[[34m2025-08-28 02:02:21[0m] Input packed_text_ids: shape=torch.Size([1953]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_text_indexes: shape=torch.Size([1953]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_position_ids: shape=torch.Size([16289]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input batch_data_indexes: list of length 23
[[34m2025-08-28 02:02:21[0m] Input split_lens: list of length 59
[[34m2025-08-28 02:02:21[0m] Input attn_modes: list of length 59
[[34m2025-08-28 02:02:21[0m] Input padded_images: shape=torch.Size([10, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input patchified_vae_latent_shapes: list of length 10
[[34m2025-08-28 02:02:21[0m] Input packed_latent_position_ids: shape=torch.Size([5760]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_vae_token_indexes: shape=torch.Size([5760]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_vit_tokens: shape=torch.Size([8576, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_vit_position_ids: shape=torch.Size([8576]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_vit_token_indexes: shape=torch.Size([8576]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input vit_token_seqlens: shape=torch.Size([13]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m]   Values: tensor([ 728,  407,  333,  851,  690,   81,  121,  925,  592,  999,  888,  888,
        1073], device='cuda:0')
[[34m2025-08-28 02:02:21[0m] Input packed_timesteps: shape=torch.Size([5760]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input mse_loss_indexes: shape=torch.Size([5760]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input packed_label_ids: shape=torch.Size([1141]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input ce_loss_indexes: shape=torch.Size([1141]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Input ce_loss_weights: shape=torch.Size([1141]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 02:02:21[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}]
[[34m2025-08-28 02:02:21[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.0874, 0.0874, 0.0874], device='cuda:0')
[[34m2025-08-28 02:02:21[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 02:02:21[0m] Sequence length: 16289
[[34m2025-08-28 02:02:21[0m] Packed text IDs shape: torch.Size([1953])
[[34m2025-08-28 02:02:21[0m] Packed text indexes shape: torch.Size([1953])
[[34m2025-08-28 02:02:21[0m] Text embeddings shape: torch.Size([1953, 896])
[[34m2025-08-28 02:02:21[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 02:02:21[0m] Initial packed sequence shape: torch.Size([16289, 896])
[[34m2025-08-28 02:02:21[0m] Non-zero elements in sequence: 1749888
[[34m2025-08-28 02:02:21[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 02:02:21[0m] ViT tokens shape: torch.Size([8576, 588])
[[34m2025-08-28 02:02:21[0m] ViT token indexes shape: torch.Size([8576])
[[34m2025-08-28 02:02:21[0m] ViT position IDs shape: torch.Size([8576])
[[34m2025-08-28 02:02:21[0m] ViT token seq lengths: tensor([ 728,  407,  333,  851,  690,   81,  121,  925,  592,  999,  888,  888,
        1073], device='cuda:0')
[[34m2025-08-28 02:02:21[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503, 8576], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 02:02:21[0m] Max seqlen: 1073
[[34m2025-08-28 02:02:21[0m] ViT output shape: torch.Size([8576, 1152])
[[34m2025-08-28 02:02:21[0m] ViT output dtype: torch.float32
[[34m2025-08-28 02:02:21[0m] After connector shape: torch.Size([8576, 896])
[[34m2025-08-28 02:02:21[0m] ViT position embeddings shape: torch.Size([8576, 896])
[[34m2025-08-28 02:02:21[0m] ViT final embeddings shape: torch.Size([8576, 896])
[[34m2025-08-28 03:27:36[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:27:36[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:27:36[0m] Setting up language model...
[[34m2025-08-28 03:27:40[0m] Language model hidden size: 896
[[34m2025-08-28 03:27:40[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:27:41[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:27:41[0m] ViT max patches per side: 50
[[34m2025-08-28 03:27:41[0m] Setting up VAE model...
[[34m2025-08-28 03:27:41[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:27:41[0m] Max latent size: 48
[[34m2025-08-28 03:27:41[0m] Creating Bagel model...
[[34m2025-08-28 03:27:42[0m] Bagel model created successfully
[[34m2025-08-28 03:27:42[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 03:27:42[0m] Original connector type: <class 'modeling.bagel.modeling_utils.MLPconnector'>
[[34m2025-08-28 03:27:42[0m] Original connector input dim: 1152
[[34m2025-08-28 03:27:42[0m] Original connector output dim: 896
[[34m2025-08-28 03:27:42[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 03:27:42[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:27:42[0m] New connector input dim: 1152
[[34m2025-08-28 03:27:42[0m] New connector output dim: 896
[[34m2025-08-28 03:27:42[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 03:27:42[0m] New connector num output tokens: 1
[[34m2025-08-28 03:27:42[0m] Setting up tokenizer...
[[34m2025-08-28 03:27:42[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 03:27:42[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 03:27:42[0m] VAE model frozen
[[34m2025-08-28 03:27:42[0m] Setting up FSDP...
[[34m2025-08-28 03:27:45[0m] Model setup complete
[[34m2025-08-28 03:27:45[0m] === SETTING UP DATASET ===
[[34m2025-08-28 03:27:45[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 03:27:50[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 03:27:50[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 03:27:50[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 03:27:50[0m] Input sequence_length: <class 'int'> = 16305
[[34m2025-08-28 03:27:50[0m] Input sample_lens: list of length 27
[[34m2025-08-28 03:27:50[0m] Input packed_text_ids: shape=torch.Size([1950]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_text_indexes: shape=torch.Size([1950]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_position_ids: shape=torch.Size([16305]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input batch_data_indexes: list of length 26
[[34m2025-08-28 03:27:50[0m] Input split_lens: list of length 66
[[34m2025-08-28 03:27:50[0m] Input attn_modes: list of length 66
[[34m2025-08-28 03:27:50[0m] Input padded_images: shape=torch.Size([13, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input patchified_vae_latent_shapes: list of length 13
[[34m2025-08-28 03:27:50[0m] Input packed_latent_position_ids: shape=torch.Size([7488]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_vae_token_indexes: shape=torch.Size([7488]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_vit_tokens: shape=torch.Size([6867, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_vit_position_ids: shape=torch.Size([6867]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_vit_token_indexes: shape=torch.Size([6867]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input vit_token_seqlens: shape=torch.Size([13]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 108, 144],
       device='cuda:0')
[[34m2025-08-28 03:27:50[0m] Input packed_timesteps: shape=torch.Size([7488]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input mse_loss_indexes: shape=torch.Size([7488]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input packed_label_ids: shape=torch.Size([1081]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input ce_loss_indexes: shape=torch.Size([1081]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Input ce_loss_weights: shape=torch.Size([1081]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:27:50[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 1, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 18, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 03:27:50[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.3780, 0.3780, 0.3780], device='cuda:0')
[[34m2025-08-28 03:27:50[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 03:27:50[0m] Sequence length: 16305
[[34m2025-08-28 03:27:50[0m] Packed text IDs shape: torch.Size([1950])
[[34m2025-08-28 03:27:50[0m] Packed text indexes shape: torch.Size([1950])
[[34m2025-08-28 03:27:50[0m] Text embeddings shape: torch.Size([1950, 896])
[[34m2025-08-28 03:27:50[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 03:27:50[0m] Initial packed sequence shape: torch.Size([16305, 896])
[[34m2025-08-28 03:27:50[0m] Non-zero elements in sequence: 1747200
[[34m2025-08-28 03:27:50[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 03:27:50[0m] ViT tokens shape: torch.Size([6867, 588])
[[34m2025-08-28 03:27:50[0m] ViT token indexes shape: torch.Size([6867])
[[34m2025-08-28 03:27:50[0m] ViT position IDs shape: torch.Size([6867])
[[34m2025-08-28 03:27:50[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 108, 144],
       device='cuda:0')
[[34m2025-08-28 03:27:50[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        6723, 6867], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 03:27:50[0m] Max seqlen: 999
[[34m2025-08-28 03:27:50[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 03:27:50[0m] ViT output shape: torch.Size([6867, 1152])
[[34m2025-08-28 03:27:50[0m] ViT output dtype: torch.float32
[[34m2025-08-28 03:27:50[0m] ViT output range: [-60.262207, 106.434052]
[[34m2025-08-28 03:27:50[0m] ViT output mean/std: 0.040770 / 2.198586
[[34m2025-08-28 03:27:50[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 03:27:50[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:27:50[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 03:27:50[0m] Projector num output tokens: 1
[[34m2025-08-28 03:27:50[0m] Projector input dim: 1152
[[34m2025-08-28 03:27:50[0m] Projector output dim: 896
[[34m2025-08-28 03:27:50[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 03:27:50[0m] Created simulated hidden states with 4 layers
[[34m2025-08-28 03:27:50[0m] Applying ViLex projector transformation...
[[34m2025-08-28 03:36:39[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:36:39[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:36:39[0m] Setting up language model...
[[34m2025-08-28 03:36:44[0m] Language model hidden size: 896
[[34m2025-08-28 03:36:44[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:36:44[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:36:44[0m] ViT max patches per side: 50
[[34m2025-08-28 03:36:44[0m] Setting up VAE model...
[[34m2025-08-28 03:36:45[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:36:45[0m] Max latent size: 48
[[34m2025-08-28 03:36:45[0m] Creating Bagel model...
[[34m2025-08-28 03:36:45[0m] Bagel model created successfully
[[34m2025-08-28 03:36:45[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 03:36:45[0m] Original connector type: <class 'modeling.bagel.modeling_utils.MLPconnector'>
[[34m2025-08-28 03:36:45[0m] Original connector input dim: 1152
[[34m2025-08-28 03:36:45[0m] Original connector output dim: 896
[[34m2025-08-28 03:36:45[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 03:36:45[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:36:45[0m] New connector input dim: 1152
[[34m2025-08-28 03:36:45[0m] New connector output dim: 896
[[34m2025-08-28 03:36:45[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 03:36:45[0m] New connector num output tokens: 1
[[34m2025-08-28 03:36:45[0m] Setting up tokenizer...
[[34m2025-08-28 03:36:45[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 03:36:45[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 03:36:45[0m] VAE model frozen
[[34m2025-08-28 03:36:45[0m] Setting up FSDP...
[[34m2025-08-28 03:36:48[0m] Model setup complete
[[34m2025-08-28 03:36:48[0m] === SETTING UP DATASET ===
[[34m2025-08-28 03:36:48[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 03:36:53[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 03:36:53[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 03:36:53[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 03:36:53[0m] Input sequence_length: <class 'int'> = 16305
[[34m2025-08-28 03:36:53[0m] Input sample_lens: list of length 27
[[34m2025-08-28 03:36:53[0m] Input packed_text_ids: shape=torch.Size([1950]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_text_indexes: shape=torch.Size([1950]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_position_ids: shape=torch.Size([16305]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input batch_data_indexes: list of length 26
[[34m2025-08-28 03:36:53[0m] Input split_lens: list of length 66
[[34m2025-08-28 03:36:53[0m] Input attn_modes: list of length 66
[[34m2025-08-28 03:36:53[0m] Input padded_images: shape=torch.Size([13, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input patchified_vae_latent_shapes: list of length 13
[[34m2025-08-28 03:36:53[0m] Input packed_latent_position_ids: shape=torch.Size([7488]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_vae_token_indexes: shape=torch.Size([7488]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_vit_tokens: shape=torch.Size([6867, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_vit_position_ids: shape=torch.Size([6867]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_vit_token_indexes: shape=torch.Size([6867]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input vit_token_seqlens: shape=torch.Size([13]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 108, 144],
       device='cuda:0')
[[34m2025-08-28 03:36:53[0m] Input packed_timesteps: shape=torch.Size([7488]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input mse_loss_indexes: shape=torch.Size([7488]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input packed_label_ids: shape=torch.Size([1081]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input ce_loss_indexes: shape=torch.Size([1081]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Input ce_loss_weights: shape=torch.Size([1081]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:36:53[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 1, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 18, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 03:36:53[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.3780, 0.3780, 0.3780], device='cuda:0')
[[34m2025-08-28 03:36:53[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 03:36:53[0m] Sequence length: 16305
[[34m2025-08-28 03:36:53[0m] Packed text IDs shape: torch.Size([1950])
[[34m2025-08-28 03:36:53[0m] Packed text indexes shape: torch.Size([1950])
[[34m2025-08-28 03:36:53[0m] Text embeddings shape: torch.Size([1950, 896])
[[34m2025-08-28 03:36:53[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 03:36:53[0m] Initial packed sequence shape: torch.Size([16305, 896])
[[34m2025-08-28 03:36:53[0m] Non-zero elements in sequence: 1747200
[[34m2025-08-28 03:36:53[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 03:36:53[0m] ViT tokens shape: torch.Size([6867, 588])
[[34m2025-08-28 03:36:53[0m] ViT token indexes shape: torch.Size([6867])
[[34m2025-08-28 03:36:53[0m] ViT position IDs shape: torch.Size([6867])
[[34m2025-08-28 03:36:53[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 108, 144],
       device='cuda:0')
[[34m2025-08-28 03:36:53[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        6723, 6867], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 03:36:53[0m] Max seqlen: 999
[[34m2025-08-28 03:36:53[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 03:36:53[0m] ViT output shape: torch.Size([6867, 1152])
[[34m2025-08-28 03:36:53[0m] ViT output dtype: torch.float32
[[34m2025-08-28 03:36:53[0m] ViT output range: [-60.262207, 106.434052]
[[34m2025-08-28 03:36:53[0m] ViT output mean/std: 0.040770 / 2.198586
[[34m2025-08-28 03:36:53[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 03:36:53[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:36:53[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 03:36:53[0m] Projector num output tokens: 1
[[34m2025-08-28 03:36:53[0m] Projector input dim: 1152
[[34m2025-08-28 03:36:53[0m] Projector output dim: 896
[[34m2025-08-28 03:36:53[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 03:36:53[0m] Created simulated hidden states with 4 layers
[[34m2025-08-28 03:36:53[0m] Applying ViLex projector transformation...
[[34m2025-08-28 03:38:14[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:38:14[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:38:14[0m] Setting up language model...
[[34m2025-08-28 03:38:19[0m] Language model hidden size: 896
[[34m2025-08-28 03:38:19[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:38:19[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:38:19[0m] ViT max patches per side: 50
[[34m2025-08-28 03:38:19[0m] Setting up VAE model...
[[34m2025-08-28 03:38:20[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:38:20[0m] Max latent size: 48
[[34m2025-08-28 03:38:20[0m] Creating Bagel model...
[[34m2025-08-28 03:39:26[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:39:27[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:39:27[0m] Setting up language model...
[[34m2025-08-28 03:39:31[0m] Language model hidden size: 896
[[34m2025-08-28 03:39:31[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:39:31[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:39:31[0m] ViT max patches per side: 50
[[34m2025-08-28 03:39:31[0m] Setting up VAE model...
[[34m2025-08-28 03:39:32[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:39:32[0m] Max latent size: 48
[[34m2025-08-28 03:39:32[0m] Creating Bagel model...
[[34m2025-08-28 03:40:43[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:40:43[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:40:43[0m] Setting up language model...
[[34m2025-08-28 03:40:48[0m] Language model hidden size: 896
[[34m2025-08-28 03:40:48[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:40:48[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:40:48[0m] ViT max patches per side: 50
[[34m2025-08-28 03:40:48[0m] Setting up VAE model...
[[34m2025-08-28 03:40:49[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:40:49[0m] Max latent size: 48
[[34m2025-08-28 03:40:49[0m] Creating Bagel model...
[[34m2025-08-28 03:42:16[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:42:16[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:42:16[0m] Setting up language model...
[[34m2025-08-28 03:42:21[0m] Language model hidden size: 896
[[34m2025-08-28 03:42:21[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:42:22[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:42:22[0m] ViT max patches per side: 50
[[34m2025-08-28 03:42:22[0m] Setting up VAE model...
[[34m2025-08-28 03:42:22[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:42:22[0m] Max latent size: 48
[[34m2025-08-28 03:42:22[0m] Creating Bagel model...
[[34m2025-08-28 03:43:49[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:43:49[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:43:49[0m] Setting up language model...
[[34m2025-08-28 03:43:54[0m] Language model hidden size: 896
[[34m2025-08-28 03:43:54[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:43:54[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:43:54[0m] ViT max patches per side: 50
[[34m2025-08-28 03:43:54[0m] Setting up VAE model...
[[34m2025-08-28 03:43:55[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:43:55[0m] Max latent size: 48
[[34m2025-08-28 03:43:55[0m] Creating Bagel model...
[[34m2025-08-28 03:44:14[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:44:14[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:44:14[0m] Setting up language model...
[[34m2025-08-28 03:44:18[0m] Language model hidden size: 896
[[34m2025-08-28 03:44:18[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:44:19[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:44:19[0m] ViT max patches per side: 50
[[34m2025-08-28 03:44:19[0m] Setting up VAE model...
[[34m2025-08-28 03:44:19[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:44:19[0m] Max latent size: 48
[[34m2025-08-28 03:44:19[0m] Creating Bagel model...
[[34m2025-08-28 03:49:10[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:49:10[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:49:10[0m] Setting up language model...
[[34m2025-08-28 03:49:15[0m] Language model hidden size: 896
[[34m2025-08-28 03:49:15[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:49:15[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:49:15[0m] ViT max patches per side: 50
[[34m2025-08-28 03:49:15[0m] Setting up VAE model...
[[34m2025-08-28 03:49:16[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:49:16[0m] Max latent size: 48
[[34m2025-08-28 03:49:16[0m] Creating Bagel model...
[[34m2025-08-28 03:49:37[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:49:37[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:49:37[0m] Setting up language model...
[[34m2025-08-28 03:49:42[0m] Language model hidden size: 896
[[34m2025-08-28 03:49:42[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:49:42[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:49:42[0m] ViT max patches per side: 50
[[34m2025-08-28 03:49:42[0m] Setting up VAE model...
[[34m2025-08-28 03:49:43[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:49:43[0m] Max latent size: 48
[[34m2025-08-28 03:49:43[0m] Creating Bagel model...
[[34m2025-08-28 03:50:44[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:50:44[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:50:44[0m] Setting up language model...
[[34m2025-08-28 03:50:48[0m] Language model hidden size: 896
[[34m2025-08-28 03:50:48[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:50:49[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:50:49[0m] ViT max patches per side: 50
[[34m2025-08-28 03:50:49[0m] Setting up VAE model...
[[34m2025-08-28 03:50:49[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:50:49[0m] Max latent size: 48
[[34m2025-08-28 03:50:49[0m] Creating Bagel model...
[[34m2025-08-28 03:50:49[0m] Bagel model created successfully
[[34m2025-08-28 03:50:49[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 03:50:49[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:50:49[0m] Original connector input dim: 1152
[[34m2025-08-28 03:50:49[0m] Original connector output dim: 896
[[34m2025-08-28 03:50:49[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 03:50:49[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:50:49[0m] New connector input dim: 1152
[[34m2025-08-28 03:50:49[0m] New connector output dim: 896
[[34m2025-08-28 03:50:49[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 03:50:49[0m] New connector num output tokens: 1
[[34m2025-08-28 03:50:49[0m] Setting up tokenizer...
[[34m2025-08-28 03:50:50[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 03:50:50[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 03:50:50[0m] VAE model frozen
[[34m2025-08-28 03:50:50[0m] Setting up FSDP...
[[34m2025-08-28 03:50:52[0m] Model setup complete
[[34m2025-08-28 03:50:52[0m] === SETTING UP DATASET ===
[[34m2025-08-28 03:50:52[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 03:50:57[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 03:50:57[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 03:50:57[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 03:50:57[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 03:50:57[0m] Input sample_lens: list of length 25
[[34m2025-08-28 03:50:57[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 03:50:57[0m] Input split_lens: list of length 61
[[34m2025-08-28 03:50:57[0m] Input attn_modes: list of length 61
[[34m2025-08-28 03:50:57[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 03:50:57[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 03:50:57[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:50:57[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 03:50:57[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 03:50:57[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 03:50:57[0m] Sequence length: 16252
[[34m2025-08-28 03:50:57[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 03:50:57[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 03:50:57[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 03:50:57[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 03:50:57[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 03:50:57[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 03:50:57[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 03:50:57[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 03:50:57[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 03:50:57[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 03:50:57[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 03:50:57[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 03:50:57[0m] Max seqlen: 999
[[34m2025-08-28 03:50:57[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 03:50:58[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 03:50:58[0m] ViT output dtype: torch.float32
[[34m2025-08-28 03:50:58[0m] ViT output range: [-60.262207, 106.434052]
[[34m2025-08-28 03:50:58[0m] ViT output mean/std: 0.039570 / 2.179576
[[34m2025-08-28 03:50:58[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 03:50:58[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:50:58[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 03:50:58[0m] Projector num output tokens: 1
[[34m2025-08-28 03:50:58[0m] Projector input dim: 1152
[[34m2025-08-28 03:50:58[0m] Projector output dim: 896
[[34m2025-08-28 03:50:58[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 03:50:58[0m] Created simulated hidden states with 4 layers
[[34m2025-08-28 03:50:58[0m] Applying ViLex projector transformation...
[[34m2025-08-28 03:52:30[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 03:52:30[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 03:52:30[0m] Setting up language model...
[[34m2025-08-28 03:52:35[0m] Language model hidden size: 896
[[34m2025-08-28 03:52:35[0m] Setting up vision model (ViT)...
[[34m2025-08-28 03:52:35[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 03:52:35[0m] ViT max patches per side: 50
[[34m2025-08-28 03:52:35[0m] Setting up VAE model...
[[34m2025-08-28 03:52:36[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 03:52:36[0m] Max latent size: 48
[[34m2025-08-28 03:52:36[0m] Creating Bagel model...
[[34m2025-08-28 03:52:36[0m] Bagel model created successfully
[[34m2025-08-28 03:52:36[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 03:52:36[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:52:36[0m] Original connector input dim: 1152
[[34m2025-08-28 03:52:36[0m] Original connector output dim: 896
[[34m2025-08-28 03:52:36[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 03:52:36[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:52:36[0m] New connector input dim: 1152
[[34m2025-08-28 03:52:36[0m] New connector output dim: 896
[[34m2025-08-28 03:52:36[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 03:52:36[0m] New connector num output tokens: 1
[[34m2025-08-28 03:52:36[0m] Setting up tokenizer...
[[34m2025-08-28 03:52:36[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 03:52:36[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 03:52:36[0m] VAE model frozen
[[34m2025-08-28 03:52:36[0m] Setting up FSDP...
[[34m2025-08-28 03:52:40[0m] Model setup complete
[[34m2025-08-28 03:52:40[0m] === SETTING UP DATASET ===
[[34m2025-08-28 03:52:40[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 03:52:45[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 03:52:45[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 03:52:45[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 03:52:45[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 03:52:45[0m] Input sample_lens: list of length 25
[[34m2025-08-28 03:52:45[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 03:52:45[0m] Input split_lens: list of length 61
[[34m2025-08-28 03:52:45[0m] Input attn_modes: list of length 61
[[34m2025-08-28 03:52:45[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 03:52:45[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 03:52:45[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 03:52:45[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 03:52:45[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 03:52:45[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 03:52:45[0m] Sequence length: 16252
[[34m2025-08-28 03:52:45[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 03:52:45[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 03:52:45[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 03:52:45[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 03:52:45[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 03:52:45[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 03:52:45[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 03:52:45[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 03:52:45[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 03:52:45[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 03:52:45[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 03:52:45[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 03:52:45[0m] Max seqlen: 999
[[34m2025-08-28 03:52:45[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 03:52:45[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 03:52:45[0m] ViT output dtype: torch.float32
[[34m2025-08-28 03:52:45[0m] ViT output range: [-60.262207, 106.434052]
[[34m2025-08-28 03:52:45[0m] ViT output mean/std: 0.039570 / 2.179576
[[34m2025-08-28 03:52:45[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 03:52:45[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 03:52:45[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 03:52:45[0m] Projector num output tokens: 1
[[34m2025-08-28 03:52:45[0m] Projector input dim: 1152
[[34m2025-08-28 03:52:45[0m] Projector output dim: 896
[[34m2025-08-28 03:52:45[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 03:52:45[0m] Created simulated hidden states with 4 layers
[[34m2025-08-28 03:52:45[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:10:44[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:10:44[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:10:44[0m] Setting up language model...
[[34m2025-08-28 04:10:48[0m] Language model hidden size: 896
[[34m2025-08-28 04:10:48[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:10:49[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:10:49[0m] ViT max patches per side: 50
[[34m2025-08-28 04:10:49[0m] Setting up VAE model...
[[34m2025-08-28 04:10:49[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:10:49[0m] Max latent size: 48
[[34m2025-08-28 04:10:49[0m] Creating Bagel model...
[[34m2025-08-28 04:10:49[0m] Bagel model created successfully
[[34m2025-08-28 04:10:49[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:10:49[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:10:49[0m] Original connector input dim: 1152
[[34m2025-08-28 04:10:49[0m] Original connector output dim: 896
[[34m2025-08-28 04:10:49[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:10:49[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:10:49[0m] New connector input dim: 1152
[[34m2025-08-28 04:10:49[0m] New connector output dim: 896
[[34m2025-08-28 04:10:49[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:10:49[0m] New connector num output tokens: 1
[[34m2025-08-28 04:10:49[0m] Setting up tokenizer...
[[34m2025-08-28 04:10:50[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:10:50[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:10:50[0m] VAE model frozen
[[34m2025-08-28 04:10:50[0m] Setting up FSDP...
[[34m2025-08-28 04:10:52[0m] Model setup complete
[[34m2025-08-28 04:10:52[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:10:52[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:10:57[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:10:57[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:10:57[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:10:57[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:10:57[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:10:57[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:10:57[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:10:57[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:10:57[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:10:57[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:10:57[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:10:57[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:10:57[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:10:57[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:10:57[0m] Sequence length: 16252
[[34m2025-08-28 04:10:57[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:10:57[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:10:57[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:10:57[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:10:57[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:10:57[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:10:57[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:10:57[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:10:57[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:10:57[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:10:57[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:10:57[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:10:57[0m] Max seqlen: 999
[[34m2025-08-28 04:10:57[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:12:14[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:12:14[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:12:14[0m] Setting up language model...
[[34m2025-08-28 04:12:18[0m] Language model hidden size: 896
[[34m2025-08-28 04:12:18[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:12:19[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:12:19[0m] ViT max patches per side: 50
[[34m2025-08-28 04:12:19[0m] Setting up VAE model...
[[34m2025-08-28 04:12:19[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:12:19[0m] Max latent size: 48
[[34m2025-08-28 04:12:19[0m] Creating Bagel model...
[[34m2025-08-28 04:12:19[0m] Bagel model created successfully
[[34m2025-08-28 04:12:19[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:12:19[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:12:19[0m] Original connector input dim: 1152
[[34m2025-08-28 04:12:19[0m] Original connector output dim: 896
[[34m2025-08-28 04:12:19[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:12:19[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:12:19[0m] New connector input dim: 1152
[[34m2025-08-28 04:12:19[0m] New connector output dim: 896
[[34m2025-08-28 04:12:19[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:12:19[0m] New connector num output tokens: 1
[[34m2025-08-28 04:12:19[0m] Setting up tokenizer...
[[34m2025-08-28 04:12:20[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:12:20[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:12:20[0m] VAE model frozen
[[34m2025-08-28 04:12:20[0m] Setting up FSDP...
[[34m2025-08-28 04:12:22[0m] Model setup complete
[[34m2025-08-28 04:12:22[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:12:22[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:12:28[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:12:28[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:12:28[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:12:28[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:12:28[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:12:28[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:12:28[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:12:28[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:12:28[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:12:28[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:12:28[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:12:28[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:12:28[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:12:28[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:12:28[0m] Sequence length: 16252
[[34m2025-08-28 04:12:28[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:12:28[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:12:28[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:12:28[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:12:28[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:12:28[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:12:28[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:12:28[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:12:28[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:12:28[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:12:28[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:12:28[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:12:28[0m] Max seqlen: 999
[[34m2025-08-28 04:12:28[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:14:04[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:14:05[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:14:05[0m] Setting up language model...
[[34m2025-08-28 04:14:09[0m] Language model hidden size: 896
[[34m2025-08-28 04:14:09[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:14:10[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:14:10[0m] ViT max patches per side: 50
[[34m2025-08-28 04:14:10[0m] Setting up VAE model...
[[34m2025-08-28 04:14:10[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:14:10[0m] Max latent size: 48
[[34m2025-08-28 04:14:10[0m] Creating Bagel model...
[[34m2025-08-28 04:14:10[0m] Bagel model created successfully
[[34m2025-08-28 04:14:10[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:14:10[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:14:10[0m] Original connector input dim: 1152
[[34m2025-08-28 04:14:10[0m] Original connector output dim: 896
[[34m2025-08-28 04:14:10[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:14:10[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:14:10[0m] New connector input dim: 1152
[[34m2025-08-28 04:14:10[0m] New connector output dim: 896
[[34m2025-08-28 04:14:10[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:14:10[0m] New connector num output tokens: 1
[[34m2025-08-28 04:14:10[0m] Setting up tokenizer...
[[34m2025-08-28 04:14:11[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:14:11[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:14:11[0m] VAE model frozen
[[34m2025-08-28 04:14:11[0m] Setting up FSDP...
[[34m2025-08-28 04:14:14[0m] Model setup complete
[[34m2025-08-28 04:14:14[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:14:14[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:14:19[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:14:19[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:14:19[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:14:19[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:14:19[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:14:19[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:14:19[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:14:19[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:14:19[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:14:19[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:14:19[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:14:19[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:14:19[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:14:19[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:14:19[0m] Sequence length: 16252
[[34m2025-08-28 04:14:19[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:14:19[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:14:19[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:14:19[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:14:19[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:14:19[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:14:19[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:14:19[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:14:19[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:14:19[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:14:19[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:14:19[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:14:19[0m] Max seqlen: 999
[[34m2025-08-28 04:14:19[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:14:19[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 04:18:49[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:18:49[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:18:49[0m] Setting up language model...
[[34m2025-08-28 04:18:54[0m] Language model hidden size: 896
[[34m2025-08-28 04:18:54[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:18:54[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:18:54[0m] ViT max patches per side: 50
[[34m2025-08-28 04:18:54[0m] Setting up VAE model...
[[34m2025-08-28 04:18:55[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:18:55[0m] Max latent size: 48
[[34m2025-08-28 04:18:55[0m] Creating Bagel model...
[[34m2025-08-28 04:18:55[0m] Bagel model created successfully
[[34m2025-08-28 04:18:55[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:18:55[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:18:55[0m] Original connector input dim: 1152
[[34m2025-08-28 04:18:55[0m] Original connector output dim: 896
[[34m2025-08-28 04:18:55[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:18:55[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:18:55[0m] New connector input dim: 1152
[[34m2025-08-28 04:18:55[0m] New connector output dim: 896
[[34m2025-08-28 04:18:55[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:18:55[0m] New connector num output tokens: 1
[[34m2025-08-28 04:18:55[0m] Setting up tokenizer...
[[34m2025-08-28 04:18:55[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:18:55[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:18:55[0m] VAE model frozen
[[34m2025-08-28 04:18:55[0m] Setting up FSDP...
[[34m2025-08-28 04:18:58[0m] Model setup complete
[[34m2025-08-28 04:18:58[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:18:58[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:19:03[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:19:03[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:19:03[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:19:03[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:19:03[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:19:03[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:19:03[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:19:03[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:19:03[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:19:03[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:19:03[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:03[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:19:03[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:19:03[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:19:03[0m] Sequence length: 16252
[[34m2025-08-28 04:19:03[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:19:03[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:19:03[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:19:03[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:19:03[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:19:03[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:19:03[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:19:03[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:19:03[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:19:03[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:19:03[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:19:03[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:19:03[0m] Max seqlen: 999
[[34m2025-08-28 04:19:03[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:19:04[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 04:19:04[0m] ViT output dtype: torch.float32
[[34m2025-08-28 04:19:04[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:19:04[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:19:04[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:19:04[0m] Projector num output tokens: 1
[[34m2025-08-28 04:19:04[0m] Projector input dim: 1152
[[34m2025-08-28 04:19:04[0m] Projector output dim: 896
[[34m2025-08-28 04:19:04[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:19:33[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:19:33[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:19:33[0m] Setting up language model...
[[34m2025-08-28 04:19:38[0m] Language model hidden size: 896
[[34m2025-08-28 04:19:38[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:19:38[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:19:38[0m] ViT max patches per side: 50
[[34m2025-08-28 04:19:38[0m] Setting up VAE model...
[[34m2025-08-28 04:19:39[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:19:39[0m] Max latent size: 48
[[34m2025-08-28 04:19:39[0m] Creating Bagel model...
[[34m2025-08-28 04:19:39[0m] Bagel model created successfully
[[34m2025-08-28 04:19:39[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:19:39[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:19:39[0m] Original connector input dim: 1152
[[34m2025-08-28 04:19:39[0m] Original connector output dim: 896
[[34m2025-08-28 04:19:39[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:19:39[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:19:39[0m] New connector input dim: 1152
[[34m2025-08-28 04:19:39[0m] New connector output dim: 896
[[34m2025-08-28 04:19:39[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:19:39[0m] New connector num output tokens: 1
[[34m2025-08-28 04:19:39[0m] Setting up tokenizer...
[[34m2025-08-28 04:19:39[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:19:39[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:19:39[0m] VAE model frozen
[[34m2025-08-28 04:19:39[0m] Setting up FSDP...
[[34m2025-08-28 04:19:43[0m] Model setup complete
[[34m2025-08-28 04:19:43[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:19:43[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:19:48[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:19:48[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:19:48[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:19:48[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:19:48[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:19:48[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:19:48[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:19:48[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:19:48[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:19:48[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:19:48[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:19:48[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:19:48[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:19:48[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:19:48[0m] Sequence length: 16252
[[34m2025-08-28 04:19:48[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:19:48[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:19:48[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:19:48[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:19:48[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:19:48[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:19:48[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:19:48[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:19:48[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:19:48[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:19:48[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:19:48[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:19:48[0m] Max seqlen: 999
[[34m2025-08-28 04:19:48[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:19:48[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 04:19:48[0m] ViT output dtype: torch.float32
[[34m2025-08-28 04:19:48[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:19:48[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:19:48[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:19:48[0m] Projector num output tokens: 1
[[34m2025-08-28 04:19:48[0m] Projector input dim: 1152
[[34m2025-08-28 04:19:48[0m] Projector output dim: 896
[[34m2025-08-28 04:19:48[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:19:48[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:23:05[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:23:05[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:23:05[0m] Setting up language model...
[[34m2025-08-28 04:23:10[0m] Language model hidden size: 896
[[34m2025-08-28 04:23:10[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:23:10[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:23:10[0m] ViT max patches per side: 50
[[34m2025-08-28 04:23:10[0m] Setting up VAE model...
[[34m2025-08-28 04:23:11[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:23:11[0m] Max latent size: 48
[[34m2025-08-28 04:23:11[0m] Creating Bagel model...
[[34m2025-08-28 04:23:11[0m] Bagel model created successfully
[[34m2025-08-28 04:23:11[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:23:11[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:23:11[0m] Original connector input dim: 1152
[[34m2025-08-28 04:23:11[0m] Original connector output dim: 896
[[34m2025-08-28 04:23:11[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:23:11[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:23:11[0m] New connector input dim: 1152
[[34m2025-08-28 04:23:11[0m] New connector output dim: 896
[[34m2025-08-28 04:23:11[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:23:11[0m] New connector num output tokens: 1
[[34m2025-08-28 04:23:11[0m] Setting up tokenizer...
[[34m2025-08-28 04:23:11[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:23:11[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:23:11[0m] VAE model frozen
[[34m2025-08-28 04:23:11[0m] Setting up FSDP...
[[34m2025-08-28 04:23:14[0m] Model setup complete
[[34m2025-08-28 04:23:14[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:23:14[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:23:19[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:23:19[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:23:19[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:23:19[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:23:19[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:23:19[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:23:19[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:23:19[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:23:19[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:23:19[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:23:19[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:23:19[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:23:19[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:23:19[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:23:19[0m] Sequence length: 16252
[[34m2025-08-28 04:23:19[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:23:19[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:23:19[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:23:19[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:23:19[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:23:19[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:23:19[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:23:19[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:23:19[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:23:19[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:23:19[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:23:19[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:23:19[0m] Max seqlen: 999
[[34m2025-08-28 04:23:19[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:24:30[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:24:30[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:24:30[0m] Setting up language model...
[[34m2025-08-28 04:24:35[0m] Language model hidden size: 896
[[34m2025-08-28 04:24:35[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:24:35[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:24:35[0m] ViT max patches per side: 50
[[34m2025-08-28 04:24:35[0m] Setting up VAE model...
[[34m2025-08-28 04:24:36[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:24:36[0m] Max latent size: 48
[[34m2025-08-28 04:24:36[0m] Creating Bagel model...
[[34m2025-08-28 04:24:36[0m] Bagel model created successfully
[[34m2025-08-28 04:24:36[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:24:36[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:24:36[0m] Original connector input dim: 1152
[[34m2025-08-28 04:24:36[0m] Original connector output dim: 896
[[34m2025-08-28 04:24:36[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:24:36[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:24:36[0m] New connector input dim: 1152
[[34m2025-08-28 04:24:36[0m] New connector output dim: 896
[[34m2025-08-28 04:24:36[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:24:36[0m] New connector num output tokens: 1
[[34m2025-08-28 04:24:36[0m] Setting up tokenizer...
[[34m2025-08-28 04:24:36[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:24:36[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:24:36[0m] VAE model frozen
[[34m2025-08-28 04:24:36[0m] Setting up FSDP...
[[34m2025-08-28 04:24:39[0m] Model setup complete
[[34m2025-08-28 04:24:39[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:24:39[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:24:44[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:24:44[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:24:44[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:24:44[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:24:44[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:24:44[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:24:44[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:24:44[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:24:44[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:24:44[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:24:44[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:24:44[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:24:44[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:24:44[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:24:44[0m] Sequence length: 16252
[[34m2025-08-28 04:24:44[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:24:44[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:24:44[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:24:44[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:24:44[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:24:44[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:24:44[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:24:44[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:24:44[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:24:44[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:24:44[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:24:44[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:24:44[0m] Max seqlen: 999
[[34m2025-08-28 04:24:44[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:24:44[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 04:24:44[0m] ViT output dtype: torch.bfloat16
[[34m2025-08-28 04:24:44[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:24:44[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:24:44[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:24:44[0m] Projector num output tokens: 1
[[34m2025-08-28 04:24:44[0m] Projector input dim: 1152
[[34m2025-08-28 04:24:44[0m] Projector output dim: 896
[[34m2025-08-28 04:24:44[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:24:44[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:27:42[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:27:42[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:27:42[0m] Setting up language model...
[[34m2025-08-28 04:27:47[0m] Language model hidden size: 896
[[34m2025-08-28 04:27:47[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:27:47[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:27:47[0m] ViT max patches per side: 50
[[34m2025-08-28 04:27:47[0m] Setting up VAE model...
[[34m2025-08-28 04:27:47[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:27:47[0m] Max latent size: 48
[[34m2025-08-28 04:27:47[0m] Creating Bagel model...
[[34m2025-08-28 04:27:48[0m] Bagel model created successfully
[[34m2025-08-28 04:27:48[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:27:48[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:27:48[0m] Original connector input dim: 1152
[[34m2025-08-28 04:27:48[0m] Original connector output dim: 896
[[34m2025-08-28 04:27:48[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:27:48[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:27:48[0m] New connector input dim: 1152
[[34m2025-08-28 04:27:48[0m] New connector output dim: 896
[[34m2025-08-28 04:27:48[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:27:48[0m] New connector num output tokens: 1
[[34m2025-08-28 04:27:48[0m] Setting up tokenizer...
[[34m2025-08-28 04:27:48[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:27:48[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:27:48[0m] VAE model frozen
[[34m2025-08-28 04:27:48[0m] Setting up FSDP...
[[34m2025-08-28 04:27:51[0m] Model setup complete
[[34m2025-08-28 04:27:51[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:27:51[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:27:55[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:27:55[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:27:55[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:27:55[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:27:55[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:27:55[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:27:55[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:27:55[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:27:55[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:27:55[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:27:55[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:27:55[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:27:55[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:27:55[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:27:55[0m] Sequence length: 16252
[[34m2025-08-28 04:27:55[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:27:55[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:27:55[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:27:55[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:27:55[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:27:56[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:27:56[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:27:56[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:27:56[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:27:56[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:27:56[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:27:56[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:27:56[0m] Max seqlen: 999
[[34m2025-08-28 04:27:56[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:27:56[0m] ViT output shape: torch.Size([7503, 1152])
[[34m2025-08-28 04:27:56[0m] ViT output dtype: torch.bfloat16
[[34m2025-08-28 04:27:56[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:27:56[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:27:56[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:27:56[0m] Projector num output tokens: 1
[[34m2025-08-28 04:27:56[0m] Projector input dim: 1152
[[34m2025-08-28 04:27:56[0m] Projector output dim: 896
[[34m2025-08-28 04:27:56[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:27:56[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:48:48[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:48:48[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:48:48[0m] Setting up language model...
[[34m2025-08-28 04:48:53[0m] Language model hidden size: 896
[[34m2025-08-28 04:48:53[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:48:53[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:48:53[0m] ViT max patches per side: 50
[[34m2025-08-28 04:48:53[0m] Setting up VAE model...
[[34m2025-08-28 04:48:53[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:48:53[0m] Max latent size: 48
[[34m2025-08-28 04:48:53[0m] Creating Bagel model...
[[34m2025-08-28 04:48:54[0m] Bagel model created successfully
[[34m2025-08-28 04:48:54[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:48:54[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:48:54[0m] Original connector input dim: 1152
[[34m2025-08-28 04:48:54[0m] Original connector output dim: 896
[[34m2025-08-28 04:48:54[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:48:54[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:48:54[0m] New connector input dim: 1152
[[34m2025-08-28 04:48:54[0m] New connector output dim: 896
[[34m2025-08-28 04:48:54[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:48:54[0m] New connector num output tokens: 1
[[34m2025-08-28 04:48:54[0m] Setting up tokenizer...
[[34m2025-08-28 04:48:54[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:48:54[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:48:54[0m] VAE model frozen
[[34m2025-08-28 04:48:54[0m] Setting up FSDP...
[[34m2025-08-28 04:48:57[0m] Model setup complete
[[34m2025-08-28 04:48:57[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:48:57[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:49:01[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:49:01[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:49:01[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:49:01[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:49:01[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:49:01[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:49:01[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:49:01[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:49:01[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:49:01[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:49:01[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:49:01[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:49:02[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:49:02[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:49:02[0m] Sequence length: 16252
[[34m2025-08-28 04:49:02[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:49:02[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:49:02[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:49:02[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:49:02[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:49:02[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:49:02[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:49:02[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:49:02[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:49:02[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:49:02[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:49:02[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:49:02[0m] Max seqlen: 999
[[34m2025-08-28 04:49:02[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:51:14[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:51:14[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:51:14[0m] Setting up language model...
[[34m2025-08-28 04:51:19[0m] Language model hidden size: 896
[[34m2025-08-28 04:51:19[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:51:19[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:51:19[0m] ViT max patches per side: 50
[[34m2025-08-28 04:51:19[0m] Setting up VAE model...
[[34m2025-08-28 04:51:20[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:51:20[0m] Max latent size: 48
[[34m2025-08-28 04:51:20[0m] Creating Bagel model...
[[34m2025-08-28 04:51:20[0m] Bagel model created successfully
[[34m2025-08-28 04:51:20[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:51:20[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:51:20[0m] Original connector input dim: 1152
[[34m2025-08-28 04:51:20[0m] Original connector output dim: 896
[[34m2025-08-28 04:51:20[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:51:20[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:51:20[0m] New connector input dim: 1152
[[34m2025-08-28 04:51:20[0m] New connector output dim: 896
[[34m2025-08-28 04:51:20[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:51:20[0m] New connector num output tokens: 1
[[34m2025-08-28 04:51:20[0m] Setting up tokenizer...
[[34m2025-08-28 04:51:20[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:51:20[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:51:20[0m] VAE model frozen
[[34m2025-08-28 04:51:20[0m] Setting up FSDP...
[[34m2025-08-28 04:51:23[0m] Model setup complete
[[34m2025-08-28 04:51:23[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:51:23[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:51:28[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:51:28[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:51:28[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:51:28[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:51:28[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:51:28[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:51:28[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:51:28[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:51:28[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:51:28[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:51:28[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:51:28[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:51:28[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:51:28[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:51:28[0m] Sequence length: 16252
[[34m2025-08-28 04:51:28[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:51:28[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:51:28[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:51:28[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:51:28[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:51:28[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:51:28[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:51:28[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:51:28[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:51:28[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:51:28[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:51:28[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:51:28[0m] Max seqlen: 999
[[34m2025-08-28 04:51:28[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:51:28[0m] ViT output layers: 27
[[34m2025-08-28 04:51:28[0m] Layer 1: tensor([[ 1.0859e+00,  6.2500e-01,  5.3516e-01,  ...,  7.7344e-01,
          1.6250e+00,  5.6250e-01],
        [-1.8750e-01, -2.6367e-02, -8.6914e-02,  ...,  1.0693e-01,
          1.2266e+00,  1.3867e-01],
        [ 1.2207e-03, -3.8086e-02, -7.2754e-02,  ...,  6.2500e-02,
          1.2109e+00,  1.0938e-01],
        ...,
        [-7.5195e-02,  9.0820e-02, -3.0518e-02,  ...,  2.4805e-01,
          3.4961e-01,  9.5703e-02],
        [ 3.4570e-01,  1.6895e-01, -5.2979e-02,  ...,  1.0059e-01,
          3.5742e-01,  2.5391e-02],
        [ 8.5449e-02,  3.1250e-02, -8.6426e-02,  ...,  2.9785e-02,
          4.4922e-01,  8.9844e-02]], device='cuda:0', dtype=torch.bfloat16)
[[34m2025-08-28 04:51:28[0m] Layer 2: tensor([[ 1.3750,  0.3848,  1.2500,  ...,  0.6602,  1.6328,  0.2246],
        [-0.2207, -0.2354,  0.2559,  ...,  0.2314,  1.2891,  0.4727],
        [ 0.0542,  0.1221,  0.1670,  ...,  0.2734,  1.6250, -0.0996],
        ...,
        [ 0.5078,  0.1572,  0.5781,  ...,  0.3281,  1.0547,  0.0527],
        [ 0.4766,  0.4102, -0.1172,  ...,  0.1592,  0.3574,  0.2207],
        [ 0.3633,  0.0056,  0.1152,  ...,  0.7344,  0.6992,  0.6953]],
       device='cuda:0', dtype=torch.bfloat16)
[[34m2025-08-28 04:52:18[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:52:18[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:52:18[0m] Setting up language model...
[[34m2025-08-28 04:52:23[0m] Language model hidden size: 896
[[34m2025-08-28 04:52:23[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:52:23[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:52:23[0m] ViT max patches per side: 50
[[34m2025-08-28 04:52:23[0m] Setting up VAE model...
[[34m2025-08-28 04:52:24[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:52:24[0m] Max latent size: 48
[[34m2025-08-28 04:52:24[0m] Creating Bagel model...
[[34m2025-08-28 04:52:24[0m] Bagel model created successfully
[[34m2025-08-28 04:52:24[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:52:24[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:52:24[0m] Original connector input dim: 1152
[[34m2025-08-28 04:52:24[0m] Original connector output dim: 896
[[34m2025-08-28 04:52:24[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:52:24[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:52:24[0m] New connector input dim: 1152
[[34m2025-08-28 04:52:24[0m] New connector output dim: 896
[[34m2025-08-28 04:52:24[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:52:24[0m] New connector num output tokens: 1
[[34m2025-08-28 04:52:24[0m] Setting up tokenizer...
[[34m2025-08-28 04:52:24[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:52:24[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:52:24[0m] VAE model frozen
[[34m2025-08-28 04:52:24[0m] Setting up FSDP...
[[34m2025-08-28 04:52:27[0m] Model setup complete
[[34m2025-08-28 04:52:27[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:52:27[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:52:32[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:52:32[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:52:32[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:52:32[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:52:32[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:52:32[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:52:32[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:52:32[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:52:32[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:52:32[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:52:32[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:52:32[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:52:33[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:52:33[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:52:33[0m] Sequence length: 16252
[[34m2025-08-28 04:52:33[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:52:33[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:52:33[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:52:33[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:52:33[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:52:33[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:52:33[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:52:33[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:52:33[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:52:33[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:52:33[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:52:33[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:52:33[0m] Max seqlen: 999
[[34m2025-08-28 04:52:33[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:52:33[0m] ViT output layers: 27
[[34m2025-08-28 04:52:33[0m] Layer 1: torch.Size([7503, 1152])
[[34m2025-08-28 04:52:33[0m] Layer 2: torch.Size([7503, 1152])
[[34m2025-08-28 04:52:33[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:52:33[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:52:33[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:52:33[0m] Projector num output tokens: 1
[[34m2025-08-28 04:52:33[0m] Projector input dim: 1152
[[34m2025-08-28 04:52:33[0m] Projector output dim: 896
[[34m2025-08-28 04:52:33[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:52:33[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:53:27[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:53:27[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:53:27[0m] Setting up language model...
[[34m2025-08-28 04:53:32[0m] Language model hidden size: 896
[[34m2025-08-28 04:53:32[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:53:32[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:53:32[0m] ViT max patches per side: 50
[[34m2025-08-28 04:53:32[0m] Setting up VAE model...
[[34m2025-08-28 04:53:33[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:53:33[0m] Max latent size: 48
[[34m2025-08-28 04:53:33[0m] Creating Bagel model...
[[34m2025-08-28 04:53:33[0m] Bagel model created successfully
[[34m2025-08-28 04:53:33[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:53:33[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:53:33[0m] Original connector input dim: 1152
[[34m2025-08-28 04:53:33[0m] Original connector output dim: 896
[[34m2025-08-28 04:53:33[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:53:33[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:53:33[0m] New connector input dim: 1152
[[34m2025-08-28 04:53:33[0m] New connector output dim: 896
[[34m2025-08-28 04:53:33[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:53:33[0m] New connector num output tokens: 1
[[34m2025-08-28 04:53:33[0m] Setting up tokenizer...
[[34m2025-08-28 04:53:33[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:53:33[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:53:33[0m] VAE model frozen
[[34m2025-08-28 04:53:33[0m] Setting up FSDP...
[[34m2025-08-28 04:53:36[0m] Model setup complete
[[34m2025-08-28 04:53:36[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:53:36[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:53:41[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:53:41[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:53:41[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:53:41[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:53:41[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:53:41[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:53:41[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:53:41[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:53:41[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:53:41[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:53:41[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:53:41[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:53:41[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:53:41[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:53:41[0m] Sequence length: 16252
[[34m2025-08-28 04:53:41[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:53:41[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:53:41[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:53:41[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:53:41[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:53:41[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:53:41[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:53:41[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:53:41[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:53:41[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:53:41[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:53:41[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:53:41[0m] Max seqlen: 999
[[34m2025-08-28 04:53:41[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:53:41[0m] ViT output layers: 27
[[34m2025-08-28 04:53:41[0m] Layer 1: torch.Size([7503, 1152])
[[34m2025-08-28 04:53:41[0m] Layer 2: torch.Size([7503, 1152])
[[34m2025-08-28 04:53:41[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:53:41[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:53:41[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:53:41[0m] Projector num output tokens: 1
[[34m2025-08-28 04:53:41[0m] Projector input dim: 1152
[[34m2025-08-28 04:53:41[0m] Projector output dim: 896
[[34m2025-08-28 04:53:41[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:53:41[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:53:41[0m] After ViLex projector shape: torch.Size([1, 896])
[[34m2025-08-28 04:53:41[0m] After ViLex projector dtype: torch.bfloat16
[[34m2025-08-28 04:53:41[0m] === VILEX PROJECTOR TRANSFORMATION ANALYSIS ===
[[34m2025-08-28 04:53:41[0m] Dimension change: 1152 -> 896
[[34m2025-08-28 04:53:41[0m] Sequence length change: 7503 -> 1
[[34m2025-08-28 04:53:41[0m] Input norm: mean=40.904427, std=21.243977
[[34m2025-08-28 04:53:41[0m] Output norm: mean=2.435223, std=nan
[[34m2025-08-28 04:53:41[0m] ViLex projector applied attention pooling: 7503 -> 1 tokens
[[34m2025-08-28 04:53:41[0m] Pooling ratio: 0.000
[[34m2025-08-28 04:53:41[0m] === SAMPLE VECTOR ANALYSIS ===
[[34m2025-08-28 04:53:41[0m] First token - post-projector (first 10): tensor([ 0.0977,  0.0850, -0.1875, -0.0295, -0.1133, -0.1279, -0.0664, -0.0549,
         0.0579, -0.0078], device='cuda:0', dtype=torch.bfloat16)
[[34m2025-08-28 04:53:41[0m] Post-projector - Contains NaN: False, Contains Inf: False
[[34m2025-08-28 04:53:41[0m] Adjusting position IDs for pooled output...
[[34m2025-08-28 04:53:41[0m] ViT position embeddings shape: torch.Size([1, 896])
[[34m2025-08-28 04:53:41[0m] ViT position embeddings range: [0.000000, 1.000000]
[[34m2025-08-28 04:53:41[0m] ViT position embeddings mean/std: 0.500000 / 0.500000
[[34m2025-08-28 04:53:41[0m] ViT final embeddings shape: torch.Size([1, 896])
[[34m2025-08-28 04:53:41[0m] ViT final embeddings range: [-0.269531, 1.210938]
[[34m2025-08-28 04:53:41[0m] ViT final embeddings mean/std: 0.498047 / 0.500000
[[34m2025-08-28 04:53:41[0m] === POSITION EMBEDDING EFFECT ===
[[34m2025-08-28 04:53:41[0m] Position effect range: [0.000000, 1.000000]
[[34m2025-08-28 04:53:41[0m] Position effect mean/std: 0.500000 / 0.500000
[[34m2025-08-28 04:53:41[0m] Adjusting token indexes for pooled output...
[[34m2025-08-28 04:58:14[0m] === SETTING UP MODEL COMPONENTS ===
[[34m2025-08-28 04:58:14[0m] Loaded LLM config from pretrained path
[[34m2025-08-28 04:58:14[0m] Setting up language model...
[[34m2025-08-28 04:58:19[0m] Language model hidden size: 896
[[34m2025-08-28 04:58:19[0m] Setting up vision model (ViT)...
[[34m2025-08-28 04:58:19[0m] ViT config - patch size: 14, hidden size: 1152
[[34m2025-08-28 04:58:19[0m] ViT max patches per side: 50
[[34m2025-08-28 04:58:19[0m] Setting up VAE model...
[[34m2025-08-28 04:58:20[0m] VAE config - downsample: 8, z_channels: 16
[[34m2025-08-28 04:58:20[0m] Max latent size: 48
[[34m2025-08-28 04:58:20[0m] Creating Bagel model...
[[34m2025-08-28 04:58:20[0m] Bagel model created successfully
[[34m2025-08-28 04:58:20[0m] === REPLACING CONNECTOR WITH VILEX PROJECTOR ===
[[34m2025-08-28 04:58:20[0m] Original connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:58:20[0m] Original connector input dim: 1152
[[34m2025-08-28 04:58:20[0m] Original connector output dim: 896
[[34m2025-08-28 04:58:20[0m] Replaced original connector with ViLex MultiLayerAttentionPoolingProjector
[[34m2025-08-28 04:58:20[0m] New connector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:58:20[0m] New connector input dim: 1152
[[34m2025-08-28 04:58:20[0m] New connector output dim: 896
[[34m2025-08-28 04:58:20[0m] New connector layer indices: [1, 2, 3]
[[34m2025-08-28 04:58:20[0m] New connector num output tokens: 1
[[34m2025-08-28 04:58:20[0m] Setting up tokenizer...
[[34m2025-08-28 04:58:20[0m] Tokenizer vocab size: 151665
[[34m2025-08-28 04:58:20[0m] New token IDs: {'bos_token_id': 151644, 'eos_token_id': 151645, 'start_of_image': 151652, 'end_of_image': 151653}
[[34m2025-08-28 04:58:20[0m] VAE model frozen
[[34m2025-08-28 04:58:20[0m] Setting up FSDP...
[[34m2025-08-28 04:58:23[0m] Model setup complete
[[34m2025-08-28 04:58:23[0m] === SETTING UP DATASET ===
[[34m2025-08-28 04:58:23[0m] === STARTING FORWARD PASS COMPONENT TEST ===
[[34m2025-08-28 04:58:28[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:58:28[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:58:28[0m] === INPUT DATA ANALYSIS ===
[[34m2025-08-28 04:58:28[0m] Input sequence_length: <class 'int'> = 16252
[[34m2025-08-28 04:58:28[0m] Input sample_lens: list of length 25
[[34m2025-08-28 04:58:28[0m] Input packed_text_ids: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_text_indexes: shape=torch.Size([1837]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_position_ids: shape=torch.Size([16252]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input batch_data_indexes: list of length 24
[[34m2025-08-28 04:58:28[0m] Input split_lens: list of length 61
[[34m2025-08-28 04:58:28[0m] Input attn_modes: list of length 61
[[34m2025-08-28 04:58:28[0m] Input padded_images: shape=torch.Size([12, 3, 512, 288]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input patchified_vae_latent_shapes: list of length 12
[[34m2025-08-28 04:58:28[0m] Input packed_latent_position_ids: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_vae_token_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_vit_tokens: shape=torch.Size([7503, 588]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_vit_position_ids: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_vit_token_indexes: shape=torch.Size([7503]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input vit_token_seqlens: shape=torch.Size([12]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m]   Values: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:58:28[0m] Input packed_timesteps: shape=torch.Size([6912]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input mse_loss_indexes: shape=torch.Size([6912]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input packed_label_ids: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input ce_loss_indexes: shape=torch.Size([1010]), dtype=torch.int64, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Input ce_loss_weights: shape=torch.Size([1010]), dtype=torch.float32, device=cuda:0
[[34m2025-08-28 04:58:28[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 9], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 1, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:58:28[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.1961, 0.1961, 0.1961], device='cuda:0')
[[34m2025-08-28 04:58:28[0m] === COMPONENT 1: TEXT EMBEDDING ===
[[34m2025-08-28 04:58:28[0m] Sequence length: 16252
[[34m2025-08-28 04:58:28[0m] Packed text IDs shape: torch.Size([1837])
[[34m2025-08-28 04:58:28[0m] Packed text indexes shape: torch.Size([1837])
[[34m2025-08-28 04:58:28[0m] Text embeddings shape: torch.Size([1837, 896])
[[34m2025-08-28 04:58:28[0m] Text embeddings dtype: torch.float32
[[34m2025-08-28 04:58:28[0m] Initial packed sequence shape: torch.Size([16252, 896])
[[34m2025-08-28 04:58:28[0m] Non-zero elements in sequence: 1645952
[[34m2025-08-28 04:58:28[0m] === COMPONENT 2: VISION UNDERSTANDING (VIT) ===
[[34m2025-08-28 04:58:28[0m] ViT tokens shape: torch.Size([7503, 588])
[[34m2025-08-28 04:58:28[0m] ViT token indexes shape: torch.Size([7503])
[[34m2025-08-28 04:58:28[0m] ViT position IDs shape: torch.Size([7503])
[[34m2025-08-28 04:58:28[0m] ViT token seq lengths: tensor([728, 407, 333, 851, 690,  81, 121, 925, 592, 999, 888, 888],
       device='cuda:0')
[[34m2025-08-28 04:58:28[0m] CU seqlens: tensor([   0,  728, 1135, 1468, 2319, 3009, 3090, 3211, 4136, 4728, 5727, 6615,
        7503], device='cuda:0', dtype=torch.int32)
[[34m2025-08-28 04:58:28[0m] Max seqlen: 999
[[34m2025-08-28 04:58:28[0m] Running ViT forward pass for ViLex projector...
[[34m2025-08-28 04:58:28[0m] ViT output layers: 27
[[34m2025-08-28 04:58:28[0m] Layer 1: torch.Size([7503, 1152])
[[34m2025-08-28 04:58:28[0m] Layer 2: torch.Size([7503, 1152])
[[34m2025-08-28 04:58:28[0m] === VILEX PROJECTOR ANALYSIS ===
[[34m2025-08-28 04:58:28[0m] Projector type: <class 'vilex.projectors.MultiLayerAttentionPoolingProjector'>
[[34m2025-08-28 04:58:28[0m] Projector layer indices: [1, 2, 3]
[[34m2025-08-28 04:58:28[0m] Projector num output tokens: 1
[[34m2025-08-28 04:58:28[0m] Projector input dim: 1152
[[34m2025-08-28 04:58:28[0m] Projector output dim: 896
[[34m2025-08-28 04:58:28[0m] Preparing input for ViLex projector (simulating multi-layer output)...
[[34m2025-08-28 04:58:28[0m] Applying ViLex projector transformation...
[[34m2025-08-28 04:58:28[0m] After ViLex projector shape: torch.Size([1, 896])
[[34m2025-08-28 04:58:28[0m] After ViLex projector dtype: torch.bfloat16
[[34m2025-08-28 04:58:28[0m] === VILEX PROJECTOR TRANSFORMATION ANALYSIS ===
[[34m2025-08-28 04:58:28[0m] Dimension change: 1152 -> 896
[[34m2025-08-28 04:58:28[0m] Sequence length change: 7503 -> 1
[[34m2025-08-28 04:58:28[0m] Input norm: mean=40.904427, std=21.243977
[[34m2025-08-28 04:58:28[0m] Output norm: mean=2.435223, std=nan
[[34m2025-08-28 04:58:28[0m] ViLex projector applied attention pooling: 7503 -> 1 tokens
[[34m2025-08-28 04:58:28[0m] Pooling ratio: 0.000
[[34m2025-08-28 04:58:28[0m] === SAMPLE VECTOR ANALYSIS ===
[[34m2025-08-28 04:58:28[0m] First token - post-projector (first 10): tensor([ 0.0977,  0.0850, -0.1875, -0.0295, -0.1133, -0.1279, -0.0664, -0.0549,
         0.0579, -0.0078], device='cuda:0', dtype=torch.bfloat16)
[[34m2025-08-28 04:58:28[0m] Post-projector - Contains NaN: False, Contains Inf: False
[[34m2025-08-28 04:58:28[0m] Adjusting position IDs for pooled output...
[[34m2025-08-28 04:58:28[0m] ViT position embeddings shape: torch.Size([1, 896])
[[34m2025-08-28 04:58:28[0m] ViT position embeddings range: [0.000000, 1.000000]
[[34m2025-08-28 04:58:28[0m] ViT position embeddings mean/std: 0.500000 / 0.500000
[[34m2025-08-28 04:58:28[0m] ViT final embeddings shape: torch.Size([1, 896])
[[34m2025-08-28 04:58:28[0m] ViT final embeddings range: [-0.269531, 1.210938]
[[34m2025-08-28 04:58:28[0m] ViT final embeddings mean/std: 0.498047 / 0.500000
[[34m2025-08-28 04:58:28[0m] === POSITION EMBEDDING EFFECT ===
[[34m2025-08-28 04:58:28[0m] Position effect range: [0.000000, 1.000000]
[[34m2025-08-28 04:58:28[0m] Position effect mean/std: 0.500000 / 0.500000
[[34m2025-08-28 04:58:28[0m] Adjusting token indexes for pooled output...
[[34m2025-08-28 04:58:28[0m] Sequence after ViT insertion - non-zero elements: 1646848
[[34m2025-08-28 04:58:28[0m] === COMPONENT 3: VISUAL GENERATION (VAE) ===
[[34m2025-08-28 04:58:28[0m] Padded images shape: torch.Size([12, 3, 512, 288])
[[34m2025-08-28 04:58:28[0m] VAE latent shapes: [(32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18)]
[[34m2025-08-28 04:58:28[0m] Latent position IDs shape: torch.Size([6912])
[[34m2025-08-28 04:58:28[0m] VAE token indexes shape: torch.Size([6912])
[[34m2025-08-28 04:58:28[0m] Timesteps shape: torch.Size([6912])
[[34m2025-08-28 04:58:29[0m] VAE encoded latent shape: torch.Size([12, 16, 64, 36])
[[34m2025-08-28 04:58:29[0m] VAE encoded latent dtype: torch.float32
[[34m2025-08-28 04:58:29[0m] Latent patch size: 2
[[34m2025-08-28 04:58:29[0m] Processing latent 0: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 0 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 1: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 1 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 2: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 2 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 3: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 3 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 4: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 4 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 5: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 5 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 6: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 6 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 7: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 7 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 8: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 8 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 9: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 9 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 10: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 10 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Processing latent 11: original shape torch.Size([16, 64, 36]), target (h,w)=(32,18)
[[34m2025-08-28 04:58:29[0m] Patchified latent 11 shape: torch.Size([576, 64])
[[34m2025-08-28 04:58:29[0m] Packed clean latent shape: torch.Size([6912, 64])
[[34m2025-08-28 04:58:29[0m] Noise shape: torch.Size([6912, 64])
[[34m2025-08-28 04:58:29[0m] Processed timesteps: tensor([0.4549, 0.4549, 0.4549,  ..., 0.4901, 0.4901, 0.4901], device='cuda:0')
[[34m2025-08-28 04:58:29[0m] Mixed latent shape: torch.Size([6912, 64])
[[34m2025-08-28 04:58:29[0m] Timestep embeddings shape: torch.Size([6912, 896])
[[34m2025-08-28 04:58:29[0m] Latent position embeddings shape: torch.Size([6912, 896])
[[34m2025-08-28 04:58:29[0m] Final VAE embeddings shape: torch.Size([6912, 896])
[[34m2025-08-28 04:59:24[0m] Wandb disabled by configuration
[[34m2025-08-28 04:59:24[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 04:59:24[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 04:59:24[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 04:59:24[0m] No finetune
[[34m2025-08-28 04:59:24[0m] set up language model
[[34m2025-08-28 04:59:28[0m] using visual_und
[[34m2025-08-28 04:59:29[0m] using generation
[[34m2025-08-28 04:59:29[0m] model loaded
[[34m2025-08-28 04:59:30[0m] tokenizers loaded
[[34m2025-08-28 04:59:30[0m] frozen done
[[34m2025-08-28 04:59:30[0m] fsdp done
[[34m2025-08-28 04:59:30[0m] Training from scratch.
[[34m2025-08-28 04:59:33[0m] checkpoint loading done
[[34m2025-08-28 04:59:33[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-28 04:59:33[0m] === STARTING SINGLE FORWARD PASS DEBUG ===
[[34m2025-08-28 04:59:37[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 04:59:37[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 04:59:37[0m] Input sequence_length: <class 'int'> = 16255
[[34m2025-08-28 04:59:37[0m] Input sample_lens: <class 'list'> = [595, 772, 595, 444, 382, 595, 1081, 733, 595, 320, 190, 595, 961, 621, 1211, 1461, 595, 936, 1244, 595, 595, 595, 212, 332, 129]
[[34m2025-08-28 04:59:37[0m] Input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input batch_data_indexes: <class 'list'> = [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 14, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:59:37[0m] Input split_lens: <class 'list'> = [17, 578, 730, 38, 4, 17, 578, 409, 32, 3, 335, 44, 3, 17, 578, 853, 86, 142, 692, 33, 8, 17, 578, 83, 113, 124, 123, 59, 8, 17, 578, 927, 28, 6, 594, 23, 4, 1001, 79, 131, 890, 9, 562, 17, 578, 890, 19, 27, 1075, 37, 132, 17, 578, 17, 578, 17, 578, 110, 11, 91, 298, 17, 17, 129]
[[34m2025-08-28 04:59:37[0m] Input attn_modes: <class 'list'> = ['causal', 'noise', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'causal', 'noise', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal']
[[34m2025-08-28 04:59:37[0m] Input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Input patchified_vae_latent_shapes: <class 'list'> = [(32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18)]
[[34m2025-08-28 04:59:37[0m] Input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Input ce_loss_weights: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 14, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 04:59:37[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.2500, 0.2500, 0.2500], device='cuda:0')
[[34m2025-08-28 04:59:37[0m] === DATA AFTER METADATA REMOVAL ===
[[34m2025-08-28 04:59:37[0m] Model input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Model input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Model input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 04:59:37[0m] Model input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] Model input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 04:59:37[0m] === VAE ENCODING ===
[[34m2025-08-28 04:59:37[0m] Original images shape: torch.Size([9, 3, 512, 288])
[[34m2025-08-28 04:59:38[0m] Encoded latents shape: torch.Size([9, 16, 64, 36])
[[34m2025-08-28 04:59:38[0m] === FINAL MODEL INPUTS ===
[[34m2025-08-28 04:59:38[0m] Final model input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 04:59:38[0m] Final model input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 04:59:38[0m] Final model input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 04:59:38[0m] Final model input padded_latent: shape=torch.Size([9, 16, 64, 36]), dtype=torch.float32
[[34m2025-08-28 04:59:38[0m] === FORWARD PASS ===
[[34m2025-08-28 05:01:03[0m] Wandb disabled by configuration
[[34m2025-08-28 05:01:03[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 05:01:03[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 05:01:03[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 05:01:03[0m] No finetune
[[34m2025-08-28 05:01:03[0m] set up language model
[[34m2025-08-28 05:01:07[0m] using visual_und
[[34m2025-08-28 05:01:08[0m] using generation
[[34m2025-08-28 05:01:08[0m] model loaded
[[34m2025-08-28 05:01:09[0m] tokenizers loaded
[[34m2025-08-28 05:01:09[0m] frozen done
[[34m2025-08-28 05:01:09[0m] fsdp done
[[34m2025-08-28 05:01:09[0m] Training from scratch.
[[34m2025-08-28 05:01:13[0m] checkpoint loading done
[[34m2025-08-28 05:01:13[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-28 05:01:13[0m] === STARTING SINGLE FORWARD PASS DEBUG ===
[[34m2025-08-28 05:01:17[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 05:01:17[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 05:01:17[0m] Input sequence_length: <class 'int'> = 16255
[[34m2025-08-28 05:01:17[0m] Input sample_lens: <class 'list'> = [595, 772, 595, 444, 382, 595, 1081, 733, 595, 320, 190, 595, 961, 621, 1211, 1461, 595, 936, 1244, 595, 595, 595, 212, 332, 129]
[[34m2025-08-28 05:01:17[0m] Input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input batch_data_indexes: <class 'list'> = [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 14, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 05:01:17[0m] Input split_lens: <class 'list'> = [17, 578, 730, 38, 4, 17, 578, 409, 32, 3, 335, 44, 3, 17, 578, 853, 86, 142, 692, 33, 8, 17, 578, 83, 113, 124, 123, 59, 8, 17, 578, 927, 28, 6, 594, 23, 4, 1001, 79, 131, 890, 9, 562, 17, 578, 890, 19, 27, 1075, 37, 132, 17, 578, 17, 578, 17, 578, 110, 11, 91, 298, 17, 17, 129]
[[34m2025-08-28 05:01:17[0m] Input attn_modes: <class 'list'> = ['causal', 'noise', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'causal', 'noise', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal']
[[34m2025-08-28 05:01:17[0m] Input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 05:01:17[0m] Input patchified_vae_latent_shapes: <class 'list'> = [(32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18)]
[[34m2025-08-28 05:01:17[0m] Input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 05:01:17[0m] Input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 05:01:17[0m] Input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:01:17[0m] Input ce_loss_weights: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 05:01:17[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 14, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 05:01:18[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.2500, 0.2500, 0.2500], device='cuda:0')
[[34m2025-08-28 05:01:18[0m] === DATA AFTER METADATA REMOVAL ===
[[34m2025-08-28 05:01:18[0m] Model input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 05:01:18[0m] Model input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 05:01:18[0m] Model input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 05:01:18[0m] Model input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Model input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] === VAE ENCODING ===
[[34m2025-08-28 05:01:18[0m] Original images shape: torch.Size([9, 3, 512, 288])
[[34m2025-08-28 05:01:18[0m] Encoded latents shape: torch.Size([9, 16, 64, 36])
[[34m2025-08-28 05:01:18[0m] === FINAL MODEL INPUTS ===
[[34m2025-08-28 05:01:18[0m] Final model input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 05:01:18[0m] Final model input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 05:01:18[0m] Final model input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:01:18[0m] Final model input padded_latent: shape=torch.Size([9, 16, 64, 36]), dtype=torch.float32
[[34m2025-08-28 05:01:18[0m] === FORWARD PASS ===
[[34m2025-08-28 05:01:22[0m] === MODEL OUTPUTS ===
[[34m2025-08-28 05:01:22[0m] Output mse: shape=torch.Size([5184, 64]), dtype=torch.float32
[[34m2025-08-28 05:01:22[0m]   Mean: 1.709884, Std: 2.403499
[[34m2025-08-28 05:01:22[0m]   Min: 0.000000, Max: 37.133789
[[34m2025-08-28 05:01:22[0m] Output ce: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 05:01:22[0m]   Mean: 21.024134, Std: 5.793601
[[34m2025-08-28 05:01:22[0m]   Min: 7.625000, Max: 45.500000
[[34m2025-08-28 05:01:22[0m] === LOSS CALCULATION ===
[[34m2025-08-28 05:01:22[0m] Processing CE loss:
[[34m2025-08-28 05:01:22[0m]   Raw CE loss shape: torch.Size([1247])
[[34m2025-08-28 05:01:22[0m]   CE loss indexes length: 1247
[[34m2025-08-28 05:01:22[0m]   Using standard CE loss averaging
[[34m2025-08-28 05:01:22[0m]   Averaged CE loss: 21.024134
[[34m2025-08-28 05:01:22[0m]   CE contribution to total loss: 21.024134
[[34m2025-08-28 05:01:22[0m] Processing MSE loss:
[[34m2025-08-28 05:01:22[0m]   Raw MSE loss shape: torch.Size([5184, 64])
[[34m2025-08-28 05:01:22[0m]   MSE loss indexes length: 5184
[[34m2025-08-28 05:01:22[0m]   Averaged MSE loss: 1.709884
[[34m2025-08-28 05:01:22[0m]   MSE contribution to total loss: 1.709884
[[34m2025-08-28 05:01:22[0m] === FINAL LOSS SUMMARY ===
[[34m2025-08-28 05:01:22[0m] Total loss: 22.734016
[[34m2025-08-28 05:01:22[0m] CE loss: 21.024134
[[34m2025-08-28 05:01:22[0m] MSE loss: 1.709884
[[34m2025-08-28 05:01:22[0m] CE weight: 1.0
[[34m2025-08-28 05:01:22[0m] MSE weight: 1.0
[[34m2025-08-28 05:01:22[0m] Total CE tokens: 1247
[[34m2025-08-28 05:01:22[0m] Total MSE tokens: 5184
[[34m2025-08-28 05:01:22[0m] === MEMORY USAGE ===
[[34m2025-08-28 05:01:22[0m] GPU memory allocated: 13238.03 MB
[[34m2025-08-28 05:01:22[0m] GPU memory reserved: 15548.00 MB
[[34m2025-08-28 05:01:22[0m] === SINGLE FORWARD PASS COMPLETED ===
[[34m2025-08-28 05:01:22[0m] Exiting after single forward pass debug
[[34m2025-08-28 05:09:09[0m] Wandb disabled by configuration
[[34m2025-08-28 05:09:09[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 05:09:09[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 05:09:09[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 05:09:09[0m] No finetune
[[34m2025-08-28 05:09:09[0m] set up language model
[[34m2025-08-28 05:09:14[0m] using visual_und
[[34m2025-08-28 05:09:14[0m] using generation
[[34m2025-08-28 05:09:15[0m] model loaded
[[34m2025-08-28 05:09:15[0m] tokenizers loaded
[[34m2025-08-28 05:09:15[0m] frozen done
[[34m2025-08-28 05:09:15[0m] fsdp done
[[34m2025-08-28 05:09:16[0m] Training from scratch.
[[34m2025-08-28 05:09:18[0m] checkpoint loading done
[[34m2025-08-28 05:09:18[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-28 05:09:18[0m] === STARTING SINGLE FORWARD PASS DEBUG ===
[[34m2025-08-28 05:09:23[0m] Raw data type: <class 'data.dataset_base.SimpleCustomBatch'>
[[34m2025-08-28 05:09:23[0m] Data keys: ['sequence_length', 'sample_lens', 'packed_text_ids', 'packed_text_indexes', 'packed_position_ids', 'batch_data_indexes', 'split_lens', 'attn_modes', 'padded_images', 'patchified_vae_latent_shapes', 'packed_latent_position_ids', 'packed_vae_token_indexes', 'packed_vit_tokens', 'packed_vit_position_ids', 'packed_vit_token_indexes', 'vit_token_seqlens', 'packed_timesteps', 'mse_loss_indexes', 'packed_label_ids', 'ce_loss_indexes', 'ce_loss_weights']
[[34m2025-08-28 05:09:23[0m] Input sequence_length: <class 'int'> = 16255
[[34m2025-08-28 05:09:23[0m] Input sample_lens: <class 'list'> = [595, 772, 595, 444, 382, 595, 1081, 733, 595, 320, 190, 595, 961, 621, 1211, 1461, 595, 936, 1244, 595, 595, 595, 212, 332, 129]
[[34m2025-08-28 05:09:23[0m] Input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input batch_data_indexes: <class 'list'> = [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 14, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 05:09:23[0m] Input split_lens: <class 'list'> = [17, 578, 730, 38, 4, 17, 578, 409, 32, 3, 335, 44, 3, 17, 578, 853, 86, 142, 692, 33, 8, 17, 578, 83, 113, 124, 123, 59, 8, 17, 578, 927, 28, 6, 594, 23, 4, 1001, 79, 131, 890, 9, 562, 17, 578, 890, 19, 27, 1075, 37, 132, 17, 578, 17, 578, 17, 578, 110, 11, 91, 298, 17, 17, 129]
[[34m2025-08-28 05:09:23[0m] Input attn_modes: <class 'list'> = ['causal', 'noise', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal', 'noise', 'causal', 'noise', 'causal', 'noise', 'full', 'causal', 'causal', 'full', 'causal', 'causal', 'causal']
[[34m2025-08-28 05:09:23[0m] Input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Input patchified_vae_latent_shapes: <class 'list'> = [(32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18), (32, 18)]
[[34m2025-08-28 05:09:23[0m] Input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Input ce_loss_weights: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Batch data indexes: [{'data_indexes': [0, 0, 0], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 0, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 1], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 1, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 2, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 2], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 3, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 4, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 3], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 5, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 6, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 4], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 7, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 8, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 9, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 10, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 5], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 11, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 12, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': [0, 0, 6], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 7], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': [0, 0, 8], 'worker_id': 0, 'dataset_name': 't2i_pretrain'}, {'data_indexes': 13, 'worker_id': 0, 'dataset_name': 'vlm_sft'}, {'data_indexes': 14, 'worker_id': 0, 'dataset_name': 'vlm_sft'}]
[[34m2025-08-28 05:09:23[0m] CE loss weights: tensor([0.5774, 0.5774, 0.5774,  ..., 0.2500, 0.2500, 0.2500], device='cuda:0')
[[34m2025-08-28 05:09:23[0m] === DATA AFTER METADATA REMOVAL ===
[[34m2025-08-28 05:09:23[0m] Model input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Model input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Model input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Model input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Model input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] === VAE ENCODING ===
[[34m2025-08-28 05:09:23[0m] Original images shape: torch.Size([9, 3, 512, 288])
[[34m2025-08-28 05:09:23[0m] Encoded latents shape: torch.Size([9, 16, 64, 36])
[[34m2025-08-28 05:09:23[0m] === FINAL MODEL INPUTS ===
[[34m2025-08-28 05:09:23[0m] Final model input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Final model input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] Final model input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 05:09:23[0m] Final model input padded_latent: shape=torch.Size([9, 16, 64, 36]), dtype=torch.float32
[[34m2025-08-28 05:09:23[0m] === FORWARD PASS ===
[[34m2025-08-28 05:09:27[0m] === MODEL OUTPUTS ===
[[34m2025-08-28 05:09:27[0m] Output mse: shape=torch.Size([5184, 64]), dtype=torch.float32
[[34m2025-08-28 05:09:27[0m]   Mean: 1.709884, Std: 2.403499
[[34m2025-08-28 05:09:27[0m]   Min: 0.000000, Max: 37.133789
[[34m2025-08-28 05:09:27[0m] Output ce: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 05:09:27[0m]   Mean: 21.024134, Std: 5.793601
[[34m2025-08-28 05:09:27[0m]   Min: 7.625000, Max: 45.500000
[[34m2025-08-28 05:09:27[0m] === LOSS CALCULATION ===
[[34m2025-08-28 05:09:27[0m] Processing CE loss:
[[34m2025-08-28 05:09:27[0m]   Raw CE loss shape: torch.Size([1247])
[[34m2025-08-28 05:09:27[0m]   CE loss indexes length: 1247
[[34m2025-08-28 05:09:27[0m]   Using standard CE loss averaging
[[34m2025-08-28 05:09:27[0m]   Averaged CE loss: 21.024134
[[34m2025-08-28 05:09:27[0m]   CE contribution to total loss: 21.024134
[[34m2025-08-28 05:09:27[0m] Processing MSE loss:
[[34m2025-08-28 05:09:27[0m]   Raw MSE loss shape: torch.Size([5184, 64])
[[34m2025-08-28 05:09:27[0m]   MSE loss indexes length: 5184
[[34m2025-08-28 05:09:27[0m]   Averaged MSE loss: 1.709884
[[34m2025-08-28 05:09:27[0m]   MSE contribution to total loss: 1.709884
[[34m2025-08-28 05:09:27[0m] === FINAL LOSS SUMMARY ===
[[34m2025-08-28 05:09:27[0m] Total loss: 22.734016
[[34m2025-08-28 05:09:27[0m] CE loss: 21.024134
[[34m2025-08-28 05:09:27[0m] MSE loss: 1.709884
[[34m2025-08-28 05:09:27[0m] CE weight: 1.0
[[34m2025-08-28 05:09:27[0m] MSE weight: 1.0
[[34m2025-08-28 05:09:27[0m] Total CE tokens: 1247
[[34m2025-08-28 05:09:27[0m] Total MSE tokens: 5184
[[34m2025-08-28 05:09:27[0m] === MEMORY USAGE ===
[[34m2025-08-28 05:09:27[0m] GPU memory allocated: 13238.03 MB
[[34m2025-08-28 05:09:27[0m] GPU memory reserved: 15548.00 MB
[[34m2025-08-28 05:09:27[0m] === SINGLE FORWARD PASS COMPLETED ===
[[34m2025-08-28 05:09:27[0m] Exiting after single forward pass debug
[[34m2025-08-28 06:12:09[0m] Wandb disabled by configuration
[[34m2025-08-28 06:12:09[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 06:12:09[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 06:12:09[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 06:12:09[0m] No finetune
[[34m2025-08-28 06:12:09[0m] set up language model
[[34m2025-08-28 06:12:14[0m] using visual_und
[[34m2025-08-28 06:12:14[0m] using generation
[[34m2025-08-28 06:12:15[0m] model loaded
[[34m2025-08-28 06:12:15[0m] tokenizers loaded
[[34m2025-08-28 06:12:15[0m] frozen done
[[34m2025-08-28 06:12:15[0m] 
============================================================
[[34m2025-08-28 06:12:15[0m] MODEL PARAMETER SUMMARY
[[34m2025-08-28 06:12:15[0m] ============================================================
[[34m2025-08-28 06:12:15[0m] Language Model (LLM):
[[34m2025-08-28 06:12:15[0m]   Total:     988,071,680 parameters
[[34m2025-08-28 06:12:15[0m]   Trainable: 988,071,680 parameters
[[34m2025-08-28 06:12:15[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:12:15[0m] 
Vision Model (ViT):
[[34m2025-08-28 06:12:15[0m]   Total:     402,552,736 parameters
[[34m2025-08-28 06:12:15[0m]   Trainable: 402,552,736 parameters
[[34m2025-08-28 06:12:15[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:12:15[0m] 
Connector/Projector:
[[34m2025-08-28 06:12:15[0m]   Total:     10,332,032 parameters
[[34m2025-08-28 06:12:15[0m]   Trainable: 10,332,032 parameters
[[34m2025-08-28 06:12:15[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:12:15[0m] 
VAE Model:
[[34m2025-08-28 06:12:15[0m]   Total:     83,819,683 parameters
[[34m2025-08-28 06:12:15[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:12:15[0m]   Frozen:    83,819,683 parameters
[[34m2025-08-28 06:12:15[0m] 
ViT Position Embeddings:
[[34m2025-08-28 06:12:15[0m]   Total:     2,240,000 parameters
[[34m2025-08-28 06:12:15[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:12:15[0m]   Frozen:    2,240,000 parameters
[[34m2025-08-28 06:12:15[0m] 
Total Bagel Model:
[[34m2025-08-28 06:12:15[0m]   Total:     1,406,410,464 parameters
[[34m2025-08-28 06:12:15[0m]   Trainable: 1,402,106,080 parameters
[[34m2025-08-28 06:12:15[0m]   Frozen:    4,304,384 parameters
[[34m2025-08-28 06:12:15[0m] 
Training 99.7% of total parameters
[[34m2025-08-28 06:12:15[0m] ============================================================
[[34m2025-08-28 06:12:15[0m] fsdp config done
[[34m2025-08-28 06:12:16[0m] Training from scratch.
[[34m2025-08-28 06:12:18[0m] checkpoint loading done
[[34m2025-08-28 06:12:18[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-28 06:12:22[0m] Input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:12:22[0m] Input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 06:12:22[0m] Input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 06:12:22[0m] Input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 06:12:22[0m] Input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 06:12:27[0m] Output mse: shape=torch.Size([5184, 64]), dtype=torch.float32
[[34m2025-08-28 06:12:27[0m] Output ce: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 06:12:33[0m] (step=0000000) Train Loss mse: 1.7099, Train Loss ce: 21.0241, Train Steps/Sec: 0.71, 
[[34m2025-08-28 06:12:33[0m] Input packed_text_ids: shape=torch.Size([1463]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_text_indexes: shape=torch.Size([1463]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_position_ids: shape=torch.Size([16269]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input padded_images: shape=torch.Size([14, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:12:33[0m] Input packed_latent_position_ids: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_vae_token_indexes: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_vit_tokens: shape=torch.Size([6742, 588]), dtype=torch.float32
[[34m2025-08-28 06:12:33[0m] Input packed_vit_position_ids: shape=torch.Size([6742]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_vit_token_indexes: shape=torch.Size([6742]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input vit_token_seqlens: shape=torch.Size([11]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_timesteps: shape=torch.Size([8064]), dtype=torch.float32
[[34m2025-08-28 06:12:33[0m] Input mse_loss_indexes: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input packed_label_ids: shape=torch.Size([610]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Input ce_loss_indexes: shape=torch.Size([610]), dtype=torch.int64
[[34m2025-08-28 06:12:33[0m] Output mse: shape=torch.Size([8064, 64]), dtype=torch.float32
[[34m2025-08-28 06:12:33[0m] Output ce: shape=torch.Size([610]), dtype=torch.float32
[[34m2025-08-28 06:12:34[0m] Input packed_text_ids: shape=torch.Size([1014]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_text_indexes: shape=torch.Size([1014]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_position_ids: shape=torch.Size([16223]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input padded_images: shape=torch.Size([16, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:12:34[0m] Input packed_latent_position_ids: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_vae_token_indexes: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_vit_tokens: shape=torch.Size([5993, 588]), dtype=torch.float32
[[34m2025-08-28 06:12:34[0m] Input packed_vit_position_ids: shape=torch.Size([5993]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_vit_token_indexes: shape=torch.Size([5993]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input vit_token_seqlens: shape=torch.Size([8]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_timesteps: shape=torch.Size([9216]), dtype=torch.float32
[[34m2025-08-28 06:12:34[0m] Input mse_loss_indexes: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input packed_label_ids: shape=torch.Size([311]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Input ce_loss_indexes: shape=torch.Size([311]), dtype=torch.int64
[[34m2025-08-28 06:12:34[0m] Output mse: shape=torch.Size([9216, 64]), dtype=torch.float32
[[34m2025-08-28 06:12:34[0m] Output ce: shape=torch.Size([311]), dtype=torch.float32
[[34m2025-08-28 06:12:35[0m] Input packed_text_ids: shape=torch.Size([1024]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_text_indexes: shape=torch.Size([1024]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_position_ids: shape=torch.Size([16384]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input padded_images: shape=torch.Size([13, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:12:35[0m] Input packed_latent_position_ids: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_vae_token_indexes: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_vit_tokens: shape=torch.Size([7872, 588]), dtype=torch.float32
[[34m2025-08-28 06:12:35[0m] Input packed_vit_position_ids: shape=torch.Size([7872]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_vit_token_indexes: shape=torch.Size([7872]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input vit_token_seqlens: shape=torch.Size([10]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_timesteps: shape=torch.Size([7488]), dtype=torch.float32
[[34m2025-08-28 06:12:35[0m] Input mse_loss_indexes: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input packed_label_ids: shape=torch.Size([528]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Input ce_loss_indexes: shape=torch.Size([528]), dtype=torch.int64
[[34m2025-08-28 06:12:35[0m] Output mse: shape=torch.Size([7488, 64]), dtype=torch.float32
[[34m2025-08-28 06:12:35[0m] Output ce: shape=torch.Size([528]), dtype=torch.float32
[[34m2025-08-28 06:12:44[0m] (step=0000010) Train Loss mse: 1.7150, Train Loss ce: 20.7461, Train Steps/Sec: 0.87, 
[[34m2025-08-28 06:12:55[0m] (step=0000020) Train Loss mse: 1.7071, Train Loss ce: 23.0292, Train Steps/Sec: 0.88, 
[[34m2025-08-28 06:13:06[0m] (step=0000030) Train Loss mse: 1.6950, Train Loss ce: 22.1971, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:13:18[0m] (step=0000040) Train Loss mse: 1.6906, Train Loss ce: 21.0699, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:13:29[0m] (step=0000050) Train Loss mse: 1.6793, Train Loss ce: 19.0334, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:13:40[0m] (step=0000060) Train Loss mse: 1.6635, Train Loss ce: 17.3664, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:13:51[0m] (step=0000070) Train Loss mse: 1.6354, Train Loss ce: 16.4821, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:14:03[0m] (step=0000080) Train Loss mse: 1.6100, Train Loss ce: 15.9435, Train Steps/Sec: 0.88, 
[[34m2025-08-28 06:14:14[0m] (step=0000090) Train Loss mse: 1.5680, Train Loss ce: 14.1076, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:15:48[0m] Wandb initialized successfully
[[34m2025-08-28 06:15:48[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=True, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=False, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 06:15:48[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 06:15:48[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 06:15:49[0m] No finetune
[[34m2025-08-28 06:15:49[0m] set up language model
[[34m2025-08-28 06:15:53[0m] using visual_und
[[34m2025-08-28 06:15:53[0m] using generation
[[34m2025-08-28 06:15:54[0m] model loaded
[[34m2025-08-28 06:15:54[0m] tokenizers loaded
[[34m2025-08-28 06:15:54[0m] frozen done
[[34m2025-08-28 06:15:54[0m] 
============================================================
[[34m2025-08-28 06:15:54[0m] MODEL PARAMETER SUMMARY
[[34m2025-08-28 06:15:54[0m] ============================================================
[[34m2025-08-28 06:15:54[0m] Language Model (LLM):
[[34m2025-08-28 06:15:54[0m]   Total:     988,071,680 parameters
[[34m2025-08-28 06:15:54[0m]   Trainable: 988,071,680 parameters
[[34m2025-08-28 06:15:54[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:15:54[0m] 
Vision Model (ViT):
[[34m2025-08-28 06:15:54[0m]   Total:     402,552,736 parameters
[[34m2025-08-28 06:15:54[0m]   Trainable: 402,552,736 parameters
[[34m2025-08-28 06:15:54[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:15:54[0m] 
Connector/Projector:
[[34m2025-08-28 06:15:54[0m]   Total:     10,332,032 parameters
[[34m2025-08-28 06:15:54[0m]   Trainable: 10,332,032 parameters
[[34m2025-08-28 06:15:54[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:15:54[0m] 
VAE Model:
[[34m2025-08-28 06:15:54[0m]   Total:     83,819,683 parameters
[[34m2025-08-28 06:15:54[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:15:54[0m]   Frozen:    83,819,683 parameters
[[34m2025-08-28 06:15:54[0m] 
ViT Position Embeddings:
[[34m2025-08-28 06:15:54[0m]   Total:     2,240,000 parameters
[[34m2025-08-28 06:15:54[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:15:54[0m]   Frozen:    2,240,000 parameters
[[34m2025-08-28 06:15:54[0m] 
Total Bagel Model:
[[34m2025-08-28 06:15:54[0m]   Total:     1,406,410,464 parameters
[[34m2025-08-28 06:15:54[0m]   Trainable: 1,402,106,080 parameters
[[34m2025-08-28 06:15:54[0m]   Frozen:    4,304,384 parameters
[[34m2025-08-28 06:15:54[0m] 
Training 99.7% of total parameters
[[34m2025-08-28 06:15:54[0m] ============================================================
[[34m2025-08-28 06:15:54[0m] fsdp config done
[[34m2025-08-28 06:15:55[0m] Training from scratch.
[[34m2025-08-28 06:15:57[0m] checkpoint loading done
[[34m2025-08-28 06:15:57[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-28 06:16:01[0m] Input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:16:01[0m] Input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-28 06:16:01[0m] Input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-28 06:16:01[0m] Input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 06:16:01[0m] Input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-28 06:16:05[0m] Output mse: shape=torch.Size([5184, 64]), dtype=torch.float32
[[34m2025-08-28 06:16:05[0m] Output ce: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-28 06:16:07[0m] (step=0000000) Train Loss mse: 1.7099, Train Loss ce: 21.0241, Train Steps/Sec: 1.00, 
[[34m2025-08-28 06:16:07[0m] Input packed_text_ids: shape=torch.Size([1463]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_text_indexes: shape=torch.Size([1463]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_position_ids: shape=torch.Size([16269]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input padded_images: shape=torch.Size([14, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:16:07[0m] Input packed_latent_position_ids: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_vae_token_indexes: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_vit_tokens: shape=torch.Size([6742, 588]), dtype=torch.float32
[[34m2025-08-28 06:16:07[0m] Input packed_vit_position_ids: shape=torch.Size([6742]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_vit_token_indexes: shape=torch.Size([6742]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input vit_token_seqlens: shape=torch.Size([11]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_timesteps: shape=torch.Size([8064]), dtype=torch.float32
[[34m2025-08-28 06:16:07[0m] Input mse_loss_indexes: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input packed_label_ids: shape=torch.Size([610]), dtype=torch.int64
[[34m2025-08-28 06:16:07[0m] Input ce_loss_indexes: shape=torch.Size([610]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Output mse: shape=torch.Size([8064, 64]), dtype=torch.float32
[[34m2025-08-28 06:16:08[0m] Output ce: shape=torch.Size([610]), dtype=torch.float32
[[34m2025-08-28 06:16:08[0m] Input packed_text_ids: shape=torch.Size([1014]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_text_indexes: shape=torch.Size([1014]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_position_ids: shape=torch.Size([16223]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input padded_images: shape=torch.Size([16, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:16:08[0m] Input packed_latent_position_ids: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_vae_token_indexes: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_vit_tokens: shape=torch.Size([5993, 588]), dtype=torch.float32
[[34m2025-08-28 06:16:08[0m] Input packed_vit_position_ids: shape=torch.Size([5993]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_vit_token_indexes: shape=torch.Size([5993]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input vit_token_seqlens: shape=torch.Size([8]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_timesteps: shape=torch.Size([9216]), dtype=torch.float32
[[34m2025-08-28 06:16:08[0m] Input mse_loss_indexes: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input packed_label_ids: shape=torch.Size([311]), dtype=torch.int64
[[34m2025-08-28 06:16:08[0m] Input ce_loss_indexes: shape=torch.Size([311]), dtype=torch.int64
[[34m2025-08-28 06:16:09[0m] Output mse: shape=torch.Size([9216, 64]), dtype=torch.float32
[[34m2025-08-28 06:16:09[0m] Output ce: shape=torch.Size([311]), dtype=torch.float32
[[34m2025-08-28 06:16:10[0m] Input packed_text_ids: shape=torch.Size([1024]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_text_indexes: shape=torch.Size([1024]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_position_ids: shape=torch.Size([16384]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input padded_images: shape=torch.Size([13, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-28 06:16:10[0m] Input packed_latent_position_ids: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_vae_token_indexes: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_vit_tokens: shape=torch.Size([7872, 588]), dtype=torch.float32
[[34m2025-08-28 06:16:10[0m] Input packed_vit_position_ids: shape=torch.Size([7872]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_vit_token_indexes: shape=torch.Size([7872]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input vit_token_seqlens: shape=torch.Size([10]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_timesteps: shape=torch.Size([7488]), dtype=torch.float32
[[34m2025-08-28 06:16:10[0m] Input mse_loss_indexes: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input packed_label_ids: shape=torch.Size([528]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Input ce_loss_indexes: shape=torch.Size([528]), dtype=torch.int64
[[34m2025-08-28 06:16:10[0m] Output mse: shape=torch.Size([7488, 64]), dtype=torch.float32
[[34m2025-08-28 06:16:10[0m] Output ce: shape=torch.Size([528]), dtype=torch.float32
[[34m2025-08-28 06:16:19[0m] (step=0000010) Train Loss mse: 1.7150, Train Loss ce: 20.7326, Train Steps/Sec: 0.88, 
[[34m2025-08-28 06:16:30[0m] (step=0000020) Train Loss mse: 1.7070, Train Loss ce: 22.9450, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:16:41[0m] (step=0000030) Train Loss mse: 1.6950, Train Loss ce: 22.1196, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:16:52[0m] (step=0000040) Train Loss mse: 1.6915, Train Loss ce: 21.2117, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:17:03[0m] (step=0000050) Train Loss mse: 1.6797, Train Loss ce: 19.1095, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:17:14[0m] (step=0000060) Train Loss mse: 1.6643, Train Loss ce: 17.4047, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:17:26[0m] (step=0000070) Train Loss mse: 1.6362, Train Loss ce: 16.6415, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:17:37[0m] (step=0000080) Train Loss mse: 1.6101, Train Loss ce: 16.0819, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:17:48[0m] (step=0000090) Train Loss mse: 1.5683, Train Loss ce: 14.4639, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:17:59[0m] (step=0000100) Train Loss mse: 1.5361, Train Loss ce: 13.4914, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:18:10[0m] (step=0000110) Train Loss mse: 1.5011, Train Loss ce: 13.0575, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:18:21[0m] (step=0000120) Train Loss mse: 1.4752, Train Loss ce: 12.3501, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:18:32[0m] (step=0000130) Train Loss mse: 1.4532, Train Loss ce: 11.1982, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:18:44[0m] (step=0000140) Train Loss mse: 1.4223, Train Loss ce: 11.3231, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:18:55[0m] (step=0000150) Train Loss mse: 1.4002, Train Loss ce: 10.7629, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:19:06[0m] (step=0000160) Train Loss mse: 1.3791, Train Loss ce: 10.0117, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:19:17[0m] (step=0000170) Train Loss mse: 1.3552, Train Loss ce: 9.8268, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:19:28[0m] (step=0000180) Train Loss mse: 1.3277, Train Loss ce: 10.1054, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:19:39[0m] (step=0000190) Train Loss mse: 1.3097, Train Loss ce: 9.6307, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:19:51[0m] (step=0000200) Train Loss mse: 1.3014, Train Loss ce: 9.7773, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:20:02[0m] (step=0000210) Train Loss mse: 1.2822, Train Loss ce: 10.1829, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:20:13[0m] (step=0000220) Train Loss mse: 1.2498, Train Loss ce: 9.1868, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:20:24[0m] (step=0000230) Train Loss mse: 1.2737, Train Loss ce: 9.4072, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:20:35[0m] (step=0000240) Train Loss mse: 1.2490, Train Loss ce: 9.5156, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:20:46[0m] (step=0000250) Train Loss mse: 1.2292, Train Loss ce: 8.1965, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:20:57[0m] (step=0000260) Train Loss mse: 1.2136, Train Loss ce: 9.3843, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:21:08[0m] (step=0000270) Train Loss mse: 1.2111, Train Loss ce: 8.8532, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:21:20[0m] (step=0000280) Train Loss mse: 1.1900, Train Loss ce: 7.9142, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:21:32[0m] (step=0000290) Train Loss mse: 1.1907, Train Loss ce: 8.1336, Train Steps/Sec: 0.81, 
[[34m2025-08-28 06:21:51[0m] (step=0000300) Train Loss mse: 1.1659, Train Loss ce: 8.1507, Train Steps/Sec: 0.54, 
[[34m2025-08-28 06:22:09[0m] (step=0000310) Train Loss mse: 1.1490, Train Loss ce: 9.0485, Train Steps/Sec: 0.55, 
[[34m2025-08-28 06:22:24[0m] (step=0000320) Train Loss mse: 1.1209, Train Loss ce: 8.9994, Train Steps/Sec: 0.64, 
[[34m2025-08-28 06:22:36[0m] (step=0000330) Train Loss mse: 1.1208, Train Loss ce: 8.1472, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:22:47[0m] (step=0000340) Train Loss mse: 1.0796, Train Loss ce: 8.4093, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:22:58[0m] (step=0000350) Train Loss mse: 1.0802, Train Loss ce: 8.9425, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:23:09[0m] (step=0000360) Train Loss mse: 1.0584, Train Loss ce: 7.0578, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:23:20[0m] (step=0000370) Train Loss mse: 1.0198, Train Loss ce: 7.1527, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:23:31[0m] (step=0000380) Train Loss mse: 1.0347, Train Loss ce: 7.8816, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:23:43[0m] (step=0000390) Train Loss mse: 0.9800, Train Loss ce: 6.7649, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:23:54[0m] (step=0000400) Train Loss mse: 1.0219, Train Loss ce: 8.0017, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:24:05[0m] (step=0000410) Train Loss mse: 0.9630, Train Loss ce: 7.7848, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:24:16[0m] (step=0000420) Train Loss mse: 0.8662, Train Loss ce: 7.8108, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:24:27[0m] (step=0000430) Train Loss mse: 0.9136, Train Loss ce: 7.4429, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:24:38[0m] (step=0000440) Train Loss mse: 0.9081, Train Loss ce: 6.2316, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:24:49[0m] (step=0000450) Train Loss mse: 0.9057, Train Loss ce: 6.7354, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:25:01[0m] (step=0000460) Train Loss mse: 0.7800, Train Loss ce: 6.7581, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:25:12[0m] (step=0000470) Train Loss mse: 0.9091, Train Loss ce: 6.4795, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:25:23[0m] (step=0000480) Train Loss mse: 0.8947, Train Loss ce: 6.6375, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:25:34[0m] (step=0000490) Train Loss mse: 0.7931, Train Loss ce: 8.6753, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:25:45[0m] (step=0000500) Train Loss mse: 0.7647, Train Loss ce: 6.9814, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:25:56[0m] (step=0000510) Train Loss mse: 0.7090, Train Loss ce: 7.5162, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:26:07[0m] (step=0000520) Train Loss mse: 0.7458, Train Loss ce: 6.3718, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:26:19[0m] (step=0000530) Train Loss mse: 0.6767, Train Loss ce: 5.8392, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:26:30[0m] (step=0000540) Train Loss mse: 0.7095, Train Loss ce: 6.4821, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:26:41[0m] (step=0000550) Train Loss mse: 0.7203, Train Loss ce: 7.2810, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:26:52[0m] (step=0000560) Train Loss mse: 0.6829, Train Loss ce: 7.3889, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:27:03[0m] (step=0000570) Train Loss mse: 0.6956, Train Loss ce: 5.9353, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:27:14[0m] (step=0000580) Train Loss mse: 0.7143, Train Loss ce: 7.2141, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:27:25[0m] (step=0000590) Train Loss mse: 0.6578, Train Loss ce: 6.4916, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:27:36[0m] (step=0000600) Train Loss mse: 0.5497, Train Loss ce: 6.4926, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:27:47[0m] (step=0000610) Train Loss mse: 0.7897, Train Loss ce: 5.2021, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:27:58[0m] (step=0000620) Train Loss mse: 0.5957, Train Loss ce: 5.5124, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:28:10[0m] (step=0000630) Train Loss mse: 0.6785, Train Loss ce: 5.6690, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:28:21[0m] (step=0000640) Train Loss mse: 0.6096, Train Loss ce: 6.1604, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:28:32[0m] (step=0000650) Train Loss mse: 0.5678, Train Loss ce: 5.1702, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:28:43[0m] (step=0000660) Train Loss mse: 0.6406, Train Loss ce: 5.3529, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:28:54[0m] (step=0000670) Train Loss mse: 0.6292, Train Loss ce: 6.6911, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:29:05[0m] (step=0000680) Train Loss mse: 0.6477, Train Loss ce: 6.7965, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:29:17[0m] (step=0000690) Train Loss mse: 0.6005, Train Loss ce: 5.9982, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:29:28[0m] (step=0000700) Train Loss mse: 0.6268, Train Loss ce: 5.6611, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:29:39[0m] (step=0000710) Train Loss mse: 0.7151, Train Loss ce: 6.1543, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:29:50[0m] (step=0000720) Train Loss mse: 0.6621, Train Loss ce: 5.5538, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:30:01[0m] (step=0000730) Train Loss mse: 0.6024, Train Loss ce: 5.4020, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:30:12[0m] (step=0000740) Train Loss mse: 0.6794, Train Loss ce: 5.3597, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:30:23[0m] (step=0000750) Train Loss mse: 0.5455, Train Loss ce: 5.9602, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:30:34[0m] (step=0000760) Train Loss mse: 0.5124, Train Loss ce: 5.9988, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:30:46[0m] (step=0000770) Train Loss mse: 0.5757, Train Loss ce: 5.9107, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:30:57[0m] (step=0000780) Train Loss mse: 0.6073, Train Loss ce: 5.1920, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:31:08[0m] (step=0000790) Train Loss mse: 0.4570, Train Loss ce: 4.9718, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:31:19[0m] (step=0000800) Train Loss mse: 0.5970, Train Loss ce: 6.0016, Train Steps/Sec: 0.88, 
[[34m2025-08-28 06:31:30[0m] (step=0000810) Train Loss mse: 0.4904, Train Loss ce: 5.1897, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:31:41[0m] (step=0000820) Train Loss mse: 0.4524, Train Loss ce: 5.4176, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:31:53[0m] (step=0000830) Train Loss mse: 0.4735, Train Loss ce: 5.9730, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:32:04[0m] (step=0000840) Train Loss mse: 0.5264, Train Loss ce: 5.2250, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:32:15[0m] (step=0000850) Train Loss mse: 0.5573, Train Loss ce: 6.3184, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:32:26[0m] (step=0000860) Train Loss mse: 0.4768, Train Loss ce: 6.2187, Train Steps/Sec: 0.89, 
[[34m2025-08-28 06:32:37[0m] (step=0000870) Train Loss mse: 0.5072, Train Loss ce: 5.5496, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:32:48[0m] (step=0000880) Train Loss mse: 0.4419, Train Loss ce: 6.3005, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:32:59[0m] (step=0000890) Train Loss mse: 0.5236, Train Loss ce: 5.6348, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:33:10[0m] (step=0000900) Train Loss mse: 0.4124, Train Loss ce: 5.5512, Train Steps/Sec: 0.90, 
[[34m2025-08-28 06:33:21[0m] (step=0000910) Train Loss mse: 0.5242, Train Loss ce: 6.3312, Train Steps/Sec: 0.91, 
[[34m2025-08-28 06:39:25[0m] Failed to initialize wandb: Attempted to change value of key "checkpoint_dir" from results/checkpoints to /home/haoming/Bagel/results/checkpoints
If you really want to do this, pass allow_val_change=True to config.update(). Continuing without wandb.
[[34m2025-08-28 06:39:25[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-28 06:39:25[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 06:39:25[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 06:39:25[0m] enabling finetuning
[[34m2025-08-28 06:39:25[0m] set up language model
[[34m2025-08-28 06:42:08[0m] using visual_und
[[34m2025-08-28 06:42:13[0m] using generation
[[34m2025-08-28 06:42:15[0m] model loaded
[[34m2025-08-28 06:42:15[0m] tokenizers loaded
[[34m2025-08-28 06:42:15[0m] frozen done
[[34m2025-08-28 06:42:15[0m] 
============================================================
[[34m2025-08-28 06:42:15[0m] MODEL PARAMETER SUMMARY
[[34m2025-08-28 06:42:15[0m] ============================================================
[[34m2025-08-28 06:42:15[0m] Language Model (LLM):
[[34m2025-08-28 06:42:15[0m]   Total:     14,141,252,608 parameters
[[34m2025-08-28 06:42:15[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:42:15[0m]   Frozen:    14,141,252,608 parameters
[[34m2025-08-28 06:42:15[0m] 
Vision Model (ViT):
[[34m2025-08-28 06:42:15[0m]   Total:     402,552,736 parameters
[[34m2025-08-28 06:42:15[0m]   Trainable: 402,552,736 parameters
[[34m2025-08-28 06:42:15[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:42:15[0m] 
Connector/Projector:
[[34m2025-08-28 06:42:15[0m]   Total:     13,431,296 parameters
[[34m2025-08-28 06:42:15[0m]   Trainable: 13,431,296 parameters
[[34m2025-08-28 06:42:15[0m]   Frozen:    0 parameters
[[34m2025-08-28 06:42:15[0m] 
VAE Model:
[[34m2025-08-28 06:42:15[0m]   Total:     83,819,683 parameters
[[34m2025-08-28 06:42:15[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:42:15[0m]   Frozen:    83,819,683 parameters
[[34m2025-08-28 06:42:15[0m] 
ViT Position Embeddings:
[[34m2025-08-28 06:42:15[0m]   Total:     8,960,000 parameters
[[34m2025-08-28 06:42:15[0m]   Trainable: 0 parameters
[[34m2025-08-28 06:42:15[0m]   Frozen:    8,960,000 parameters
[[34m2025-08-28 06:42:15[0m] 
Total Bagel Model:
[[34m2025-08-28 06:42:15[0m]   Total:     14,588,686,304 parameters
[[34m2025-08-28 06:42:15[0m]   Trainable: 430,216,160 parameters
[[34m2025-08-28 06:42:15[0m]   Frozen:    14,158,470,144 parameters
[[34m2025-08-28 06:42:15[0m] 
Training 2.9% of total parameters
[[34m2025-08-28 06:42:15[0m] ============================================================
[[34m2025-08-28 06:42:15[0m] fsdp config done
[[34m2025-08-28 06:42:19[0m] Training from scratch.
[[34m2025-08-28 06:51:09[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 06:51:12[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 06:51:12[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 06:51:12[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 06:54:07[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 06:54:13[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 06:54:14[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 06:54:15[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 06:54:37[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 06:54:47[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 06:54:54[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 06:54:54[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 06:55:04[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 13.61 GB
Reserved:  23.82 GB
Max Allocated: 21.80 GB
Max Reserved:  23.82 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:01:43[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:01:46[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 07:01:46[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 07:01:46[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 07:04:33[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:04:39[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:04:39[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:04:40[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:04:59[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 07:05:41[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 07:05:55[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-28 07:05:56[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:06:10[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 0.00 GB
Reserved:  12.06 GB
Max Allocated: 9.14 GB
Max Reserved:  12.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:06:31[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.06 GB
Max Allocated: 9.14 GB
Max Reserved:  12.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:06:31[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 0.00 GB
Reserved:  12.06 GB
Max Allocated: 9.14 GB
Max Reserved:  12.06 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:06:32[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.07 GB
Max Allocated: 9.14 GB
Max Reserved:  12.07 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:06:32[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-28 07:06:33[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 0.32 GB
Reserved:  12.07 GB
Max Allocated: 9.14 GB
Max Reserved:  12.07 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:06:33[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 0.33 GB
Reserved:  12.07 GB
Max Allocated: 9.14 GB
Max Reserved:  12.07 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:13:00[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:13:02[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 07:13:02[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 07:13:02[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 07:15:51[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:15:57[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:15:58[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:18:09[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:18:11[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 07:18:11[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 07:18:11[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 07:21:05[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:22:36[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:22:38[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 07:22:38[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 07:22:38[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 07:25:28[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:25:34[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:25:35[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:26:49[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:26:51[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 07:26:51[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-28 07:26:51[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-28 07:29:47[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:29:53[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:29:54[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:31:24[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:31:27[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-28 07:31:27[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-28 07:31:27[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-28 07:34:26[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:34:33[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:34:33[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:34:34[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:34:55[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-28 07:35:02[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.query', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.2.weight', 'connector.layer_projectors.2.bias', 'connector.attn.in_proj_weight', 'connector.attn.in_proj_bias', 'connector.attn.out_proj.weight', 'connector.attn.out_proj.bias', 'connector.proj.weight', 'connector.proj.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.fc1.bias', 'connector.fc1.weight', 'connector.fc2.bias', 'connector.fc2.weight'])
[[34m2025-08-28 07:35:09[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.query', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.2.weight', 'connector.layer_projectors.2.bias', 'connector.attn.in_proj_weight', 'connector.attn.in_proj_bias', 'connector.attn.out_proj.weight', 'connector.attn.out_proj.bias', 'connector.proj.weight', 'connector.proj.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.fc1.bias', 'connector.fc1.weight', 'connector.fc2.bias', 'connector.fc2.weight'])
[[34m2025-08-28 07:35:10[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:35:27[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:35:49[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:35:49[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:35:50[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:35:50[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-28 07:35:51[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:35:51[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 0.33 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:26[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 3.04 GB
Reserved:  18.45 GB
Max Allocated: 9.26 GB
Max Reserved:  18.45 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:29[0m] 
=== MEMORY STEP 0 - AFTER BACKWARD+UPDATE ===
Allocated: 0.34 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:29[0m] (step=0000000) Train Loss mse: 0.3993, Train Loss ce: 1.7664, Train Steps/Sec: 0.03, 
[[34m2025-08-28 07:36:30[0m] 
=== MEMORY STEP 1 - START ===
Allocated: 0.33 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:30[0m] 
=== MEMORY STEP 1 - AFTER DATA LOAD ===
Allocated: 0.35 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:32[0m] 
=== MEMORY STEP 1 - AFTER FORWARD ===
Allocated: 3.21 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:34[0m] 
=== MEMORY STEP 1 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:35[0m] (step=0000001) Train Loss mse: 0.4218, Train Loss ce: 1.4434, Train Steps/Sec: 0.21, 
[[34m2025-08-28 07:36:35[0m] 
=== MEMORY STEP 2 - START ===
Allocated: 0.33 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:35[0m] 
=== MEMORY STEP 2 - AFTER DATA LOAD ===
Allocated: 0.35 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:36[0m] 
=== MEMORY STEP 2 - AFTER FORWARD ===
Allocated: 3.01 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:39[0m] 
=== MEMORY STEP 2 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:39[0m] (step=0000002) Train Loss mse: 0.4075, Train Loss ce: 0.9364, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:36:39[0m] 
=== MEMORY STEP 3 - START ===
Allocated: 0.33 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:39[0m] 
=== MEMORY STEP 3 - AFTER DATA LOAD ===
Allocated: 0.35 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:40[0m] 
=== MEMORY STEP 3 - AFTER FORWARD ===
Allocated: 3.41 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:43[0m] 
=== MEMORY STEP 3 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-28 07:36:43[0m] (step=0000003) Train Loss mse: 0.4088, Train Loss ce: 1.1870, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:36:48[0m] (step=0000004) Train Loss mse: 0.3789, Train Loss ce: 1.7456, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:36:52[0m] (step=0000005) Train Loss mse: 0.3948, Train Loss ce: 0.4003, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:36:56[0m] (step=0000006) Train Loss mse: 0.4182, Train Loss ce: 1.6882, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:37:01[0m] (step=0000007) Train Loss mse: 0.3921, Train Loss ce: 0.3044, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:37:05[0m] (step=0000008) Train Loss mse: 0.3772, Train Loss ce: 1.1085, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:37:09[0m] (step=0000009) Train Loss mse: 0.4210, Train Loss ce: 1.4197, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:37:14[0m] (step=0000010) Train Loss mse: 0.4332, Train Loss ce: 1.3470, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:37:18[0m] (step=0000011) Train Loss mse: 0.3821, Train Loss ce: 1.4958, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:37:22[0m] (step=0000012) Train Loss mse: 0.3915, Train Loss ce: 1.0502, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:37:27[0m] (step=0000013) Train Loss mse: 0.4014, Train Loss ce: 0.8318, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:37:31[0m] (step=0000014) Train Loss mse: 0.3698, Train Loss ce: 1.0099, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:37:35[0m] (step=0000015) Train Loss mse: 0.3661, Train Loss ce: 1.4339, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:37:40[0m] (step=0000016) Train Loss mse: 0.3348, Train Loss ce: 0.5345, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:37:44[0m] (step=0000017) Train Loss mse: 0.3914, Train Loss ce: 1.6791, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:37:48[0m] (step=0000018) Train Loss mse: 0.3922, Train Loss ce: 1.1355, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:37:53[0m] (step=0000019) Train Loss mse: 0.4108, Train Loss ce: 1.6672, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:37:57[0m] (step=0000020) Train Loss mse: 0.4142, Train Loss ce: 1.1797, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:01[0m] (step=0000021) Train Loss mse: 0.3789, Train Loss ce: 1.2702, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:06[0m] (step=0000022) Train Loss mse: 0.4073, Train Loss ce: 1.1772, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:38:10[0m] (step=0000023) Train Loss mse: 0.3455, Train Loss ce: 0.4478, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:38:14[0m] (step=0000024) Train Loss mse: 0.3955, Train Loss ce: 1.8629, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:18[0m] (step=0000025) Train Loss mse: 0.3936, Train Loss ce: 0.3879, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:38:23[0m] (step=0000026) Train Loss mse: 0.3952, Train Loss ce: 1.2797, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:38:27[0m] (step=0000027) Train Loss mse: 0.4004, Train Loss ce: 1.8487, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:38:31[0m] (step=0000028) Train Loss mse: 0.3906, Train Loss ce: 0.6035, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:38:35[0m] (step=0000029) Train Loss mse: 0.3849, Train Loss ce: 2.1193, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:40[0m] (step=0000030) Train Loss mse: 0.4199, Train Loss ce: 1.9056, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:44[0m] (step=0000031) Train Loss mse: 0.4511, Train Loss ce: 1.8616, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:49[0m] (step=0000032) Train Loss mse: 0.3942, Train Loss ce: 0.8996, Train Steps/Sec: 0.21, 
[[34m2025-08-28 07:38:53[0m] (step=0000033) Train Loss mse: 0.4124, Train Loss ce: 1.4518, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:38:57[0m] (step=0000034) Train Loss mse: 0.3941, Train Loss ce: 1.3450, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:39:01[0m] (step=0000035) Train Loss mse: 0.4300, Train Loss ce: 1.2453, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:39:06[0m] (step=0000036) Train Loss mse: 0.3773, Train Loss ce: 0.9768, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:39:10[0m] (step=0000037) Train Loss mse: 0.3510, Train Loss ce: 1.4573, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:39:14[0m] (step=0000038) Train Loss mse: 0.3810, Train Loss ce: 1.6644, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:39:19[0m] (step=0000039) Train Loss mse: 0.4100, Train Loss ce: 0.9837, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:39:23[0m] (step=0000040) Train Loss mse: 0.3571, Train Loss ce: 1.1971, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:39:27[0m] (step=0000041) Train Loss mse: 0.4108, Train Loss ce: 2.0451, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:39:31[0m] (step=0000042) Train Loss mse: 0.3951, Train Loss ce: 1.2179, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:39:35[0m] (step=0000043) Train Loss mse: 0.4131, Train Loss ce: 1.1077, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:39:40[0m] (step=0000044) Train Loss mse: 0.3845, Train Loss ce: 0.9434, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:39:44[0m] (step=0000045) Train Loss mse: 0.3813, Train Loss ce: 0.7907, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:39:48[0m] (step=0000046) Train Loss mse: 0.3785, Train Loss ce: 0.7761, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:39:53[0m] (step=0000047) Train Loss mse: 0.3860, Train Loss ce: 0.6570, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:39:57[0m] (step=0000048) Train Loss mse: 0.3500, Train Loss ce: 1.4551, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:40:01[0m] (step=0000049) Train Loss mse: 0.3851, Train Loss ce: 1.0150, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:05[0m] (step=0000050) Train Loss mse: 0.3882, Train Loss ce: 1.3801, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:10[0m] (step=0000051) Train Loss mse: 0.4189, Train Loss ce: 1.0086, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:40:14[0m] (step=0000052) Train Loss mse: 0.4086, Train Loss ce: 1.3089, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:40:18[0m] (step=0000053) Train Loss mse: 0.4239, Train Loss ce: 1.3680, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:22[0m] (step=0000054) Train Loss mse: 0.3875, Train Loss ce: 1.4717, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:27[0m] (step=0000055) Train Loss mse: 0.3653, Train Loss ce: 1.1538, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:40:31[0m] (step=0000056) Train Loss mse: 0.4115, Train Loss ce: 1.1420, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:35[0m] (step=0000057) Train Loss mse: 0.4264, Train Loss ce: 1.1213, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:40:39[0m] (step=0000058) Train Loss mse: 0.3831, Train Loss ce: 1.5565, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:43[0m] (step=0000059) Train Loss mse: 0.3844, Train Loss ce: 0.6088, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:48[0m] (step=0000060) Train Loss mse: 0.4259, Train Loss ce: 0.8024, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:40:52[0m] (step=0000061) Train Loss mse: 0.3973, Train Loss ce: 1.2256, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:40:56[0m] (step=0000062) Train Loss mse: 0.3890, Train Loss ce: 1.1310, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:41:01[0m] (step=0000063) Train Loss mse: 0.3410, Train Loss ce: 1.1101, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:41:05[0m] (step=0000064) Train Loss mse: 0.3696, Train Loss ce: 1.3317, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:41:09[0m] (step=0000065) Train Loss mse: 0.3988, Train Loss ce: 0.7317, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:14[0m] (step=0000066) Train Loss mse: 0.3964, Train Loss ce: 0.9572, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:41:18[0m] (step=0000067) Train Loss mse: 0.3721, Train Loss ce: 1.8095, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:22[0m] (step=0000068) Train Loss mse: 0.4042, Train Loss ce: 1.9626, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:26[0m] (step=0000069) Train Loss mse: 0.3893, Train Loss ce: 0.8833, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:30[0m] (step=0000070) Train Loss mse: 0.3999, Train Loss ce: 0.3839, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:35[0m] (step=0000071) Train Loss mse: 0.4367, Train Loss ce: 1.1824, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:41:39[0m] (step=0000072) Train Loss mse: 0.3975, Train Loss ce: 0.8894, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:41:43[0m] (step=0000073) Train Loss mse: 0.4005, Train Loss ce: 1.1682, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:41:47[0m] (step=0000074) Train Loss mse: 0.4187, Train Loss ce: 0.9116, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:52[0m] (step=0000075) Train Loss mse: 0.3961, Train Loss ce: 0.6946, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:41:56[0m] (step=0000076) Train Loss mse: 0.4181, Train Loss ce: 1.0133, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:42:00[0m] (step=0000077) Train Loss mse: 0.3583, Train Loss ce: 1.4312, Train Steps/Sec: 0.26, 
[[34m2025-08-28 07:42:04[0m] (step=0000078) Train Loss mse: 0.3716, Train Loss ce: 1.2769, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:09[0m] (step=0000079) Train Loss mse: 0.3945, Train Loss ce: 0.7339, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:13[0m] (step=0000080) Train Loss mse: 0.3668, Train Loss ce: 1.6717, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:42:17[0m] (step=0000081) Train Loss mse: 0.4117, Train Loss ce: 1.6198, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:21[0m] (step=0000082) Train Loss mse: 0.3990, Train Loss ce: 1.1197, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:26[0m] (step=0000083) Train Loss mse: 0.3968, Train Loss ce: 0.9589, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:30[0m] (step=0000084) Train Loss mse: 0.4198, Train Loss ce: 1.0111, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:34[0m] (step=0000085) Train Loss mse: 0.4153, Train Loss ce: 1.7064, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:39[0m] (step=0000086) Train Loss mse: 0.3964, Train Loss ce: 0.5264, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:42:43[0m] (step=0000087) Train Loss mse: 0.4095, Train Loss ce: 0.9348, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:42:47[0m] (step=0000088) Train Loss mse: 0.4139, Train Loss ce: 1.1449, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:42:51[0m] (step=0000089) Train Loss mse: 0.4030, Train Loss ce: 1.0202, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:42:55[0m] (step=0000090) Train Loss mse: 0.3999, Train Loss ce: 0.6224, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:00[0m] (step=0000091) Train Loss mse: 0.4164, Train Loss ce: 1.0724, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:43:04[0m] (step=0000092) Train Loss mse: 0.4143, Train Loss ce: 0.4522, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:43:08[0m] (step=0000093) Train Loss mse: 0.3922, Train Loss ce: 1.1399, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:12[0m] (step=0000094) Train Loss mse: 0.3879, Train Loss ce: 1.5706, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:43:17[0m] (step=0000095) Train Loss mse: 0.3886, Train Loss ce: 0.8252, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:21[0m] (step=0000096) Train Loss mse: 0.3822, Train Loss ce: 1.4479, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:25[0m] (step=0000097) Train Loss mse: 0.4137, Train Loss ce: 0.6200, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:30[0m] (step=0000098) Train Loss mse: 0.3806, Train Loss ce: 1.1953, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:34[0m] (step=0000099) Train Loss mse: 0.4091, Train Loss ce: 1.4649, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:38[0m] (step=0000100) Train Loss mse: 0.3590, Train Loss ce: 1.4043, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:43[0m] (step=0000101) Train Loss mse: 0.4132, Train Loss ce: 0.6375, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:43:47[0m] (step=0000102) Train Loss mse: 0.3448, Train Loss ce: 0.8524, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:43:51[0m] (step=0000103) Train Loss mse: 0.4248, Train Loss ce: 1.0697, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:43:55[0m] (step=0000104) Train Loss mse: 0.3929, Train Loss ce: 1.6034, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:00[0m] (step=0000105) Train Loss mse: 0.3581, Train Loss ce: 1.6993, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:04[0m] (step=0000106) Train Loss mse: 0.3999, Train Loss ce: 1.2607, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:08[0m] (step=0000107) Train Loss mse: 0.4116, Train Loss ce: 1.3237, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:12[0m] (step=0000108) Train Loss mse: 0.3918, Train Loss ce: 1.3672, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:17[0m] (step=0000109) Train Loss mse: 0.3736, Train Loss ce: 1.5515, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:21[0m] (step=0000110) Train Loss mse: 0.3951, Train Loss ce: 1.0033, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:25[0m] (step=0000111) Train Loss mse: 0.3987, Train Loss ce: 1.3656, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:44:29[0m] (step=0000112) Train Loss mse: 0.4051, Train Loss ce: 1.1735, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:34[0m] (step=0000113) Train Loss mse: 0.4212, Train Loss ce: 1.5594, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:38[0m] (step=0000114) Train Loss mse: 0.4330, Train Loss ce: 0.5672, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:42[0m] (step=0000115) Train Loss mse: 0.4033, Train Loss ce: 1.5244, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:47[0m] (step=0000116) Train Loss mse: 0.3962, Train Loss ce: 1.7240, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:51[0m] (step=0000117) Train Loss mse: 0.4210, Train Loss ce: 1.7264, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:44:55[0m] (step=0000118) Train Loss mse: 0.3871, Train Loss ce: 1.8095, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:44:59[0m] (step=0000119) Train Loss mse: 0.3792, Train Loss ce: 0.7827, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:45:04[0m] (step=0000120) Train Loss mse: 0.3753, Train Loss ce: 0.9862, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:45:08[0m] (step=0000121) Train Loss mse: 0.3943, Train Loss ce: 1.5321, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:12[0m] (step=0000122) Train Loss mse: 0.4157, Train Loss ce: 1.5883, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:16[0m] (step=0000123) Train Loss mse: 0.3798, Train Loss ce: 1.6997, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:20[0m] (step=0000124) Train Loss mse: 0.3534, Train Loss ce: 0.4131, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:45:25[0m] (step=0000125) Train Loss mse: 0.3896, Train Loss ce: 1.6865, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:45:29[0m] (step=0000126) Train Loss mse: 0.3647, Train Loss ce: 0.5915, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:34[0m] (step=0000127) Train Loss mse: 0.4015, Train Loss ce: 1.0943, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:45:38[0m] (step=0000128) Train Loss mse: 0.4178, Train Loss ce: 2.5955, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:42[0m] (step=0000129) Train Loss mse: 0.4179, Train Loss ce: 1.2073, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:45:46[0m] (step=0000130) Train Loss mse: 0.4050, Train Loss ce: 1.6821, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:50[0m] (step=0000131) Train Loss mse: 0.3973, Train Loss ce: 1.5774, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:45:54[0m] (step=0000132) Train Loss mse: 0.3665, Train Loss ce: 1.4072, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:45:58[0m] (step=0000133) Train Loss mse: 0.3879, Train Loss ce: 1.4112, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:46:03[0m] (step=0000134) Train Loss mse: 0.4345, Train Loss ce: 0.8279, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:46:07[0m] (step=0000135) Train Loss mse: 0.3962, Train Loss ce: 0.8302, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:46:11[0m] (step=0000136) Train Loss mse: 0.4398, Train Loss ce: 1.5100, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:46:16[0m] (step=0000137) Train Loss mse: 0.4012, Train Loss ce: 1.3705, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:46:20[0m] (step=0000138) Train Loss mse: 0.3928, Train Loss ce: 1.2834, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:46:24[0m] (step=0000139) Train Loss mse: 0.4003, Train Loss ce: 1.4128, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:46:28[0m] (step=0000140) Train Loss mse: 0.3889, Train Loss ce: 1.7172, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:46:33[0m] (step=0000141) Train Loss mse: 0.4126, Train Loss ce: 0.8850, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:46:37[0m] (step=0000142) Train Loss mse: 0.3780, Train Loss ce: 1.0049, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:46:41[0m] (step=0000143) Train Loss mse: 0.3750, Train Loss ce: 1.4052, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:46:45[0m] (step=0000144) Train Loss mse: 0.4277, Train Loss ce: 1.4728, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:46:49[0m] (step=0000145) Train Loss mse: 0.3820, Train Loss ce: 1.0355, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:46:54[0m] (step=0000146) Train Loss mse: 0.3680, Train Loss ce: 1.2999, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:46:58[0m] (step=0000147) Train Loss mse: 0.3679, Train Loss ce: 0.8648, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:02[0m] (step=0000148) Train Loss mse: 0.3695, Train Loss ce: 1.6164, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:07[0m] (step=0000149) Train Loss mse: 0.4565, Train Loss ce: 0.9753, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:47:11[0m] (step=0000150) Train Loss mse: 0.4450, Train Loss ce: 0.1403, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:15[0m] (step=0000151) Train Loss mse: 0.4171, Train Loss ce: 1.1141, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:19[0m] (step=0000152) Train Loss mse: 0.4471, Train Loss ce: 1.0900, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:23[0m] (step=0000153) Train Loss mse: 0.3951, Train Loss ce: 1.7408, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:47:28[0m] (step=0000154) Train Loss mse: 0.3740, Train Loss ce: 1.1339, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:32[0m] (step=0000155) Train Loss mse: 0.4202, Train Loss ce: 1.1093, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:36[0m] (step=0000156) Train Loss mse: 0.3604, Train Loss ce: 0.7165, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:40[0m] (step=0000157) Train Loss mse: 0.3714, Train Loss ce: 0.7447, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:44[0m] (step=0000158) Train Loss mse: 0.3646, Train Loss ce: 1.1641, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:49[0m] (step=0000159) Train Loss mse: 0.4291, Train Loss ce: 0.8767, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:53[0m] (step=0000160) Train Loss mse: 0.4006, Train Loss ce: 0.5946, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:47:57[0m] (step=0000161) Train Loss mse: 0.4145, Train Loss ce: 1.7785, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:48:01[0m] (step=0000162) Train Loss mse: 0.3909, Train Loss ce: 0.6537, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:48:05[0m] (step=0000163) Train Loss mse: 0.3568, Train Loss ce: 0.4410, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:48:10[0m] (step=0000164) Train Loss mse: 0.4286, Train Loss ce: 1.3013, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:48:14[0m] (step=0000165) Train Loss mse: 0.4334, Train Loss ce: 0.7333, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:48:19[0m] (step=0000166) Train Loss mse: 0.3896, Train Loss ce: 1.1300, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:48:23[0m] (step=0000167) Train Loss mse: 0.4106, Train Loss ce: 0.9149, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:48:27[0m] (step=0000168) Train Loss mse: 0.3931, Train Loss ce: 1.4679, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:48:31[0m] (step=0000169) Train Loss mse: 0.3898, Train Loss ce: 0.9700, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:48:36[0m] (step=0000170) Train Loss mse: 0.3711, Train Loss ce: 1.1288, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:48:40[0m] (step=0000171) Train Loss mse: 0.4212, Train Loss ce: 1.0136, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:48:44[0m] (step=0000172) Train Loss mse: 0.4000, Train Loss ce: 0.9161, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:48:48[0m] (step=0000173) Train Loss mse: 0.3822, Train Loss ce: 1.2045, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:48:53[0m] (step=0000174) Train Loss mse: 0.4253, Train Loss ce: 1.2530, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:48:57[0m] (step=0000175) Train Loss mse: 0.4292, Train Loss ce: 1.8491, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:49:01[0m] (step=0000176) Train Loss mse: 0.4148, Train Loss ce: 0.9662, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:49:05[0m] (step=0000177) Train Loss mse: 0.3980, Train Loss ce: 1.5749, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:49:09[0m] (step=0000178) Train Loss mse: 0.3894, Train Loss ce: 1.0135, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:49:13[0m] (step=0000179) Train Loss mse: 0.4090, Train Loss ce: 0.8982, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:49:18[0m] (step=0000180) Train Loss mse: 0.4098, Train Loss ce: 1.1088, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:49:22[0m] (step=0000181) Train Loss mse: 0.3860, Train Loss ce: 1.0278, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:49:27[0m] (step=0000182) Train Loss mse: 0.3870, Train Loss ce: 1.5817, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:49:31[0m] (step=0000183) Train Loss mse: 0.3796, Train Loss ce: 0.9716, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:49:35[0m] (step=0000184) Train Loss mse: 0.3509, Train Loss ce: 1.7421, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:49:40[0m] (step=0000185) Train Loss mse: 0.3458, Train Loss ce: 0.9891, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:49:44[0m] (step=0000186) Train Loss mse: 0.3843, Train Loss ce: 0.5840, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:49:48[0m] (step=0000187) Train Loss mse: 0.4145, Train Loss ce: 2.4023, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:49:53[0m] (step=0000188) Train Loss mse: 0.3525, Train Loss ce: 1.1056, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:49:57[0m] (step=0000189) Train Loss mse: 0.3656, Train Loss ce: 0.4542, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:01[0m] (step=0000190) Train Loss mse: 0.3809, Train Loss ce: 1.1121, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:50:06[0m] (step=0000191) Train Loss mse: 0.3700, Train Loss ce: 0.8703, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:50:10[0m] (step=0000192) Train Loss mse: 0.3820, Train Loss ce: 1.2695, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:14[0m] (step=0000193) Train Loss mse: 0.3777, Train Loss ce: 1.6235, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:50:18[0m] (step=0000194) Train Loss mse: 0.3868, Train Loss ce: 1.8348, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:23[0m] (step=0000195) Train Loss mse: 0.3960, Train Loss ce: 0.6552, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:27[0m] (step=0000196) Train Loss mse: 0.3949, Train Loss ce: 0.8208, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:50:31[0m] (step=0000197) Train Loss mse: 0.4083, Train Loss ce: 1.2001, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:35[0m] (step=0000198) Train Loss mse: 0.3792, Train Loss ce: 1.4128, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:39[0m] (step=0000199) Train Loss mse: 0.4016, Train Loss ce: 0.8470, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:44[0m] (step=0000200) Train Loss mse: 0.3621, Train Loss ce: 1.1129, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:50:48[0m] (step=0000201) Train Loss mse: 0.3672, Train Loss ce: 1.2633, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:50:52[0m] (step=0000202) Train Loss mse: 0.4263, Train Loss ce: 0.9197, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:50:56[0m] (step=0000203) Train Loss mse: 0.3940, Train Loss ce: 0.9413, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:51:00[0m] (step=0000204) Train Loss mse: 0.3996, Train Loss ce: 1.4916, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:04[0m] (step=0000205) Train Loss mse: 0.4006, Train Loss ce: 0.9285, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:08[0m] (step=0000206) Train Loss mse: 0.4358, Train Loss ce: 1.4706, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:51:12[0m] (step=0000207) Train Loss mse: 0.4094, Train Loss ce: 1.5685, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:17[0m] (step=0000208) Train Loss mse: 0.3826, Train Loss ce: 1.2751, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:51:21[0m] (step=0000209) Train Loss mse: 0.4144, Train Loss ce: 0.9051, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:25[0m] (step=0000210) Train Loss mse: 0.4231, Train Loss ce: 0.8890, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:29[0m] (step=0000211) Train Loss mse: 0.3755, Train Loss ce: 1.1063, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:34[0m] (step=0000212) Train Loss mse: 0.3940, Train Loss ce: 0.7992, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:51:38[0m] (step=0000213) Train Loss mse: 0.4010, Train Loss ce: 1.9985, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:51:42[0m] (step=0000214) Train Loss mse: 0.3685, Train Loss ce: 0.8618, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:47[0m] (step=0000215) Train Loss mse: 0.4012, Train Loss ce: 0.4039, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:51[0m] (step=0000216) Train Loss mse: 0.4095, Train Loss ce: 1.3500, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:51:55[0m] (step=0000217) Train Loss mse: 0.4192, Train Loss ce: 0.8761, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:51:59[0m] (step=0000218) Train Loss mse: 0.3956, Train Loss ce: 0.5870, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:04[0m] (step=0000219) Train Loss mse: 0.3975, Train Loss ce: 1.2699, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:52:08[0m] (step=0000220) Train Loss mse: 0.3819, Train Loss ce: 1.1033, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:12[0m] (step=0000221) Train Loss mse: 0.3727, Train Loss ce: 1.2251, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:17[0m] (step=0000222) Train Loss mse: 0.3834, Train Loss ce: 1.1565, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:21[0m] (step=0000223) Train Loss mse: 0.3875, Train Loss ce: 0.8786, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:25[0m] (step=0000224) Train Loss mse: 0.4142, Train Loss ce: 1.1147, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:29[0m] (step=0000225) Train Loss mse: 0.3916, Train Loss ce: 0.8347, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:52:34[0m] (step=0000226) Train Loss mse: 0.4065, Train Loss ce: 0.5241, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:52:38[0m] (step=0000227) Train Loss mse: 0.4268, Train Loss ce: 1.4351, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:52:42[0m] (step=0000228) Train Loss mse: 0.4219, Train Loss ce: 0.3100, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:46[0m] (step=0000229) Train Loss mse: 0.3956, Train Loss ce: 0.6801, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:51[0m] (step=0000230) Train Loss mse: 0.3599, Train Loss ce: 1.2241, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:52:55[0m] (step=0000231) Train Loss mse: 0.3931, Train Loss ce: 1.7133, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:52:59[0m] (step=0000232) Train Loss mse: 0.3789, Train Loss ce: 1.0600, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:04[0m] (step=0000233) Train Loss mse: 0.4187, Train Loss ce: 0.8168, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:08[0m] (step=0000234) Train Loss mse: 0.3668, Train Loss ce: 0.4898, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:12[0m] (step=0000235) Train Loss mse: 0.3810, Train Loss ce: 0.9923, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:17[0m] (step=0000236) Train Loss mse: 0.3790, Train Loss ce: 2.1762, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:21[0m] (step=0000237) Train Loss mse: 0.3783, Train Loss ce: 1.4046, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:25[0m] (step=0000238) Train Loss mse: 0.3808, Train Loss ce: 1.6085, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:29[0m] (step=0000239) Train Loss mse: 0.4482, Train Loss ce: 0.8675, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:33[0m] (step=0000240) Train Loss mse: 0.4023, Train Loss ce: 1.7539, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:38[0m] (step=0000241) Train Loss mse: 0.3626, Train Loss ce: 1.1950, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:42[0m] (step=0000242) Train Loss mse: 0.3509, Train Loss ce: 0.9276, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:46[0m] (step=0000243) Train Loss mse: 0.3649, Train Loss ce: 0.3702, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:51[0m] (step=0000244) Train Loss mse: 0.3925, Train Loss ce: 1.4189, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:53:55[0m] (step=0000245) Train Loss mse: 0.3564, Train Loss ce: 1.3573, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:53:59[0m] (step=0000246) Train Loss mse: 0.3968, Train Loss ce: 0.7809, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:54:03[0m] (step=0000247) Train Loss mse: 0.3893, Train Loss ce: 1.3960, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:54:08[0m] (step=0000248) Train Loss mse: 0.3808, Train Loss ce: 0.9851, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:12[0m] (step=0000249) Train Loss mse: 0.4036, Train Loss ce: 0.8812, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:54:16[0m] (step=0000250) Train Loss mse: 0.4237, Train Loss ce: 0.8811, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:21[0m] (step=0000251) Train Loss mse: 0.3464, Train Loss ce: 1.1809, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:25[0m] (step=0000252) Train Loss mse: 0.3624, Train Loss ce: 0.7568, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:54:29[0m] (step=0000253) Train Loss mse: 0.3753, Train Loss ce: 0.9020, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:34[0m] (step=0000254) Train Loss mse: 0.4128, Train Loss ce: 2.7732, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:38[0m] (step=0000255) Train Loss mse: 0.3687, Train Loss ce: 1.0063, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:42[0m] (step=0000256) Train Loss mse: 0.3536, Train Loss ce: 1.5540, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:47[0m] (step=0000257) Train Loss mse: 0.3824, Train Loss ce: 1.0214, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:51[0m] (step=0000258) Train Loss mse: 0.3672, Train Loss ce: 1.2854, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:54:56[0m] (step=0000259) Train Loss mse: 0.4061, Train Loss ce: 1.3427, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:55:00[0m] (step=0000260) Train Loss mse: 0.3910, Train Loss ce: 0.6042, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:55:04[0m] (step=0000261) Train Loss mse: 0.3869, Train Loss ce: 1.4963, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:09[0m] (step=0000262) Train Loss mse: 0.3783, Train Loss ce: 2.7382, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:13[0m] (step=0000263) Train Loss mse: 0.3821, Train Loss ce: 1.4157, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:17[0m] (step=0000264) Train Loss mse: 0.3661, Train Loss ce: 1.2761, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:21[0m] (step=0000265) Train Loss mse: 0.3910, Train Loss ce: 1.4116, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:25[0m] (step=0000266) Train Loss mse: 0.3872, Train Loss ce: 1.6202, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:30[0m] (step=0000267) Train Loss mse: 0.4367, Train Loss ce: 1.1868, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:55:34[0m] (step=0000268) Train Loss mse: 0.3829, Train Loss ce: 1.1209, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:38[0m] (step=0000269) Train Loss mse: 0.4211, Train Loss ce: 2.1742, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:42[0m] (step=0000270) Train Loss mse: 0.3846, Train Loss ce: 1.6072, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:55:47[0m] (step=0000271) Train Loss mse: 0.4093, Train Loss ce: 1.7920, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:55:51[0m] (step=0000272) Train Loss mse: 0.3836, Train Loss ce: 0.7910, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:55:55[0m] (step=0000273) Train Loss mse: 0.3953, Train Loss ce: 1.3277, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:55:59[0m] (step=0000274) Train Loss mse: 0.4224, Train Loss ce: 0.8647, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:56:04[0m] (step=0000275) Train Loss mse: 0.3807, Train Loss ce: 1.1640, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:56:08[0m] (step=0000276) Train Loss mse: 0.4303, Train Loss ce: 1.0643, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:56:12[0m] (step=0000277) Train Loss mse: 0.4166, Train Loss ce: 1.3789, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:56:16[0m] (step=0000278) Train Loss mse: 0.3576, Train Loss ce: 0.9048, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:56:20[0m] (step=0000279) Train Loss mse: 0.3743, Train Loss ce: 1.0516, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:56:25[0m] (step=0000280) Train Loss mse: 0.3907, Train Loss ce: 1.2688, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:56:29[0m] (step=0000281) Train Loss mse: 0.3829, Train Loss ce: 0.4018, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:56:33[0m] (step=0000282) Train Loss mse: 0.3704, Train Loss ce: 1.3074, Train Steps/Sec: 0.26, 
[[34m2025-08-28 07:56:37[0m] (step=0000283) Train Loss mse: 0.3783, Train Loss ce: 1.1303, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:56:41[0m] (step=0000284) Train Loss mse: 0.3900, Train Loss ce: 0.9029, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:56:45[0m] (step=0000285) Train Loss mse: 0.4130, Train Loss ce: 1.3695, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:56:50[0m] (step=0000286) Train Loss mse: 0.3886, Train Loss ce: 1.6115, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:56:54[0m] (step=0000287) Train Loss mse: 0.4266, Train Loss ce: 1.4530, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:56:58[0m] (step=0000288) Train Loss mse: 0.4027, Train Loss ce: 0.7247, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:02[0m] (step=0000289) Train Loss mse: 0.3682, Train Loss ce: 0.7035, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:06[0m] (step=0000290) Train Loss mse: 0.3910, Train Loss ce: 0.4875, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:11[0m] (step=0000291) Train Loss mse: 0.3870, Train Loss ce: 0.6069, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:15[0m] (step=0000292) Train Loss mse: 0.3526, Train Loss ce: 1.0364, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:57:19[0m] (step=0000293) Train Loss mse: 0.3972, Train Loss ce: 1.4991, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:57:23[0m] (step=0000294) Train Loss mse: 0.4106, Train Loss ce: 1.1714, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:57:27[0m] (step=0000295) Train Loss mse: 0.3701, Train Loss ce: 1.1164, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:32[0m] (step=0000296) Train Loss mse: 0.3813, Train Loss ce: 1.3754, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:36[0m] (step=0000297) Train Loss mse: 0.3841, Train Loss ce: 1.3796, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:57:40[0m] (step=0000298) Train Loss mse: 0.3771, Train Loss ce: 0.6175, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:44[0m] (step=0000299) Train Loss mse: 0.3877, Train Loss ce: 0.5838, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:57:49[0m] (step=0000300) Train Loss mse: 0.4026, Train Loss ce: 1.2437, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:57:53[0m] (step=0000301) Train Loss mse: 0.3788, Train Loss ce: 1.4549, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:57:57[0m] (step=0000302) Train Loss mse: 0.3649, Train Loss ce: 0.7067, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:01[0m] (step=0000303) Train Loss mse: 0.3720, Train Loss ce: 0.8012, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:05[0m] (step=0000304) Train Loss mse: 0.3649, Train Loss ce: 0.7206, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:10[0m] (step=0000305) Train Loss mse: 0.4048, Train Loss ce: 0.6123, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:58:14[0m] (step=0000306) Train Loss mse: 0.3690, Train Loss ce: 1.2693, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:18[0m] (step=0000307) Train Loss mse: 0.4002, Train Loss ce: 0.4753, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:23[0m] (step=0000308) Train Loss mse: 0.4003, Train Loss ce: 2.3042, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:58:27[0m] (step=0000309) Train Loss mse: 0.3937, Train Loss ce: 1.5048, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:58:31[0m] (step=0000310) Train Loss mse: 0.3796, Train Loss ce: 1.1982, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:58:36[0m] (step=0000311) Train Loss mse: 0.4026, Train Loss ce: 0.7008, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:58:40[0m] (step=0000312) Train Loss mse: 0.3928, Train Loss ce: 1.3906, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:45[0m] (step=0000313) Train Loss mse: 0.3647, Train Loss ce: 0.5274, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:58:49[0m] (step=0000314) Train Loss mse: 0.3912, Train Loss ce: 0.9863, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:58:53[0m] (step=0000315) Train Loss mse: 0.4082, Train Loss ce: 1.1749, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:58:58[0m] (step=0000316) Train Loss mse: 0.3805, Train Loss ce: 0.7270, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:02[0m] (step=0000317) Train Loss mse: 0.3726, Train Loss ce: 1.3849, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:06[0m] (step=0000318) Train Loss mse: 0.3830, Train Loss ce: 0.9244, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:10[0m] (step=0000319) Train Loss mse: 0.3708, Train Loss ce: 1.3716, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:59:15[0m] (step=0000320) Train Loss mse: 0.3755, Train Loss ce: 1.2479, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:19[0m] (step=0000321) Train Loss mse: 0.4148, Train Loss ce: 1.4853, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:23[0m] (step=0000322) Train Loss mse: 0.3733, Train Loss ce: 1.0051, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:59:28[0m] (step=0000323) Train Loss mse: 0.3655, Train Loss ce: 0.8541, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:32[0m] (step=0000324) Train Loss mse: 0.3914, Train Loss ce: 1.8011, Train Steps/Sec: 0.22, 
[[34m2025-08-28 07:59:36[0m] (step=0000325) Train Loss mse: 0.3765, Train Loss ce: 1.5596, Train Steps/Sec: 0.24, 
[[34m2025-08-28 07:59:40[0m] (step=0000326) Train Loss mse: 0.3884, Train Loss ce: 1.2623, Train Steps/Sec: 0.25, 
[[34m2025-08-28 07:59:45[0m] (step=0000327) Train Loss mse: 0.3960, Train Loss ce: 1.5582, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:49[0m] (step=0000328) Train Loss mse: 0.3860, Train Loss ce: 0.7398, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:53[0m] (step=0000329) Train Loss mse: 0.3774, Train Loss ce: 1.0381, Train Steps/Sec: 0.23, 
[[34m2025-08-28 07:59:58[0m] (step=0000330) Train Loss mse: 0.4087, Train Loss ce: 0.5787, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:02[0m] (step=0000331) Train Loss mse: 0.4131, Train Loss ce: 0.1345, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:00:06[0m] (step=0000332) Train Loss mse: 0.4215, Train Loss ce: 1.4385, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:00:10[0m] (step=0000333) Train Loss mse: 0.3895, Train Loss ce: 0.6405, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:15[0m] (step=0000334) Train Loss mse: 0.3909, Train Loss ce: 1.1472, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:19[0m] (step=0000335) Train Loss mse: 0.3716, Train Loss ce: 0.6072, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:23[0m] (step=0000336) Train Loss mse: 0.3686, Train Loss ce: 1.1170, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:00:27[0m] (step=0000337) Train Loss mse: 0.3694, Train Loss ce: 1.2364, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:32[0m] (step=0000338) Train Loss mse: 0.3886, Train Loss ce: 1.6770, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:00:36[0m] (step=0000339) Train Loss mse: 0.3741, Train Loss ce: 1.0718, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:40[0m] (step=0000340) Train Loss mse: 0.3762, Train Loss ce: 1.1899, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:45[0m] (step=0000341) Train Loss mse: 0.3931, Train Loss ce: 1.0649, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:00:49[0m] (step=0000342) Train Loss mse: 0.3662, Train Loss ce: 1.8770, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:00:53[0m] (step=0000343) Train Loss mse: 0.3509, Train Loss ce: 0.9260, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:00:57[0m] (step=0000344) Train Loss mse: 0.4072, Train Loss ce: 0.4202, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:01:01[0m] (step=0000345) Train Loss mse: 0.3780, Train Loss ce: 0.9781, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:01:06[0m] (step=0000346) Train Loss mse: 0.3783, Train Loss ce: 1.1659, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:01:10[0m] (step=0000347) Train Loss mse: 0.3903, Train Loss ce: 0.9461, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:01:14[0m] (step=0000348) Train Loss mse: 0.3909, Train Loss ce: 0.8895, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:01:19[0m] (step=0000349) Train Loss mse: 0.3955, Train Loss ce: 1.5982, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:01:23[0m] (step=0000350) Train Loss mse: 0.3771, Train Loss ce: 1.2050, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:01:27[0m] (step=0000351) Train Loss mse: 0.3748, Train Loss ce: 0.9935, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:01:32[0m] (step=0000352) Train Loss mse: 0.4020, Train Loss ce: 0.7817, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:01:36[0m] (step=0000353) Train Loss mse: 0.4037, Train Loss ce: 1.5029, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:01:40[0m] (step=0000354) Train Loss mse: 0.4133, Train Loss ce: 1.8950, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:01:44[0m] (step=0000355) Train Loss mse: 0.4278, Train Loss ce: 1.0809, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:01:49[0m] (step=0000356) Train Loss mse: 0.3853, Train Loss ce: 1.0980, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:01:53[0m] (step=0000357) Train Loss mse: 0.3650, Train Loss ce: 0.9179, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:01:58[0m] (step=0000358) Train Loss mse: 0.4216, Train Loss ce: 1.2354, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:02:02[0m] (step=0000359) Train Loss mse: 0.4372, Train Loss ce: 1.6853, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:02:07[0m] (step=0000360) Train Loss mse: 0.4070, Train Loss ce: 0.7981, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:02:11[0m] (step=0000361) Train Loss mse: 0.3852, Train Loss ce: 0.6890, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:02:16[0m] (step=0000362) Train Loss mse: 0.4030, Train Loss ce: 0.8922, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:02:20[0m] (step=0000363) Train Loss mse: 0.3755, Train Loss ce: 0.7743, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:24[0m] (step=0000364) Train Loss mse: 0.3873, Train Loss ce: 0.9201, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:28[0m] (step=0000365) Train Loss mse: 0.4172, Train Loss ce: 0.6041, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:32[0m] (step=0000366) Train Loss mse: 0.4221, Train Loss ce: 1.3435, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:37[0m] (step=0000367) Train Loss mse: 0.3891, Train Loss ce: 1.2991, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:41[0m] (step=0000368) Train Loss mse: 0.3798, Train Loss ce: 0.7260, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:02:45[0m] (step=0000369) Train Loss mse: 0.3848, Train Loss ce: 1.1161, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:02:50[0m] (step=0000370) Train Loss mse: 0.3873, Train Loss ce: 1.0389, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:54[0m] (step=0000371) Train Loss mse: 0.4175, Train Loss ce: 0.5327, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:02:58[0m] (step=0000372) Train Loss mse: 0.3583, Train Loss ce: 0.9106, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:02[0m] (step=0000373) Train Loss mse: 0.4008, Train Loss ce: 1.7226, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:03:07[0m] (step=0000374) Train Loss mse: 0.4190, Train Loss ce: 0.6297, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:11[0m] (step=0000375) Train Loss mse: 0.4112, Train Loss ce: 1.8736, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:15[0m] (step=0000376) Train Loss mse: 0.3635, Train Loss ce: 0.6512, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:20[0m] (step=0000377) Train Loss mse: 0.3899, Train Loss ce: 0.8350, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:03:24[0m] (step=0000378) Train Loss mse: 0.4264, Train Loss ce: 0.7403, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:28[0m] (step=0000379) Train Loss mse: 0.3972, Train Loss ce: 0.7733, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:03:33[0m] (step=0000380) Train Loss mse: 0.3543, Train Loss ce: 1.7000, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:37[0m] (step=0000381) Train Loss mse: 0.3954, Train Loss ce: 1.4449, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:41[0m] (step=0000382) Train Loss mse: 0.3973, Train Loss ce: 1.3941, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:03:46[0m] (step=0000383) Train Loss mse: 0.3911, Train Loss ce: 1.0408, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:03:50[0m] (step=0000384) Train Loss mse: 0.3758, Train Loss ce: 1.3621, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:03:54[0m] (step=0000385) Train Loss mse: 0.3703, Train Loss ce: 0.8690, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:03:58[0m] (step=0000386) Train Loss mse: 0.3924, Train Loss ce: 0.8465, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:04:02[0m] (step=0000387) Train Loss mse: 0.3567, Train Loss ce: 0.7754, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:04:06[0m] (step=0000388) Train Loss mse: 0.3887, Train Loss ce: 0.9084, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:04:11[0m] (step=0000389) Train Loss mse: 0.3924, Train Loss ce: 1.0044, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:04:15[0m] (step=0000390) Train Loss mse: 0.3796, Train Loss ce: 1.1210, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:04:19[0m] (step=0000391) Train Loss mse: 0.3930, Train Loss ce: 1.4210, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:04:23[0m] (step=0000392) Train Loss mse: 0.3822, Train Loss ce: 1.0386, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:04:28[0m] (step=0000393) Train Loss mse: 0.3803, Train Loss ce: 0.7149, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:04:32[0m] (step=0000394) Train Loss mse: 0.4025, Train Loss ce: 1.5403, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:04:36[0m] (step=0000395) Train Loss mse: 0.3828, Train Loss ce: 1.5624, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:04:40[0m] (step=0000396) Train Loss mse: 0.3852, Train Loss ce: 1.3844, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:04:45[0m] (step=0000397) Train Loss mse: 0.3926, Train Loss ce: 0.8014, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:04:49[0m] (step=0000398) Train Loss mse: 0.3849, Train Loss ce: 0.4576, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:04:53[0m] (step=0000399) Train Loss mse: 0.4126, Train Loss ce: 1.6762, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:04:57[0m] (step=0000400) Train Loss mse: 0.3715, Train Loss ce: 1.2945, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:05:02[0m] (step=0000401) Train Loss mse: 0.3744, Train Loss ce: 1.1257, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:05:06[0m] (step=0000402) Train Loss mse: 0.3701, Train Loss ce: 1.4976, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:05:11[0m] (step=0000403) Train Loss mse: 0.3785, Train Loss ce: 1.5076, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:05:15[0m] (step=0000404) Train Loss mse: 0.3871, Train Loss ce: 0.9617, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:05:19[0m] (step=0000405) Train Loss mse: 0.3715, Train Loss ce: 0.7734, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:05:23[0m] (step=0000406) Train Loss mse: 0.4418, Train Loss ce: 1.0622, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:05:27[0m] (step=0000407) Train Loss mse: 0.3811, Train Loss ce: 1.6212, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:05:31[0m] (step=0000408) Train Loss mse: 0.3420, Train Loss ce: 1.2184, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:05:36[0m] (step=0000409) Train Loss mse: 0.4104, Train Loss ce: 1.6173, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:05:40[0m] (step=0000410) Train Loss mse: 0.4141, Train Loss ce: 1.4170, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:05:44[0m] (step=0000411) Train Loss mse: 0.3956, Train Loss ce: 1.2934, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:05:48[0m] (step=0000412) Train Loss mse: 0.3848, Train Loss ce: 1.5594, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:05:52[0m] (step=0000413) Train Loss mse: 0.4003, Train Loss ce: 1.2094, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:05:57[0m] (step=0000414) Train Loss mse: 0.4086, Train Loss ce: 0.7310, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:01[0m] (step=0000415) Train Loss mse: 0.4140, Train Loss ce: 1.5706, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:06:05[0m] (step=0000416) Train Loss mse: 0.3938, Train Loss ce: 2.3037, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:09[0m] (step=0000417) Train Loss mse: 0.4111, Train Loss ce: 1.4374, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:13[0m] (step=0000418) Train Loss mse: 0.3932, Train Loss ce: 0.6555, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:06:18[0m] (step=0000419) Train Loss mse: 0.3333, Train Loss ce: 1.0744, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:06:22[0m] (step=0000420) Train Loss mse: 0.3764, Train Loss ce: 1.1065, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:06:26[0m] (step=0000421) Train Loss mse: 0.4096, Train Loss ce: 1.8947, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:31[0m] (step=0000422) Train Loss mse: 0.4237, Train Loss ce: 1.8361, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:06:35[0m] (step=0000423) Train Loss mse: 0.4109, Train Loss ce: 1.1991, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:39[0m] (step=0000424) Train Loss mse: 0.3771, Train Loss ce: 1.0329, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:43[0m] (step=0000425) Train Loss mse: 0.4041, Train Loss ce: 0.7015, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:06:47[0m] (step=0000426) Train Loss mse: 0.3864, Train Loss ce: 1.0886, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:06:52[0m] (step=0000427) Train Loss mse: 0.3672, Train Loss ce: 1.3569, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:06:56[0m] (step=0000428) Train Loss mse: 0.3733, Train Loss ce: 1.2560, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:07:01[0m] (step=0000429) Train Loss mse: 0.3825, Train Loss ce: 1.1190, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:07:05[0m] (step=0000430) Train Loss mse: 0.4104, Train Loss ce: 1.0970, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:07:09[0m] (step=0000431) Train Loss mse: 0.3756, Train Loss ce: 0.8206, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:07:13[0m] (step=0000432) Train Loss mse: 0.3622, Train Loss ce: 0.6523, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:07:18[0m] (step=0000433) Train Loss mse: 0.4101, Train Loss ce: 0.9712, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:07:22[0m] (step=0000434) Train Loss mse: 0.3944, Train Loss ce: 1.1538, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:07:26[0m] (step=0000435) Train Loss mse: 0.3443, Train Loss ce: 1.0740, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:07:30[0m] (step=0000436) Train Loss mse: 0.3993, Train Loss ce: 0.9752, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:07:34[0m] (step=0000437) Train Loss mse: 0.3875, Train Loss ce: 0.9034, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:07:39[0m] (step=0000438) Train Loss mse: 0.3573, Train Loss ce: 1.2483, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:07:43[0m] (step=0000439) Train Loss mse: 0.3660, Train Loss ce: 1.1415, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:07:47[0m] (step=0000440) Train Loss mse: 0.4291, Train Loss ce: 0.9496, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:07:51[0m] (step=0000441) Train Loss mse: 0.3987, Train Loss ce: 1.4184, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:07:56[0m] (step=0000442) Train Loss mse: 0.3754, Train Loss ce: 1.1597, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:00[0m] (step=0000443) Train Loss mse: 0.3578, Train Loss ce: 1.5130, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:04[0m] (step=0000444) Train Loss mse: 0.3662, Train Loss ce: 0.7490, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:08:08[0m] (step=0000445) Train Loss mse: 0.3740, Train Loss ce: 0.6116, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:08:12[0m] (step=0000446) Train Loss mse: 0.3978, Train Loss ce: 0.6307, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:17[0m] (step=0000447) Train Loss mse: 0.4089, Train Loss ce: 0.5540, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:21[0m] (step=0000448) Train Loss mse: 0.3836, Train Loss ce: 0.3367, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:08:26[0m] (step=0000449) Train Loss mse: 0.3870, Train Loss ce: 1.5937, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:30[0m] (step=0000450) Train Loss mse: 0.3589, Train Loss ce: 0.7853, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:34[0m] (step=0000451) Train Loss mse: 0.3505, Train Loss ce: 0.6689, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:08:39[0m] (step=0000452) Train Loss mse: 0.3646, Train Loss ce: 1.7939, Train Steps/Sec: 0.22, 
[[34m2025-08-28 08:08:43[0m] (step=0000453) Train Loss mse: 0.3897, Train Loss ce: 1.0599, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:08:47[0m] (step=0000454) Train Loss mse: 0.3604, Train Loss ce: 1.3319, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:08:52[0m] (step=0000455) Train Loss mse: 0.4085, Train Loss ce: 1.1969, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:08:56[0m] (step=0000456) Train Loss mse: 0.3996, Train Loss ce: 1.4655, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:09:00[0m] (step=0000457) Train Loss mse: 0.4008, Train Loss ce: 0.6721, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:04[0m] (step=0000458) Train Loss mse: 0.3610, Train Loss ce: 0.6621, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:09[0m] (step=0000459) Train Loss mse: 0.3850, Train Loss ce: 1.0525, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:13[0m] (step=0000460) Train Loss mse: 0.3869, Train Loss ce: 1.2003, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:17[0m] (step=0000461) Train Loss mse: 0.3706, Train Loss ce: 1.8084, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:09:21[0m] (step=0000462) Train Loss mse: 0.3653, Train Loss ce: 0.9261, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:25[0m] (step=0000463) Train Loss mse: 0.3848, Train Loss ce: 1.3378, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:09:30[0m] (step=0000464) Train Loss mse: 0.3939, Train Loss ce: 1.7923, Train Steps/Sec: 0.23, 
[[34m2025-08-28 08:09:34[0m] (step=0000465) Train Loss mse: 0.3732, Train Loss ce: 0.8932, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:38[0m] (step=0000466) Train Loss mse: 0.4336, Train Loss ce: 1.0112, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:42[0m] (step=0000467) Train Loss mse: 0.3970, Train Loss ce: 1.0863, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:46[0m] (step=0000468) Train Loss mse: 0.4175, Train Loss ce: 1.2660, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:51[0m] (step=0000469) Train Loss mse: 0.3797, Train Loss ce: 0.1717, Train Steps/Sec: 0.25, 
[[34m2025-08-28 08:09:55[0m] (step=0000470) Train Loss mse: 0.3937, Train Loss ce: 0.6859, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:09:59[0m] (step=0000471) Train Loss mse: 0.3916, Train Loss ce: 0.4962, Train Steps/Sec: 0.24, 
[[34m2025-08-28 08:10:03[0m] (step=0000472) Train Loss mse: 0.3654, Train Loss ce: 0.9705, Train Steps/Sec: 0.25, 
[[34m2025-08-29 06:54:51[0m] Failed to initialize wandb: Attempted to change value of key "checkpoint_dir" from results/checkpoints to /home/haoming/Bagel/results/checkpoints
If you really want to do this, pass allow_val_change=True to config.update(). Continuing without wandb.
[[34m2025-08-29 06:54:51[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-29 06:54:51[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-29 06:54:51[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 06:54:51[0m] enabling finetuning
[[34m2025-08-29 06:54:51[0m] set up language model
[[34m2025-08-29 06:57:37[0m] using visual_und
[[34m2025-08-29 06:57:42[0m] using generation
[[34m2025-08-29 06:57:43[0m] model loaded
[[34m2025-08-29 06:57:44[0m] tokenizers loaded
[[34m2025-08-29 06:57:44[0m] frozen done
[[34m2025-08-29 06:57:44[0m] 
============================================================
[[34m2025-08-29 06:57:44[0m] MODEL PARAMETER SUMMARY
[[34m2025-08-29 06:57:44[0m] ============================================================
[[34m2025-08-29 06:57:44[0m] Language Model (LLM):
[[34m2025-08-29 06:57:44[0m]   Total:     14,141,252,608 parameters
[[34m2025-08-29 06:57:44[0m]   Trainable: 0 parameters
[[34m2025-08-29 06:57:44[0m]   Frozen:    14,141,252,608 parameters
[[34m2025-08-29 06:57:44[0m] 
Vision Model (ViT):
[[34m2025-08-29 06:57:44[0m]   Total:     402,552,736 parameters
[[34m2025-08-29 06:57:44[0m]   Trainable: 402,552,736 parameters
[[34m2025-08-29 06:57:44[0m]   Frozen:    0 parameters
[[34m2025-08-29 06:57:44[0m] 
Connector/Projector:
[[34m2025-08-29 06:57:44[0m]   Total:     13,431,296 parameters
[[34m2025-08-29 06:57:44[0m]   Trainable: 13,431,296 parameters
[[34m2025-08-29 06:57:44[0m]   Frozen:    0 parameters
[[34m2025-08-29 06:57:44[0m] 
VAE Model:
[[34m2025-08-29 06:57:44[0m]   Total:     83,819,683 parameters
[[34m2025-08-29 06:57:44[0m]   Trainable: 0 parameters
[[34m2025-08-29 06:57:44[0m]   Frozen:    83,819,683 parameters
[[34m2025-08-29 06:57:44[0m] 
ViT Position Embeddings:
[[34m2025-08-29 06:57:44[0m]   Total:     8,960,000 parameters
[[34m2025-08-29 06:57:44[0m]   Trainable: 0 parameters
[[34m2025-08-29 06:57:44[0m]   Frozen:    8,960,000 parameters
[[34m2025-08-29 06:57:44[0m] 
Total Bagel Model:
[[34m2025-08-29 06:57:44[0m]   Total:     14,588,686,304 parameters
[[34m2025-08-29 06:57:44[0m]   Trainable: 430,216,160 parameters
[[34m2025-08-29 06:57:44[0m]   Frozen:    14,158,470,144 parameters
[[34m2025-08-29 06:57:44[0m] 
Training 2.9% of total parameters
[[34m2025-08-29 06:57:44[0m] ============================================================
[[34m2025-08-29 06:57:44[0m] fsdp config done
[[34m2025-08-29 06:57:49[0m] Training from scratch.
[[34m2025-08-29 06:59:24[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 06:59:27[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=2000, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-29 06:59:27[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-29 06:59:27[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 07:03:26[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:03:33[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:03:34[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:03:35[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:03:57[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-29 07:04:05[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.query', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.2.weight', 'connector.layer_projectors.2.bias', 'connector.attn.in_proj_weight', 'connector.attn.in_proj_bias', 'connector.attn.out_proj.weight', 'connector.attn.out_proj.bias', 'connector.proj.weight', 'connector.proj.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.fc1.bias', 'connector.fc1.weight', 'connector.fc2.bias', 'connector.fc2.weight'])
[[34m2025-08-29 07:04:12[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.query', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.2.weight', 'connector.layer_projectors.2.bias', 'connector.attn.in_proj_weight', 'connector.attn.in_proj_bias', 'connector.attn.out_proj.weight', 'connector.attn.out_proj.bias', 'connector.proj.weight', 'connector.proj.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.fc1.bias', 'connector.fc1.weight', 'connector.fc2.bias', 'connector.fc2.weight'])
[[34m2025-08-29 07:04:13[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:04:27[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:04:44[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:04:44[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:04:44[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:04:44[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-29 07:04:46[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:04:46[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 0.33 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:07[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 3.04 GB
Reserved:  18.45 GB
Max Allocated: 9.26 GB
Max Reserved:  18.45 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:11[0m] 
=== MEMORY STEP 0 - AFTER BACKWARD+UPDATE ===
Allocated: 0.34 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:11[0m] (step=0000000) Train Loss mse: 0.3993, Train Loss ce: 1.7664, Train Steps/Sec: 0.04, 
[[34m2025-08-29 07:05:12[0m] 
=== MEMORY STEP 1 - START ===
Allocated: 0.33 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:12[0m] 
=== MEMORY STEP 1 - AFTER DATA LOAD ===
Allocated: 0.35 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:13[0m] 
=== MEMORY STEP 1 - AFTER FORWARD ===
Allocated: 3.21 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:15[0m] 
=== MEMORY STEP 1 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:16[0m] (step=0000001) Train Loss mse: 0.4218, Train Loss ce: 1.4434, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:05:16[0m] 
=== MEMORY STEP 2 - START ===
Allocated: 0.33 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:16[0m] 
=== MEMORY STEP 2 - AFTER DATA LOAD ===
Allocated: 0.35 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:17[0m] 
=== MEMORY STEP 2 - AFTER FORWARD ===
Allocated: 3.01 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:19[0m] 
=== MEMORY STEP 2 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:20[0m] (step=0000002) Train Loss mse: 0.4075, Train Loss ce: 0.9328, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:05:20[0m] 
=== MEMORY STEP 3 - START ===
Allocated: 0.33 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:20[0m] 
=== MEMORY STEP 3 - AFTER DATA LOAD ===
Allocated: 0.35 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:21[0m] 
=== MEMORY STEP 3 - AFTER FORWARD ===
Allocated: 3.41 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:24[0m] 
=== MEMORY STEP 3 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:05:24[0m] (step=0000003) Train Loss mse: 0.4088, Train Loss ce: 1.1887, Train Steps/Sec: 0.23, 
[[34m2025-08-29 07:05:28[0m] (step=0000004) Train Loss mse: 0.3789, Train Loss ce: 1.7431, Train Steps/Sec: 0.23, 
[[34m2025-08-29 07:05:33[0m] (step=0000005) Train Loss mse: 0.3948, Train Loss ce: 0.3973, Train Steps/Sec: 0.23, 
[[34m2025-08-29 07:05:37[0m] (step=0000006) Train Loss mse: 0.4183, Train Loss ce: 1.6872, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:05:41[0m] (step=0000007) Train Loss mse: 0.3921, Train Loss ce: 0.3031, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:05:45[0m] (step=0000008) Train Loss mse: 0.3772, Train Loss ce: 1.1114, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:05:49[0m] (step=0000009) Train Loss mse: 0.4210, Train Loss ce: 1.4185, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:05:53[0m] (step=0000010) Train Loss mse: 0.4332, Train Loss ce: 1.3458, Train Steps/Sec: 0.23, 
[[34m2025-08-29 07:05:57[0m] (step=0000011) Train Loss mse: 0.3821, Train Loss ce: 1.4966, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:06:02[0m] (step=0000012) Train Loss mse: 0.3915, Train Loss ce: 1.0473, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:06:06[0m] (step=0000013) Train Loss mse: 0.4014, Train Loss ce: 0.8384, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:10[0m] (step=0000014) Train Loss mse: 0.3698, Train Loss ce: 1.0098, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:14[0m] (step=0000015) Train Loss mse: 0.3660, Train Loss ce: 1.4378, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:18[0m] (step=0000016) Train Loss mse: 0.3347, Train Loss ce: 0.5350, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:22[0m] (step=0000017) Train Loss mse: 0.3914, Train Loss ce: 1.6802, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:06:26[0m] (step=0000018) Train Loss mse: 0.3922, Train Loss ce: 1.1368, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:06:30[0m] (step=0000019) Train Loss mse: 0.4109, Train Loss ce: 1.6697, Train Steps/Sec: 0.23, 
[[34m2025-08-29 07:06:35[0m] (step=0000020) Train Loss mse: 0.4142, Train Loss ce: 1.1788, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:06:39[0m] (step=0000021) Train Loss mse: 0.3789, Train Loss ce: 1.2713, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:06:43[0m] (step=0000022) Train Loss mse: 0.4073, Train Loss ce: 1.1735, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:47[0m] (step=0000023) Train Loss mse: 0.3455, Train Loss ce: 0.4513, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:51[0m] (step=0000024) Train Loss mse: 0.3955, Train Loss ce: 1.8570, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:55[0m] (step=0000025) Train Loss mse: 0.3936, Train Loss ce: 0.3872, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:06:59[0m] (step=0000026) Train Loss mse: 0.3953, Train Loss ce: 1.2746, Train Steps/Sec: 0.23, 
[[34m2025-08-29 07:07:03[0m] (step=0000027) Train Loss mse: 0.4004, Train Loss ce: 1.8514, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:07:07[0m] (step=0000028) Train Loss mse: 0.3905, Train Loss ce: 0.6044, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:07:11[0m] (step=0000029) Train Loss mse: 0.3849, Train Loss ce: 2.1201, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:07:15[0m] (step=0000030) Train Loss mse: 0.4199, Train Loss ce: 1.9029, Train Steps/Sec: 0.25, 
[[34m2025-08-29 07:07:19[0m] (step=0000031) Train Loss mse: 0.4512, Train Loss ce: 1.8539, Train Steps/Sec: 0.26, 
[[34m2025-08-29 07:07:23[0m] (step=0000032) Train Loss mse: 0.3941, Train Loss ce: 0.8983, Train Steps/Sec: 0.24, 
[[34m2025-08-29 07:08:13[0m] Failed to initialize wandb: Attempted to change value of key "checkpoint_dir" from results/checkpoints to /home/haoming/Bagel/results/checkpoints
If you really want to do this, pass allow_val_change=True to config.update(). Continuing without wandb.
[[34m2025-08-29 07:08:13[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='bagel_training', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=False, finetune_from_hf=False, log_every=10, save_every=2000, total_steps=100000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=32768, num_replicate=1, num_shard=1, sharding_strategy='NO_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=False, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=True)
[[34m2025-08-29 07:08:13[0m] Model arguments ModelArguments(model_path='models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='/home/haoming/Bagel/models/BAGEL-7B-MoT/ae.safetensors', vit_path='HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit', max_latent_size=48, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=50, use_vilex='True', num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-08-29 07:08:13[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=8192, max_num_tokens=16384, prefer_buffer_before=8192, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 07:08:13[0m] No finetune
[[34m2025-08-29 07:08:13[0m] set up language model
[[34m2025-08-29 07:08:18[0m] using visual_und
[[34m2025-08-29 07:08:19[0m] using generation
[[34m2025-08-29 07:08:19[0m] model loaded
[[34m2025-08-29 07:08:20[0m] tokenizers loaded
[[34m2025-08-29 07:08:20[0m] frozen done
[[34m2025-08-29 07:08:20[0m] 
============================================================
[[34m2025-08-29 07:08:20[0m] MODEL PARAMETER SUMMARY
[[34m2025-08-29 07:08:20[0m] ============================================================
[[34m2025-08-29 07:08:20[0m] Language Model (LLM):
[[34m2025-08-29 07:08:20[0m]   Total:     988,071,680 parameters
[[34m2025-08-29 07:08:20[0m]   Trainable: 0 parameters
[[34m2025-08-29 07:08:20[0m]   Frozen:    988,071,680 parameters
[[34m2025-08-29 07:08:20[0m] 
Vision Model (ViT):
[[34m2025-08-29 07:08:20[0m]   Total:     402,552,736 parameters
[[34m2025-08-29 07:08:20[0m]   Trainable: 402,552,736 parameters
[[34m2025-08-29 07:08:20[0m]   Frozen:    0 parameters
[[34m2025-08-29 07:08:20[0m] 
Connector/Projector:
[[34m2025-08-29 07:08:20[0m]   Total:     10,332,032 parameters
[[34m2025-08-29 07:08:20[0m]   Trainable: 10,332,032 parameters
[[34m2025-08-29 07:08:20[0m]   Frozen:    0 parameters
[[34m2025-08-29 07:08:20[0m] 
VAE Model:
[[34m2025-08-29 07:08:20[0m]   Total:     83,819,683 parameters
[[34m2025-08-29 07:08:20[0m]   Trainable: 0 parameters
[[34m2025-08-29 07:08:20[0m]   Frozen:    83,819,683 parameters
[[34m2025-08-29 07:08:20[0m] 
ViT Position Embeddings:
[[34m2025-08-29 07:08:20[0m]   Total:     2,240,000 parameters
[[34m2025-08-29 07:08:20[0m]   Trainable: 0 parameters
[[34m2025-08-29 07:08:20[0m]   Frozen:    2,240,000 parameters
[[34m2025-08-29 07:08:20[0m] 
Total Bagel Model:
[[34m2025-08-29 07:08:20[0m]   Total:     1,406,410,464 parameters
[[34m2025-08-29 07:08:20[0m]   Trainable: 414,034,400 parameters
[[34m2025-08-29 07:08:20[0m]   Frozen:    992,376,064 parameters
[[34m2025-08-29 07:08:20[0m] 
Training 29.4% of total parameters
[[34m2025-08-29 07:08:20[0m] ============================================================
[[34m2025-08-29 07:08:20[0m] fsdp config done
[[34m2025-08-29 07:08:23[0m] Training from scratch.
[[34m2025-08-29 07:08:27[0m] checkpoint loading done
[[34m2025-08-29 07:08:27[0m] Dataset config: <data.dataset_base.DataConfig object at 0x7cff5476f430>
[[34m2025-08-29 07:08:27[0m] Expected num tokens per step: 32768
[[34m2025-08-29 07:08:27[0m] Max tokens per sample: 8192
[[34m2025-08-29 07:08:27[0m] Max buffer size: 50
[[34m2025-08-29 07:08:27[0m] Training for 100000 steps, starting at 0...
[[34m2025-08-29 07:08:31[0m] Input packed_text_ids: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_text_indexes: shape=torch.Size([2091]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_position_ids: shape=torch.Size([16255]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input padded_images: shape=torch.Size([9, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-29 07:08:31[0m] Input packed_latent_position_ids: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_vae_token_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_vit_tokens: shape=torch.Size([8980, 588]), dtype=torch.float32
[[34m2025-08-29 07:08:31[0m] Input packed_vit_position_ids: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_vit_token_indexes: shape=torch.Size([8980]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input vit_token_seqlens: shape=torch.Size([15]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_timesteps: shape=torch.Size([5184]), dtype=torch.float32
[[34m2025-08-29 07:08:31[0m] Input mse_loss_indexes: shape=torch.Size([5184]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input packed_label_ids: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-29 07:08:31[0m] Input ce_loss_indexes: shape=torch.Size([1247]), dtype=torch.int64
[[34m2025-08-29 07:08:36[0m] Output mse: shape=torch.Size([5184, 64]), dtype=torch.float32
[[34m2025-08-29 07:08:36[0m] Output ce: shape=torch.Size([1247]), dtype=torch.float32
[[34m2025-08-29 07:08:38[0m] (step=0000000) Train Loss mse: 1.7099, Train Loss ce: 21.0241, Train Steps/Sec: 0.94, 
[[34m2025-08-29 07:08:38[0m] Input packed_text_ids: shape=torch.Size([1463]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_text_indexes: shape=torch.Size([1463]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_position_ids: shape=torch.Size([16269]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input padded_images: shape=torch.Size([14, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-29 07:08:38[0m] Input packed_latent_position_ids: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_vae_token_indexes: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_vit_tokens: shape=torch.Size([6742, 588]), dtype=torch.float32
[[34m2025-08-29 07:08:38[0m] Input packed_vit_position_ids: shape=torch.Size([6742]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_vit_token_indexes: shape=torch.Size([6742]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input vit_token_seqlens: shape=torch.Size([11]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_timesteps: shape=torch.Size([8064]), dtype=torch.float32
[[34m2025-08-29 07:08:38[0m] Input mse_loss_indexes: shape=torch.Size([8064]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input packed_label_ids: shape=torch.Size([610]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Input ce_loss_indexes: shape=torch.Size([610]), dtype=torch.int64
[[34m2025-08-29 07:08:38[0m] Output mse: shape=torch.Size([8064, 64]), dtype=torch.float32
[[34m2025-08-29 07:08:38[0m] Output ce: shape=torch.Size([610]), dtype=torch.float32
[[34m2025-08-29 07:08:39[0m] Input packed_text_ids: shape=torch.Size([1014]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_text_indexes: shape=torch.Size([1014]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_position_ids: shape=torch.Size([16223]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input padded_images: shape=torch.Size([16, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-29 07:08:39[0m] Input packed_latent_position_ids: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_vae_token_indexes: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_vit_tokens: shape=torch.Size([5993, 588]), dtype=torch.float32
[[34m2025-08-29 07:08:39[0m] Input packed_vit_position_ids: shape=torch.Size([5993]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_vit_token_indexes: shape=torch.Size([5993]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input vit_token_seqlens: shape=torch.Size([8]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_timesteps: shape=torch.Size([9216]), dtype=torch.float32
[[34m2025-08-29 07:08:39[0m] Input mse_loss_indexes: shape=torch.Size([9216]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input packed_label_ids: shape=torch.Size([311]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Input ce_loss_indexes: shape=torch.Size([311]), dtype=torch.int64
[[34m2025-08-29 07:08:39[0m] Output mse: shape=torch.Size([9216, 64]), dtype=torch.float32
[[34m2025-08-29 07:08:39[0m] Output ce: shape=torch.Size([311]), dtype=torch.float32
[[34m2025-08-29 07:08:40[0m] Input packed_text_ids: shape=torch.Size([1024]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_text_indexes: shape=torch.Size([1024]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_position_ids: shape=torch.Size([16384]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input padded_images: shape=torch.Size([13, 3, 512, 288]), dtype=torch.float32
[[34m2025-08-29 07:08:40[0m] Input packed_latent_position_ids: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_vae_token_indexes: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_vit_tokens: shape=torch.Size([7872, 588]), dtype=torch.float32
[[34m2025-08-29 07:08:40[0m] Input packed_vit_position_ids: shape=torch.Size([7872]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_vit_token_indexes: shape=torch.Size([7872]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input vit_token_seqlens: shape=torch.Size([10]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_timesteps: shape=torch.Size([7488]), dtype=torch.float32
[[34m2025-08-29 07:08:40[0m] Input mse_loss_indexes: shape=torch.Size([7488]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input packed_label_ids: shape=torch.Size([528]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Input ce_loss_indexes: shape=torch.Size([528]), dtype=torch.int64
[[34m2025-08-29 07:08:40[0m] Output mse: shape=torch.Size([7488, 64]), dtype=torch.float32
[[34m2025-08-29 07:08:40[0m] Output ce: shape=torch.Size([528]), dtype=torch.float32
[[34m2025-08-29 07:08:48[0m] (step=0000010) Train Loss mse: 1.7150, Train Loss ce: 20.7769, Train Steps/Sec: 0.98, 
[[34m2025-08-29 07:08:58[0m] (step=0000020) Train Loss mse: 1.7074, Train Loss ce: 23.0372, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:09:07[0m] (step=0000030) Train Loss mse: 1.6958, Train Loss ce: 22.6543, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:09:17[0m] (step=0000040) Train Loss mse: 1.6935, Train Loss ce: 21.6482, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:09:27[0m] (step=0000050) Train Loss mse: 1.6850, Train Loss ce: 20.3665, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:09:37[0m] (step=0000060) Train Loss mse: 1.6787, Train Loss ce: 19.8495, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:09:47[0m] (step=0000070) Train Loss mse: 1.6601, Train Loss ce: 19.8771, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:09:57[0m] (step=0000080) Train Loss mse: 1.6532, Train Loss ce: 20.3282, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:10:07[0m] (step=0000090) Train Loss mse: 1.6287, Train Loss ce: 21.0254, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:10:17[0m] (step=0000100) Train Loss mse: 1.6142, Train Loss ce: 21.4990, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:10:27[0m] (step=0000110) Train Loss mse: 1.6026, Train Loss ce: 20.3110, Train Steps/Sec: 1.04, 
[[34m2025-08-29 07:10:36[0m] (step=0000120) Train Loss mse: 1.5862, Train Loss ce: 21.4703, Train Steps/Sec: 1.03, 
[[34m2025-08-29 07:10:46[0m] (step=0000130) Train Loss mse: 1.5714, Train Loss ce: 20.8907, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:10:56[0m] (step=0000140) Train Loss mse: 1.5482, Train Loss ce: 19.9380, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:11:06[0m] (step=0000150) Train Loss mse: 1.5270, Train Loss ce: 21.1691, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:11:16[0m] (step=0000160) Train Loss mse: 1.5145, Train Loss ce: 20.8045, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:11:26[0m] (step=0000170) Train Loss mse: 1.4940, Train Loss ce: 19.2673, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:11:36[0m] (step=0000180) Train Loss mse: 1.4709, Train Loss ce: 19.3220, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:11:45[0m] (step=0000190) Train Loss mse: 1.4556, Train Loss ce: 19.2160, Train Steps/Sec: 1.04, 
[[34m2025-08-29 07:11:55[0m] (step=0000200) Train Loss mse: 1.4424, Train Loss ce: 19.6385, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:12:05[0m] (step=0000210) Train Loss mse: 1.4244, Train Loss ce: 17.2825, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:12:15[0m] (step=0000220) Train Loss mse: 1.3953, Train Loss ce: 17.7571, Train Steps/Sec: 1.03, 
[[34m2025-08-29 07:12:25[0m] (step=0000230) Train Loss mse: 1.3981, Train Loss ce: 17.6453, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:12:35[0m] (step=0000240) Train Loss mse: 1.3773, Train Loss ce: 16.4217, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:12:45[0m] (step=0000250) Train Loss mse: 1.3623, Train Loss ce: 20.0087, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:12:54[0m] (step=0000260) Train Loss mse: 1.3529, Train Loss ce: 17.0155, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:13:04[0m] (step=0000270) Train Loss mse: 1.3482, Train Loss ce: 18.8354, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:13:14[0m] (step=0000280) Train Loss mse: 1.3350, Train Loss ce: 16.3147, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:13:24[0m] (step=0000290) Train Loss mse: 1.3299, Train Loss ce: 15.7395, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:13:34[0m] (step=0000300) Train Loss mse: 1.3169, Train Loss ce: 17.1811, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:13:44[0m] (step=0000310) Train Loss mse: 1.3094, Train Loss ce: 16.2075, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:13:54[0m] (step=0000320) Train Loss mse: 1.3033, Train Loss ce: 15.1493, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:14:04[0m] (step=0000330) Train Loss mse: 1.3006, Train Loss ce: 15.0041, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:14:14[0m] (step=0000340) Train Loss mse: 1.2850, Train Loss ce: 14.6100, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:14:23[0m] (step=0000350) Train Loss mse: 1.2808, Train Loss ce: 15.3933, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:14:33[0m] (step=0000360) Train Loss mse: 1.2763, Train Loss ce: 15.0078, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:14:43[0m] (step=0000370) Train Loss mse: 1.2716, Train Loss ce: 14.6369, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:14:53[0m] (step=0000380) Train Loss mse: 1.2625, Train Loss ce: 18.0107, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:15:03[0m] (step=0000390) Train Loss mse: 1.2523, Train Loss ce: 18.9353, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:15:13[0m] (step=0000400) Train Loss mse: 1.2479, Train Loss ce: 14.3526, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:15:23[0m] (step=0000410) Train Loss mse: 1.2381, Train Loss ce: 14.2930, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:15:33[0m] (step=0000420) Train Loss mse: 1.2196, Train Loss ce: 14.2108, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:15:43[0m] (step=0000430) Train Loss mse: 1.2151, Train Loss ce: 16.8528, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:15:52[0m] (step=0000440) Train Loss mse: 1.2103, Train Loss ce: 14.4325, Train Steps/Sec: 1.03, 
[[34m2025-08-29 07:16:02[0m] (step=0000450) Train Loss mse: 1.2035, Train Loss ce: 13.9939, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:16:12[0m] (step=0000460) Train Loss mse: 1.1806, Train Loss ce: 15.3042, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:16:22[0m] (step=0000470) Train Loss mse: 1.1875, Train Loss ce: 16.3492, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:16:32[0m] (step=0000480) Train Loss mse: 1.1817, Train Loss ce: 14.6198, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:16:42[0m] (step=0000490) Train Loss mse: 1.1556, Train Loss ce: 14.9238, Train Steps/Sec: 1.03, 
[[34m2025-08-29 07:16:52[0m] (step=0000500) Train Loss mse: 1.1459, Train Loss ce: 13.1653, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:17:01[0m] (step=0000510) Train Loss mse: 1.1261, Train Loss ce: 13.1148, Train Steps/Sec: 1.03, 
[[34m2025-08-29 07:17:11[0m] (step=0000520) Train Loss mse: 1.1280, Train Loss ce: 12.7488, Train Steps/Sec: 1.00, 
[[34m2025-08-29 07:17:21[0m] (step=0000530) Train Loss mse: 1.1021, Train Loss ce: 14.2943, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:17:31[0m] (step=0000540) Train Loss mse: 1.1083, Train Loss ce: 13.0039, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:17:41[0m] (step=0000550) Train Loss mse: 1.0961, Train Loss ce: 12.8296, Train Steps/Sec: 1.03, 
[[34m2025-08-29 07:17:51[0m] (step=0000560) Train Loss mse: 1.0860, Train Loss ce: 13.1219, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:18:00[0m] (step=0000570) Train Loss mse: 1.0721, Train Loss ce: 12.4944, Train Steps/Sec: 1.04, 
[[34m2025-08-29 07:18:10[0m] (step=0000580) Train Loss mse: 1.0817, Train Loss ce: 12.8941, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:18:20[0m] (step=0000590) Train Loss mse: 1.0408, Train Loss ce: 12.5675, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:18:30[0m] (step=0000600) Train Loss mse: 0.9904, Train Loss ce: 12.4129, Train Steps/Sec: 1.02, 
[[34m2025-08-29 07:18:40[0m] (step=0000610) Train Loss mse: 1.0923, Train Loss ce: 13.7970, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:18:50[0m] (step=0000620) Train Loss mse: 1.0002, Train Loss ce: 12.3019, Train Steps/Sec: 1.01, 
[[34m2025-08-29 07:52:44[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:52:47[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from='/home/haoming/Bagel/models/BAGEL-7B-MoT', resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=5000, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-29 07:52:47[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-29 07:52:47[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=10000, max_num_tokens=10000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 07:55:37[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:55:43[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:55:43[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:55:44[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 07:56:04[0m] Loading checkpoint from /home/haoming/Bagel/models/BAGEL-7B-MoT.
[[34m2025-08-29 08:03:44[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:03:46[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=5000, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-29 08:03:46[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-29 08:03:46[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=10000, max_num_tokens=10000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 08:06:35[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:06:42[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:06:42[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:06:43[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:04[0m] Training from scratch.
[[34m2025-08-29 08:07:04[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:17[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:35[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:35[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:35[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:35[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-29 08:07:37[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:07:37[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 0.41 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:09[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 4.43 GB
Reserved:  18.45 GB
Max Allocated: 9.26 GB
Max Reserved:  18.45 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:14[0m] 
=== MEMORY STEP 0 - AFTER BACKWARD+UPDATE ===
Allocated: 0.43 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:15[0m] (step=0000000) Train Loss mse: 1.7993, Train Loss ce: 12.6766, Train Steps/Sec: 0.03, 
[[34m2025-08-29 08:08:15[0m] 
=== MEMORY STEP 1 - START ===
Allocated: 0.33 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:15[0m] 
=== MEMORY STEP 1 - AFTER DATA LOAD ===
Allocated: 0.42 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:16[0m] 
=== MEMORY STEP 1 - AFTER FORWARD ===
Allocated: 4.34 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:21[0m] 
=== MEMORY STEP 1 - AFTER BACKWARD+UPDATE ===
Allocated: 0.42 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:21[0m] (step=0000001) Train Loss mse: 1.8016, Train Loss ce: 12.5006, Train Steps/Sec: 0.15, 
[[34m2025-08-29 08:08:21[0m] 
=== MEMORY STEP 2 - START ===
Allocated: 0.33 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:21[0m] 
=== MEMORY STEP 2 - AFTER DATA LOAD ===
Allocated: 0.40 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:23[0m] 
=== MEMORY STEP 2 - AFTER FORWARD ===
Allocated: 3.97 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:27[0m] 
=== MEMORY STEP 2 - AFTER BACKWARD+UPDATE ===
Allocated: 0.40 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:28[0m] (step=0000002) Train Loss mse: 1.7965, Train Loss ce: 12.8114, Train Steps/Sec: 0.16, 
[[34m2025-08-29 08:08:28[0m] 
=== MEMORY STEP 3 - START ===
Allocated: 0.33 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:28[0m] 
=== MEMORY STEP 3 - AFTER DATA LOAD ===
Allocated: 0.40 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:29[0m] 
=== MEMORY STEP 3 - AFTER FORWARD ===
Allocated: 4.31 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:34[0m] 
=== MEMORY STEP 3 - AFTER BACKWARD+UPDATE ===
Allocated: 0.40 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:08:34[0m] (step=0000003) Train Loss mse: 1.7987, Train Loss ce: 12.5053, Train Steps/Sec: 0.16, 
[[34m2025-08-29 08:08:41[0m] (step=0000004) Train Loss mse: 1.7984, Train Loss ce: 12.6491, Train Steps/Sec: 0.15, 
[[34m2025-08-29 08:08:47[0m] (step=0000005) Train Loss mse: 1.7980, Train Loss ce: 12.8392, Train Steps/Sec: 0.16, 
[[34m2025-08-29 08:27:19[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:27:21[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-29 08:27:21[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-29 08:27:21[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 08:30:09[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:30:16[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:30:16[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:30:17[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:30:35[0m] Training from scratch.
[[34m2025-08-29 08:30:35[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:30:48[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:31:07[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:31:08[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:31:08[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:31:08[0m] Training for 500000 steps, starting at 0...
[[34m2025-08-29 08:31:09[0m] 
=== MEMORY STEP 0 - START ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:31:09[0m] 
=== MEMORY STEP 0 - AFTER DATA LOAD ===
Allocated: 0.33 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:31:59[0m] 
=== MEMORY STEP 0 - AFTER FORWARD ===
Allocated: 3.01 GB
Reserved:  18.44 GB
Max Allocated: 9.26 GB
Max Reserved:  18.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:02[0m] 
=== MEMORY STEP 0 - AFTER BACKWARD+UPDATE ===
Allocated: 0.35 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:03[0m] (step=0000000) Train Loss mse: 1.7088, Train Loss ce: 12.7286, Train Steps/Sec: 0.02, 
[[34m2025-08-29 08:32:03[0m] 
=== MEMORY STEP 1 - START ===
Allocated: 0.33 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:03[0m] 
=== MEMORY STEP 1 - AFTER DATA LOAD ===
Allocated: 0.34 GB
Reserved:  23.00 GB
Max Allocated: 9.26 GB
Max Reserved:  23.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:05[0m] 
=== MEMORY STEP 1 - AFTER FORWARD ===
Allocated: 2.94 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:08[0m] 
=== MEMORY STEP 1 - AFTER BACKWARD+UPDATE ===
Allocated: 0.34 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:09[0m] (step=0000001) Train Loss mse: 1.7109, Train Loss ce: 12.6054, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:32:09[0m] 
=== MEMORY STEP 2 - START ===
Allocated: 0.33 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:09[0m] 
=== MEMORY STEP 2 - AFTER DATA LOAD ===
Allocated: 0.34 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:11[0m] 
=== MEMORY STEP 2 - AFTER FORWARD ===
Allocated: 2.91 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:13[0m] 
=== MEMORY STEP 2 - AFTER BACKWARD+UPDATE ===
Allocated: 0.34 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:14[0m] (step=0000002) Train Loss mse: 1.7027, Train Loss ce: 12.7304, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:32:14[0m] 
=== MEMORY STEP 3 - START ===
Allocated: 0.33 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:14[0m] 
=== MEMORY STEP 3 - AFTER DATA LOAD ===
Allocated: 0.34 GB
Reserved:  23.44 GB
Max Allocated: 9.26 GB
Max Reserved:  23.44 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:16[0m] 
=== MEMORY STEP 3 - AFTER FORWARD ===
Allocated: 2.94 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:19[0m] 
=== MEMORY STEP 3 - AFTER BACKWARD+UPDATE ===
Allocated: 0.34 GB
Reserved:  23.88 GB
Max Allocated: 9.26 GB
Max Reserved:  23.88 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 08:32:19[0m] (step=0000003) Train Loss mse: 1.7048, Train Loss ce: 12.5915, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:32:25[0m] (step=0000004) Train Loss mse: 1.7076, Train Loss ce: 12.7273, Train Steps/Sec: 0.18, 
[[34m2025-08-29 08:32:30[0m] (step=0000005) Train Loss mse: 1.7092, Train Loss ce: 12.5988, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:32:35[0m] (step=0000006) Train Loss mse: 1.7054, Train Loss ce: 12.7230, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:32:40[0m] (step=0000007) Train Loss mse: 1.7149, Train Loss ce: 12.6161, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:32:45[0m] (step=0000008) Train Loss mse: 1.7093, Train Loss ce: 12.7192, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:32:50[0m] (step=0000009) Train Loss mse: 1.7058, Train Loss ce: 12.5754, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:32:55[0m] (step=0000010) Train Loss mse: 1.7082, Train Loss ce: 12.7187, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:32:56[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000010.
[[34m2025-08-29 08:41:02[0m] (step=0000011) Train Loss mse: 1.7070, Train Loss ce: 12.5999, Train Steps/Sec: 0.00, 
[[34m2025-08-29 08:41:08[0m] (step=0000012) Train Loss mse: 1.7103, Train Loss ce: 12.7113, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:41:13[0m] (step=0000013) Train Loss mse: 1.7035, Train Loss ce: 12.5545, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:41:18[0m] (step=0000014) Train Loss mse: 1.7125, Train Loss ce: 12.7005, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:41:23[0m] (step=0000015) Train Loss mse: 1.7079, Train Loss ce: 12.5766, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:41:28[0m] (step=0000016) Train Loss mse: 1.7128, Train Loss ce: 12.6955, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:41:33[0m] (step=0000017) Train Loss mse: 1.7084, Train Loss ce: 12.5379, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:41:38[0m] (step=0000018) Train Loss mse: 1.6978, Train Loss ce: 12.6885, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:41:44[0m] (step=0000019) Train Loss mse: 1.7150, Train Loss ce: 12.5449, Train Steps/Sec: 0.18, 
[[34m2025-08-29 08:41:49[0m] (step=0000020) Train Loss mse: 1.7064, Train Loss ce: 12.6776, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:41:49[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000020.
[[34m2025-08-29 08:49:20[0m] (step=0000021) Train Loss mse: 1.7055, Train Loss ce: 12.5024, Train Steps/Sec: 0.00, 
[[34m2025-08-29 08:49:25[0m] (step=0000022) Train Loss mse: 1.7072, Train Loss ce: 12.6721, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:49:30[0m] (step=0000023) Train Loss mse: 1.7042, Train Loss ce: 12.5075, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:49:35[0m] (step=0000024) Train Loss mse: 1.7090, Train Loss ce: 12.6509, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:49:41[0m] (step=0000025) Train Loss mse: 1.7056, Train Loss ce: 12.4603, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:49:46[0m] (step=0000026) Train Loss mse: 1.7014, Train Loss ce: 12.6426, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:49:51[0m] (step=0000027) Train Loss mse: 1.6998, Train Loss ce: 12.4579, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:49:56[0m] (step=0000028) Train Loss mse: 1.7024, Train Loss ce: 12.6316, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:50:01[0m] (step=0000029) Train Loss mse: 1.7031, Train Loss ce: 12.4092, Train Steps/Sec: 0.19, 
[[34m2025-08-29 08:50:06[0m] (step=0000030) Train Loss mse: 1.6932, Train Loss ce: 12.6137, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:50:06[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000030.
[[34m2025-08-29 08:58:21[0m] (step=0000031) Train Loss mse: 1.7008, Train Loss ce: 12.3664, Train Steps/Sec: 0.00, 
[[34m2025-08-29 08:58:26[0m] (step=0000032) Train Loss mse: 1.6992, Train Loss ce: 12.5994, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:58:31[0m] (step=0000033) Train Loss mse: 1.6970, Train Loss ce: 12.3532, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:58:36[0m] (step=0000034) Train Loss mse: 1.6984, Train Loss ce: 12.5816, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:58:41[0m] (step=0000035) Train Loss mse: 1.6979, Train Loss ce: 12.3221, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:58:46[0m] (step=0000036) Train Loss mse: 1.7006, Train Loss ce: 12.5626, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:58:51[0m] (step=0000037) Train Loss mse: 1.6965, Train Loss ce: 12.2773, Train Steps/Sec: 0.21, 
[[34m2025-08-29 08:58:56[0m] (step=0000038) Train Loss mse: 1.7013, Train Loss ce: 12.5488, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:59:01[0m] (step=0000039) Train Loss mse: 1.7117, Train Loss ce: 12.2545, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:59:06[0m] (step=0000040) Train Loss mse: 1.6886, Train Loss ce: 12.5280, Train Steps/Sec: 0.20, 
[[34m2025-08-29 08:59:06[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000040.
[[34m2025-08-29 09:07:50[0m] (step=0000041) Train Loss mse: 1.7008, Train Loss ce: 12.2405, Train Steps/Sec: 0.00, 
[[34m2025-08-29 09:07:55[0m] (step=0000042) Train Loss mse: 1.6977, Train Loss ce: 12.5045, Train Steps/Sec: 0.20, 
[[34m2025-08-29 09:08:00[0m] (step=0000043) Train Loss mse: 1.6936, Train Loss ce: 12.1825, Train Steps/Sec: 0.19, 
[[34m2025-08-29 09:08:05[0m] (step=0000044) Train Loss mse: 1.6931, Train Loss ce: 12.4844, Train Steps/Sec: 0.20, 
[[34m2025-08-29 17:56:40[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 17:56:45[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-08-29 17:56:45[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-08-29 17:56:45[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-08-29 17:59:32[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 17:59:38[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 17:59:39[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 17:59:39[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 17:59:57[0m] Loading checkpoint from /home/haoming/Bagel/results/checkpoints/0000040.
[[34m2025-08-29 18:01:44[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-29 18:04:26[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-08-29 18:04:26[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 18:04:46[0m] 
=== MEMORY AFTER EMA FSDP SETUP ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 18:05:11[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 18:05:11[0m] 
=== MEMORY AFTER OPTIMIZER CREATION ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 18:05:12[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-08-29 18:05:12[0m] Training for 500000 steps, starting at 41...
[[34m2025-08-29 18:06:10[0m] (step=0000041) Train Loss mse: 1.6945, Train Loss ce: 12.5155, Train Steps/Sec: 0.02, 
[[34m2025-08-29 18:06:17[0m] (step=0000042) Train Loss mse: 1.6969, Train Loss ce: 12.2073, Train Steps/Sec: 0.16, 
[[34m2025-08-29 18:06:23[0m] (step=0000043) Train Loss mse: 1.6882, Train Loss ce: 12.5014, Train Steps/Sec: 0.16, 
[[34m2025-08-29 18:06:28[0m] (step=0000044) Train Loss mse: 1.6899, Train Loss ce: 12.1586, Train Steps/Sec: 0.17, 
[[34m2025-08-29 18:06:34[0m] (step=0000045) Train Loss mse: 1.6920, Train Loss ce: 12.4704, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:06:40[0m] (step=0000046) Train Loss mse: 1.6921, Train Loss ce: 12.1314, Train Steps/Sec: 0.17, 
[[34m2025-08-29 18:06:46[0m] (step=0000047) Train Loss mse: 1.6889, Train Loss ce: 12.4466, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:06:51[0m] (step=0000048) Train Loss mse: 1.6966, Train Loss ce: 12.1020, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:06:57[0m] (step=0000049) Train Loss mse: 1.6898, Train Loss ce: 12.4269, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:07:03[0m] (step=0000050) Train Loss mse: 1.6852, Train Loss ce: 12.0374, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:07:03[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000050.
[[34m2025-08-29 18:14:49[0m] (step=0000051) Train Loss mse: 1.6858, Train Loss ce: 12.4003, Train Steps/Sec: 0.00, 
[[34m2025-08-29 18:14:55[0m] (step=0000052) Train Loss mse: 1.6830, Train Loss ce: 12.0308, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:15:00[0m] (step=0000053) Train Loss mse: 1.6864, Train Loss ce: 12.3826, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:15:06[0m] (step=0000054) Train Loss mse: 1.6787, Train Loss ce: 11.9600, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:15:12[0m] (step=0000055) Train Loss mse: 1.6865, Train Loss ce: 12.3517, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:15:17[0m] (step=0000056) Train Loss mse: 1.6801, Train Loss ce: 11.9295, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:15:22[0m] (step=0000057) Train Loss mse: 1.6854, Train Loss ce: 12.3341, Train Steps/Sec: 0.20, 
[[34m2025-08-29 18:15:28[0m] (step=0000058) Train Loss mse: 1.6806, Train Loss ce: 11.8864, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:15:33[0m] (step=0000059) Train Loss mse: 1.6691, Train Loss ce: 12.3135, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:15:39[0m] (step=0000060) Train Loss mse: 1.6857, Train Loss ce: 11.8597, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:15:39[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000060.
[[34m2025-08-29 18:22:11[0m] (step=0000061) Train Loss mse: 1.6760, Train Loss ce: 12.2736, Train Steps/Sec: 0.00, 
[[34m2025-08-29 18:22:17[0m] (step=0000062) Train Loss mse: 1.6752, Train Loss ce: 11.8161, Train Steps/Sec: 0.16, 
[[34m2025-08-29 18:22:22[0m] (step=0000063) Train Loss mse: 1.6774, Train Loss ce: 12.2537, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:22:28[0m] (step=0000064) Train Loss mse: 1.6725, Train Loss ce: 11.7687, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:22:34[0m] (step=0000065) Train Loss mse: 1.6776, Train Loss ce: 12.2188, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:22:39[0m] (step=0000066) Train Loss mse: 1.6740, Train Loss ce: 11.7093, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:22:45[0m] (step=0000067) Train Loss mse: 1.6664, Train Loss ce: 12.1955, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:22:50[0m] (step=0000068) Train Loss mse: 1.6648, Train Loss ce: 11.6794, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:22:56[0m] (step=0000069) Train Loss mse: 1.6669, Train Loss ce: 12.1730, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:23:01[0m] (step=0000070) Train Loss mse: 1.6679, Train Loss ce: 11.6203, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:23:01[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000070.
[[34m2025-08-29 18:27:43[0m] (step=0000071) Train Loss mse: 1.6621, Train Loss ce: 12.1346, Train Steps/Sec: 0.00, 
[[34m2025-08-29 18:27:49[0m] (step=0000072) Train Loss mse: 1.6657, Train Loss ce: 11.5454, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:27:54[0m] (step=0000073) Train Loss mse: 1.6628, Train Loss ce: 12.1119, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:27:59[0m] (step=0000074) Train Loss mse: 1.6589, Train Loss ce: 11.5479, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:28:05[0m] (step=0000075) Train Loss mse: 1.6614, Train Loss ce: 12.0804, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:28:11[0m] (step=0000076) Train Loss mse: 1.6564, Train Loss ce: 11.5012, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:28:16[0m] (step=0000077) Train Loss mse: 1.6620, Train Loss ce: 12.0570, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:28:22[0m] (step=0000078) Train Loss mse: 1.6573, Train Loss ce: 11.4105, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:28:27[0m] (step=0000079) Train Loss mse: 1.6621, Train Loss ce: 12.0328, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:28:33[0m] (step=0000080) Train Loss mse: 1.6704, Train Loss ce: 11.3952, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:28:33[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000080.
[[34m2025-08-29 18:38:57[0m] (step=0000081) Train Loss mse: 1.6501, Train Loss ce: 12.0023, Train Steps/Sec: 0.00, 
[[34m2025-08-29 18:39:02[0m] (step=0000082) Train Loss mse: 1.6576, Train Loss ce: 11.3580, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:39:07[0m] (step=0000083) Train Loss mse: 1.6566, Train Loss ce: 11.9686, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:39:13[0m] (step=0000084) Train Loss mse: 1.6474, Train Loss ce: 11.2982, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:39:19[0m] (step=0000085) Train Loss mse: 1.6449, Train Loss ce: 11.9384, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:39:24[0m] (step=0000086) Train Loss mse: 1.6541, Train Loss ce: 11.2423, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:39:30[0m] (step=0000087) Train Loss mse: 1.6455, Train Loss ce: 11.9065, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:39:36[0m] (step=0000088) Train Loss mse: 1.6486, Train Loss ce: 11.2153, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:39:41[0m] (step=0000089) Train Loss mse: 1.6513, Train Loss ce: 11.8911, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:39:46[0m] (step=0000090) Train Loss mse: 1.6380, Train Loss ce: 11.1859, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:39:46[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000090.
[[34m2025-08-29 18:49:50[0m] (step=0000091) Train Loss mse: 1.6353, Train Loss ce: 11.8668, Train Steps/Sec: 0.00, 
[[34m2025-08-29 18:49:56[0m] (step=0000092) Train Loss mse: 1.6359, Train Loss ce: 11.1260, Train Steps/Sec: 0.17, 
[[34m2025-08-29 18:50:01[0m] (step=0000093) Train Loss mse: 1.6321, Train Loss ce: 11.8233, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:50:07[0m] (step=0000094) Train Loss mse: 1.6275, Train Loss ce: 11.0513, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:50:12[0m] (step=0000095) Train Loss mse: 1.6430, Train Loss ce: 11.7945, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:50:18[0m] (step=0000096) Train Loss mse: 1.6358, Train Loss ce: 11.0164, Train Steps/Sec: 0.17, 
[[34m2025-08-29 18:50:24[0m] (step=0000097) Train Loss mse: 1.6414, Train Loss ce: 11.7628, Train Steps/Sec: 0.17, 
[[34m2025-08-29 18:50:29[0m] (step=0000098) Train Loss mse: 1.6335, Train Loss ce: 10.9919, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:50:34[0m] (step=0000099) Train Loss mse: 1.6241, Train Loss ce: 11.7324, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:50:40[0m] (step=0000100) Train Loss mse: 1.6143, Train Loss ce: 10.9229, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:50:40[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000100.
[[34m2025-08-29 18:59:31[0m] (step=0000101) Train Loss mse: 1.6197, Train Loss ce: 11.7114, Train Steps/Sec: 0.00, 
[[34m2025-08-29 18:59:36[0m] (step=0000102) Train Loss mse: 1.6284, Train Loss ce: 10.8955, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:59:42[0m] (step=0000103) Train Loss mse: 1.6128, Train Loss ce: 11.6718, Train Steps/Sec: 0.17, 
[[34m2025-08-29 18:59:47[0m] (step=0000104) Train Loss mse: 1.6224, Train Loss ce: 10.8062, Train Steps/Sec: 0.19, 
[[34m2025-08-29 18:59:53[0m] (step=0000105) Train Loss mse: 1.6087, Train Loss ce: 11.6429, Train Steps/Sec: 0.18, 
[[34m2025-08-29 18:59:58[0m] (step=0000106) Train Loss mse: 1.6135, Train Loss ce: 10.7310, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:00:04[0m] (step=0000107) Train Loss mse: 1.6127, Train Loss ce: 11.6200, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:00:10[0m] (step=0000108) Train Loss mse: 1.6095, Train Loss ce: 10.6792, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:00:15[0m] (step=0000109) Train Loss mse: 1.6056, Train Loss ce: 11.5798, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:00:20[0m] (step=0000110) Train Loss mse: 1.6004, Train Loss ce: 10.6343, Train Steps/Sec: 0.20, 
[[34m2025-08-29 19:00:20[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000110.
[[34m2025-08-29 19:06:08[0m] (step=0000111) Train Loss mse: 1.6105, Train Loss ce: 11.5570, Train Steps/Sec: 0.00, 
[[34m2025-08-29 19:06:13[0m] (step=0000112) Train Loss mse: 1.6096, Train Loss ce: 10.5909, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:06:19[0m] (step=0000113) Train Loss mse: 1.6020, Train Loss ce: 11.5200, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:25[0m] (step=0000114) Train Loss mse: 1.5955, Train Loss ce: 10.5437, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:30[0m] (step=0000115) Train Loss mse: 1.5956, Train Loss ce: 11.5155, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:06:36[0m] (step=0000116) Train Loss mse: 1.5916, Train Loss ce: 10.5532, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:41[0m] (step=0000117) Train Loss mse: 1.5867, Train Loss ce: 11.4853, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:47[0m] (step=0000118) Train Loss mse: 1.5932, Train Loss ce: 10.4482, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:53[0m] (step=0000119) Train Loss mse: 1.5822, Train Loss ce: 11.4590, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:58[0m] (step=0000120) Train Loss mse: 1.5829, Train Loss ce: 10.4005, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:06:58[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000120.
[[34m2025-08-29 19:14:39[0m] (step=0000121) Train Loss mse: 1.5816, Train Loss ce: 11.4090, Train Steps/Sec: 0.00, 
[[34m2025-08-29 19:14:44[0m] (step=0000122) Train Loss mse: 1.5802, Train Loss ce: 10.3542, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:14:50[0m] (step=0000123) Train Loss mse: 1.5809, Train Loss ce: 11.3818, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:14:55[0m] (step=0000124) Train Loss mse: 1.5695, Train Loss ce: 10.3564, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:15:01[0m] (step=0000125) Train Loss mse: 1.5715, Train Loss ce: 11.3522, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:15:06[0m] (step=0000126) Train Loss mse: 1.5726, Train Loss ce: 10.3715, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:15:12[0m] (step=0000127) Train Loss mse: 1.5823, Train Loss ce: 11.3339, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:15:18[0m] (step=0000128) Train Loss mse: 1.5786, Train Loss ce: 10.3042, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:15:23[0m] (step=0000129) Train Loss mse: 1.5592, Train Loss ce: 11.2844, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:15:29[0m] (step=0000130) Train Loss mse: 1.5664, Train Loss ce: 10.1778, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:15:29[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000130.
[[34m2025-08-29 19:24:58[0m] (step=0000131) Train Loss mse: 1.5766, Train Loss ce: 11.2589, Train Steps/Sec: 0.00, 
[[34m2025-08-29 19:25:03[0m] (step=0000132) Train Loss mse: 1.5609, Train Loss ce: 10.1306, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:25:09[0m] (step=0000133) Train Loss mse: 1.5614, Train Loss ce: 11.2562, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:25:15[0m] (step=0000134) Train Loss mse: 1.5709, Train Loss ce: 10.2026, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:25:20[0m] (step=0000135) Train Loss mse: 1.5543, Train Loss ce: 11.2017, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:25:26[0m] (step=0000136) Train Loss mse: 1.5625, Train Loss ce: 10.1085, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:25:32[0m] (step=0000137) Train Loss mse: 1.5494, Train Loss ce: 11.1833, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:25:37[0m] (step=0000138) Train Loss mse: 1.5535, Train Loss ce: 10.0645, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:25:43[0m] (step=0000139) Train Loss mse: 1.5494, Train Loss ce: 11.1568, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:25:48[0m] (step=0000140) Train Loss mse: 1.5505, Train Loss ce: 10.0248, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:25:48[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000140.
[[34m2025-08-29 19:35:05[0m] (step=0000141) Train Loss mse: 1.5454, Train Loss ce: 11.1426, Train Steps/Sec: 0.00, 
[[34m2025-08-29 19:35:11[0m] (step=0000142) Train Loss mse: 1.5384, Train Loss ce: 10.0324, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:35:16[0m] (step=0000143) Train Loss mse: 1.5336, Train Loss ce: 11.1023, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:35:22[0m] (step=0000144) Train Loss mse: 1.5440, Train Loss ce: 9.9411, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:35:27[0m] (step=0000145) Train Loss mse: 1.5469, Train Loss ce: 11.0697, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:35:33[0m] (step=0000146) Train Loss mse: 1.5374, Train Loss ce: 9.8752, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:35:38[0m] (step=0000147) Train Loss mse: 1.5421, Train Loss ce: 11.0433, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:35:44[0m] (step=0000148) Train Loss mse: 1.5374, Train Loss ce: 9.8406, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:35:49[0m] (step=0000149) Train Loss mse: 1.5433, Train Loss ce: 11.0457, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:35:55[0m] (step=0000150) Train Loss mse: 1.5307, Train Loss ce: 9.7615, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:35:55[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000150.
[[34m2025-08-29 19:44:58[0m] (step=0000151) Train Loss mse: 1.5269, Train Loss ce: 11.0000, Train Steps/Sec: 0.00, 
[[34m2025-08-29 19:45:04[0m] (step=0000152) Train Loss mse: 1.5235, Train Loss ce: 9.7735, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:45:09[0m] (step=0000153) Train Loss mse: 1.5291, Train Loss ce: 10.9655, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:45:14[0m] (step=0000154) Train Loss mse: 1.5191, Train Loss ce: 9.7318, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:45:20[0m] (step=0000155) Train Loss mse: 1.5150, Train Loss ce: 10.9390, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:45:25[0m] (step=0000156) Train Loss mse: 1.5271, Train Loss ce: 9.6755, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:45:31[0m] (step=0000157) Train Loss mse: 1.5089, Train Loss ce: 10.9225, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:45:37[0m] (step=0000158) Train Loss mse: 1.5146, Train Loss ce: 9.6164, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:45:42[0m] (step=0000159) Train Loss mse: 1.5133, Train Loss ce: 10.8963, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:45:47[0m] (step=0000160) Train Loss mse: 1.5092, Train Loss ce: 9.5995, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:45:47[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000160.
[[34m2025-08-29 19:53:01[0m] (step=0000161) Train Loss mse: 1.5115, Train Loss ce: 10.8833, Train Steps/Sec: 0.00, 
[[34m2025-08-29 19:53:06[0m] (step=0000162) Train Loss mse: 1.5064, Train Loss ce: 9.4902, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:53:12[0m] (step=0000163) Train Loss mse: 1.5021, Train Loss ce: 10.8641, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:53:17[0m] (step=0000164) Train Loss mse: 1.5134, Train Loss ce: 9.5286, Train Steps/Sec: 0.19, 
[[34m2025-08-29 19:53:23[0m] (step=0000165) Train Loss mse: 1.4860, Train Loss ce: 10.8102, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:53:29[0m] (step=0000166) Train Loss mse: 1.5018, Train Loss ce: 9.4742, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:53:34[0m] (step=0000167) Train Loss mse: 1.4942, Train Loss ce: 10.7855, Train Steps/Sec: 0.17, 
[[34m2025-08-29 19:53:40[0m] (step=0000168) Train Loss mse: 1.4933, Train Loss ce: 9.4980, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:53:46[0m] (step=0000169) Train Loss mse: 1.4973, Train Loss ce: 10.7612, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:53:51[0m] (step=0000170) Train Loss mse: 1.4869, Train Loss ce: 9.4084, Train Steps/Sec: 0.18, 
[[34m2025-08-29 19:53:51[0m] Saving checkpoint to /home/haoming/Bagel/results/checkpoints/0000170.
[[34m2025-09-02 18:36:00[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:36:03[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=100, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=True, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-09-02 18:36:03[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-09-02 18:36:03[0m] Data arguments DataArguments(dataset_config_file='/home/haoming/Bagel/data/configs/datacomp.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-09-02 18:38:59[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:39:06[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:39:06[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:39:07[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:39:28[0m] Loading checkpoint from /home/haoming/Bagel/results/checkpoints/0000160.
[[34m2025-09-02 18:40:09[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-02 18:40:27[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-02 18:40:29[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:41:15[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:41:17[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 18:41:17[0m] Training for 500000 steps, starting at 161...
[[34m2025-09-02 19:19:26[0m] 
=== MEMORY AFTER SETUP ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:19:29[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='/home/haoming/Bagel/results/checkpoints', use_wandb=False, wandb_project='bagel', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=10, save_every=100, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=True, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-09-02 19:19:29[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='Qwen/Qwen2.5-0.5B-Instruct', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='google/siglip-so400m-patch14-384', max_latent_size=64, latent_patch_size=2, vit_patch_size=16, vit_max_num_patch_per_side=32, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3, num_layer=4, num_heads=8, num_output_tokens=1, tail_drop_prob=0.0, tail_drop_max=0)
[[34m2025-09-02 19:19:29[0m] Data arguments DataArguments(dataset_config_file='/home/haoming/Bagel/data/configs/datacomp.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-09-02 19:22:28[0m] 
=== MEMORY AFTER LANGUAGE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:22:35[0m] 
=== MEMORY AFTER VISION MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:22:35[0m] 
=== MEMORY AFTER VAE MODEL ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:22:36[0m] 
=== MEMORY AFTER BAGEL MODEL CREATION ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:23:12[0m] Loading checkpoint from /home/haoming/Bagel/results/checkpoints/0000160.
[[34m2025-09-02 19:23:36[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-02 19:23:54[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-02 19:23:55[0m] 
=== MEMORY AFTER CHECKPOINT LOAD ===
Allocated: 0.00 GB
Reserved:  0.00 GB
Max Allocated: 0.00 GB
Max Reserved:  0.00 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:25:01[0m] 
=== MEMORY AFTER FSDP WRAPPER ===
Allocated: 0.00 GB
Reserved:  12.11 GB
Max Allocated: 9.26 GB
Max Reserved:  12.11 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:25:02[0m] 
=== MEMORY BEFORE TRAINING LOOP ===
Allocated: 0.32 GB
Reserved:  12.12 GB
Max Allocated: 9.26 GB
Max Reserved:  12.12 GB
Available GPUs: 8
Current Device: 0
========================================
[[34m2025-09-02 19:25:02[0m] Training for 500000 steps, starting at 161...
[[34m2025-09-15 03:07:19[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', wandb_project='bagel_original', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=False, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-09-15 03:07:19[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-09-15 03:07:19[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-09-15 03:10:32[0m] Loading checkpoint from results/checkpoints/0000130.
[[34m2025-09-15 03:10:34[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.fc1.weight', 'connector.fc1.bias', 'connector.fc2.weight', 'connector.fc2.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.attn.in_proj_bias', 'connector.attn.in_proj_weight', 'connector.attn.out_proj.bias', 'connector.attn.out_proj.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.2.bias', 'connector.layer_projectors.2.weight', 'connector.proj.bias', 'connector.proj.weight', 'connector.query'])
[[34m2025-09-15 03:10:36[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.fc1.weight', 'connector.fc1.bias', 'connector.fc2.weight', 'connector.fc2.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.attn.in_proj_bias', 'connector.attn.in_proj_weight', 'connector.attn.out_proj.bias', 'connector.attn.out_proj.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.2.bias', 'connector.layer_projectors.2.weight', 'connector.proj.bias', 'connector.proj.weight', 'connector.query'])
[[34m2025-09-15 03:19:12[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', wandb_project='bagel_original', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=True, resume_from=None, resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-09-15 03:19:12[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-09-15 03:19:12[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-09-15 03:22:32[0m] Training from scratch.
[[34m2025-09-15 03:23:08[0m] Training for 500000 steps, starting at 0...
[[34m2025-09-15 03:23:37[0m] (step=0000000) Train Loss mse: 1.7067, Train Loss ce: 12.5309, Train Steps/Sec: 0.04, 
[[34m2025-09-15 03:23:41[0m] (step=0000001) Train Loss mse: 1.7126, Train Loss ce: 12.6242, Train Steps/Sec: 0.25, 
[[34m2025-09-15 03:23:45[0m] (step=0000002) Train Loss mse: 1.7012, Train Loss ce: 12.5370, Train Steps/Sec: 0.25, 
[[34m2025-09-15 03:23:49[0m] (step=0000003) Train Loss mse: 1.7093, Train Loss ce: 12.5934, Train Steps/Sec: 0.26, 
[[34m2025-09-15 03:23:53[0m] (step=0000004) Train Loss mse: 1.7108, Train Loss ce: 12.5191, Train Steps/Sec: 0.23, 
[[34m2025-09-15 03:23:57[0m] (step=0000005) Train Loss mse: 1.7103, Train Loss ce: 12.6076, Train Steps/Sec: 0.26, 
[[34m2025-09-15 03:39:22[0m] Training arguments TrainingArguments(visual_gen=True, visual_und=True, results_dir='results', checkpoint_dir='results/checkpoints', wandb_project='bagel_original', wandb_name='run', wandb_runid='0', wandb_resume='allow', wandb_offline=False, global_seed=4396, auto_resume=False, resume_from='models/BAGEL-7B-MoT', resume_model_only=True, finetune_from_ema=True, finetune_from_hf=True, log_every=1, save_every=10, total_steps=500000, warmup_steps=2000, lr_scheduler='constant', lr=2e-05, min_lr=1e-07, beta1=0.9, beta2=0.95, eps=1e-15, ema=0.0, max_grad_norm=1.0, timestep_shift=1.0, mse_weight=1.0, ce_weight=1.0, ce_loss_reweighting=False, expected_num_tokens=2048, num_replicate=1, num_shard=4, sharding_strategy='FULL_SHARD', backward_prefetch='BACKWARD_PRE', cpu_offload=True, freeze_llm=True, freeze_vit=False, freeze_vae=True, freeze_und=False, copy_init_moe=True, use_flex=False)
[[34m2025-09-15 03:39:22[0m] Model arguments ModelArguments(model_path='/home/haoming/Bagel/models/BAGEL-7B-MoT', llm_path='hf/Qwen2.5-0.5B-Instruct/', llm_qk_norm=True, tie_word_embeddings=False, layer_module='Qwen2MoTDecoderLayer', vae_path='flux/vae/ae.safetensors', vit_path='hf/siglip-so400m-14-980-flash-attn2-navit/', max_latent_size=64, latent_patch_size=2, vit_patch_size=14, vit_max_num_patch_per_side=70, connector_act='gelu_pytorch_tanh', interpolate_pos=False, vit_select_layer=-2, vit_rope=False, text_cond_dropout_prob=0.1, vae_cond_dropout_prob=0.3, vit_cond_dropout_prob=0.3)
[[34m2025-09-15 03:39:22[0m] Data arguments DataArguments(dataset_config_file='./data/configs/example.yaml', prefetch_factor=2, num_workers=2, max_num_tokens_per_sample=2048, max_num_tokens=3000, prefer_buffer_before=16384, max_buffer_size=50, data_seed=42)
[[34m2025-09-15 03:42:39[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-09-15 03:42:42[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-15 03:42:45[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-15 03:43:21[0m] Training for 500000 steps, starting at 0...
[[34m2025-09-15 03:43:38[0m] (step=0000000) Train Loss mse: 0.3784, Train Loss ce: 1.5732, Train Steps/Sec: 0.06, 
[[34m2025-09-15 03:43:41[0m] (step=0000001) Train Loss mse: 0.3883, Train Loss ce: 0.3909, Train Steps/Sec: 0.38, 
[[34m2025-09-15 03:43:43[0m] (step=0000002) Train Loss mse: 0.4095, Train Loss ce: 1.5994, Train Steps/Sec: 0.39, 
[[34m2025-09-15 03:43:46[0m] (step=0000003) Train Loss mse: 0.4111, Train Loss ce: 0.2845, Train Steps/Sec: 0.38, 
[[34m2025-09-15 03:43:49[0m] (step=0000004) Train Loss mse: 0.3850, Train Loss ce: 1.5740, Train Steps/Sec: 0.34, 
[[34m2025-09-15 03:43:52[0m] (step=0000005) Train Loss mse: 0.3545, Train Loss ce: 0.2568, Train Steps/Sec: 0.38, 
[[34m2025-09-15 03:43:54[0m] (step=0000006) Train Loss mse: 0.5007, Train Loss ce: 1.5696, Train Steps/Sec: 0.40, 
[[34m2025-09-15 03:43:57[0m] (step=0000007) Train Loss mse: 0.3925, Train Loss ce: 0.3138, Train Steps/Sec: 0.41, 
[[34m2025-09-15 03:43:59[0m] (step=0000008) Train Loss mse: 0.4348, Train Loss ce: 1.5717, Train Steps/Sec: 0.38, 
[[34m2025-09-15 03:44:02[0m] (step=0000009) Train Loss mse: 0.3754, Train Loss ce: 0.3672, Train Steps/Sec: 0.40, 
[[34m2025-09-15 03:44:04[0m] (step=0000010) Train Loss mse: 0.4059, Train Loss ce: 1.5730, Train Steps/Sec: 0.38, 
[[34m2025-09-15 03:44:05[0m] Saving checkpoint to results/checkpoints/0000010.
[[34m2025-09-15 07:00:14[0m] Loading checkpoint from /home/haoming/Bagel/experiments/results_tune/checkpoints/0000050.
[[34m2025-09-15 07:00:15[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-15 07:00:15[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'vit_pos_embed.pos_embed'], unexpected_keys=[])
[[34m2025-09-15 07:08:32[0m] Loading checkpoint from models/BAGEL-7B-MoT.
[[34m2025-09-15 07:08:33[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.query', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.2.weight', 'connector.layer_projectors.2.bias', 'connector.attn.in_proj_weight', 'connector.attn.in_proj_bias', 'connector.attn.out_proj.weight', 'connector.attn.out_proj.bias', 'connector.proj.weight', 'connector.proj.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.fc1.bias', 'connector.fc1.weight', 'connector.fc2.bias', 'connector.fc2.weight'])
[[34m2025-09-15 07:08:33[0m] _IncompatibleKeys(missing_keys=['latent_pos_embed.pos_embed', 'connector.query', 'connector.layer_projectors.0.weight', 'connector.layer_projectors.0.bias', 'connector.layer_projectors.1.weight', 'connector.layer_projectors.1.bias', 'connector.layer_projectors.2.weight', 'connector.layer_projectors.2.bias', 'connector.attn.in_proj_weight', 'connector.attn.in_proj_bias', 'connector.attn.out_proj.weight', 'connector.attn.out_proj.bias', 'connector.proj.weight', 'connector.proj.bias', 'vit_pos_embed.pos_embed'], unexpected_keys=['connector.fc1.bias', 'connector.fc1.weight', 'connector.fc2.bias', 'connector.fc2.weight'])
